\documentclass{article}

\usepackage{Custom_Latex/Summary_Notes/notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}
% WEEK 1 %
% TUTORIAL - NONE %

% DAY - TUESDAY %
% LECTURE - 1 %
% READING - DONE %
% NOTES_COMPLETE - 40% %

\title{Learning - Summary Notes}
\author{Maksymilian Mozolewski}
\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
\subsection{Information}
\nDefinition{General Information}{
url: http://ww.inf.ed.ac.uk/teahing/courses/inf2b/\par
drop in labs: Tue 11:10013:00, Wed 13:10-15:00\par 
CW1 06/Mar - 03/Apr
}
\subsubsection{How to do well in this course}
\nDefinition{Success Criteria}{
Look through slides in advance of lectures \par
have questions prepared to ask \par
laptops not allowed ?}
\subsection{Machine Learning}
\nDefinition{Definition}{Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.}
\subsubsection{Applications}
\nDefinition{Applications}{
Vision, Graphics, AI & NLP, Robotics, Compilers etc..}
\subsubsection{Examples}
\nDefinition{Example}{Viola-jones face detection (2001) - Face detector consists of linear combination of 'weak' classifiers that utilise five types of primitives. think of this as convolution filter applied over image}
\subsection{Classification}
\subsubsection{Classifying oranges and lemons}
\nDefinition{In two dimensional space}{
Represent each sample as a point (w,h) in a 2D space
\<Graph\>}
\subsubsection{Feature Vector}
\nDefinition{Pixel image to feature vector}{
Perhaps turn each cell (pixel) into a number (real or integer), Unravel into a column vector, a \bd{feature vector}, this way we can represent an image as a point in high-dimensional space, and compare images based on their euclidian distance!}
\nDefinition{Euclidian distance}{
distance between D-dimensional vectors: $\bd{u}=(u_{1},\hdots,u_{D})^{T}$ and $\bd{v}=(v_{1},\hdots,v_{D})^{T}$
\begin{gather*}
    r_{2}(\bd{u},\bd{v}) = \sqrt{\sum_{k=1}^{D}(u_{k} - v_{k})^{2}}
\end{gather*}}
\subsubsection{Image Resolution}
\nDefinition{Which is better}{
2x2, 4x4, 16x16 or 100x100 ?}

% DAY - X %
% LECTURE - 2 %
% READING - 3 %
% NOTES_COMPLETE -  %
\section{Similarity and Recomendation systems}
\subsection{Recomending movies toy example}
\nDefinition{}{find the closest critic to the user, and fill in unseen movies' scores with that critics reviews}
\section{clustering and data visualisation}
\nDefinition{Clustering}{Partition a data set into meaningful or useful groups, based on distances between data points\par
it's an unsupervised process - the data items do not have class labels\par 
why cluster?\par \bd{interpreting data} - analyse and describe a situation by automatically dividing a data set into groupings\par 
\bd{Compressing data} - represent data vectors by their cluster index - vector quantisation}
\subsection{K-Means Clustering}
\nDefinition{definition}{A simple algorithm to find clusters:\par Initialise K cluster centres\par\bigskip
While not converged:\par
\begin{itemize}
    \item Assign each data vector to the closest cluster center
    \item Recompute each cluster mean s the mean of the vectors assigned to that cluster
\end{itemize}
k-means clustering is optimal for finding a local solution not a global one
}
\subsubsection{Evaluation of clustering}
\nDefinition{total squared error}{
where $z_{kn}$ is 1 if the point $\bd{x}_{n}$ belongs to cluster k and 0 
\begin{equation*}
    E = \sum_{k=1}^{K}\sum_{n=1}^{N}z_{kn}||\bd{x}_{n} - \bd{m}_{k}||^{2}
\end{equation*}
the total squared error decreases as K increases (when K = N you win)}
\subsection{dimensionality reduction and data visualisation}
\nDefinition{Definition}{High-dimensional data are dificult to understand and visualise.\par
consider dimensionality reduction of data for visualisation (3d to 2d, projection of points onto plane $||x||cos(\theta)$)\par
better projections are spread out more, higher variance   
\begin{equation*}
    Var(y) = \frac{1}{N-1}\sum_{n=1}^{N}(y_{n} - \hat{y})^{2}
\end{equation*}    
    }
    
\nDefinition{Principal Component Analysis}{
mapping D-dimensional data to a principal component axis \bd{u} that maximises Var(y)\par
\bd{u} is given as the eigenvector with the largest eigenvalue of the covariance matrix, S:\par
Eigen Values $\lambda$}
\section{Classification and K-nearest neighbours}
\subsection{types of learning problems}
\nDefinition{}{regression vs classification - regression works on continous output, classification deals with discrete outputs}
\section{statistical pattern recognition and optimisation}
\nDefinition{}{Empirical vs Theoretical probability = observed vs theoretical probability}
\subsection{Rules of Probability}
\nDefinition{}{Product rule, sum rule, bayes' theorem, for random variables}
\nDefinition{Idea}{Statistical prediction: given two conditional probabilities, the higher one for the test data, will be the one corresponding to the label}

\section{Continuous R.V. statistical predictions}

\end{document}
