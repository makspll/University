\documentclass{article}

\usepackage{Custom_Latex/Summary_Notes/notes}


\begin{document}
\title{Reasoning \& Agents - Summary Notes}
\author{Maksymilian Mozolewski}
\maketitle
\pagebreak
\tableofcontents
\pagebreak 
27


% WEEK 1 %
% TUTORIAL - NONE %

% DAY - TUESDAY %
% LECTURE - 1 %
% READING - DONE %
% NOTES_COMPLETE - 80% %
\section{Introduction}
\subsection{Book}
\paragraph{Artificial Intelligence: A Modern Approach}

\section{Intelligent Agents}

\subsection{Agents}

\nDefinition{Agent}{\bd{Perceives} its \bd{environment}, through its \bd{sensors}, then achieves its \bd{goals}, by acting on its environment via \bd{actuators} \par
\bd{Rational Agent} - always makes decisions which maximize its \bd{performance measure}, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has. A `rational` agent doesn't necessarily pick the best outcome, but the best outcome given its state/percepts etc. which might involve \bd{information-gathering} or \bd{exploration}, and \bd{learning on past experiences} (crossing the road requires a look around!/a roomba which forgets where the kitchen is is not rational) \par
\bd{omniscence} - an omniscent agent knows the actual outcome of its actions and can act accordingly, very different to a rational agent.\par
\bd{agent function} - maps any given \bd{percept} sequence to an action (abstract mathematical concept, basically a big table) \par
the process of writing an agent function is called \bd{tabulating} \par
\bd{agent program} - concrete implementation of the agent function running within some physical system \par
\bd{autonomy} - an autonomous agent relies more on its own percepts rather than initial knowledge from the designer}
\nDefinition{Percept}{An agent's perceptual inputs at any given instant. An agent's \bd{percept sequence} is the complete history of everything the agent has ever perceived \par f}
\nDefinition{Task Environment}{
PEAS - \bd{P}erformance measure, \bd{E}nvironment, \bd{A}ctuators, \bd{S}ensors}
\nDefinition{Examples}{
\small{
\begin{tabularx}{\textwidth}{ X | X | l | X | X }
    Agent Type & Performance Measure & Environment & Actuators & Sensors \\\hline
    Medical diagnosis system & Healthy patient, reduced costs & Patient,hospital,staff & Display of questions, tests, diagnoses, treatments, referrals & Keyboard entry of symptoms, findings, patient's answers\\\hline
    Satellite image analysis system & Correct image categorization & Downlink from orbiting satellite & Display of scene categorisation & Color pixel arrays\\\hline
    Part-picking robot & Percentage of parts in correct bins & Conveyor belt with parts; bins & jointed arm and hand & Camera, joint angle sensors \\\hline
    Refinery controller & Purity, yield, safety & Refinery, operators & Valves, pumps, heaters, displays & Temperature, pressure, chemical sensors\\\hline
    Interactive English tutor & Student's score on test & Set of students, testing agency & Display of exercises suggestions corrections & Keyboard entry\\\hline
\end{tabularx}
}}

\subsubsection{Simple Reflex Agents}

\nDefinition{Simple Reflex Agents}{
action depends only on \emph{current} percepts(they are the state), implement by condition-action rules, e.g.:\par
\bd{if} car-in-front-is-braking \bd{then} initiate-braking. \par
work well in fully-observable environments. Can be made to function better in partially-observable environments with \bd{randomisation}}

\subsubsection{Model-Based Reflex Agents}

\nDefinition{Model-Based Reflex Agents}{
Build on simple reflex agents, by adding persistent \bd{state}, and making decisions based on percept history as well as current percepts.\\
Need to maintain internal world model - needs to understand how it affect the environment with its actions and the `rules` of the world - the \bd{model}.
}

\subsubsection{Goal-Based Agents}

\nDefinition{Goal-Based Agents}{Simple Reflex agents don't really have an explicit 'goal', they have functions telling them what to do at each point in time. Goal-Based-Agents, determine what to do from their goal as well as the model of the environment.}

\subsubsection{Utility-Based Agents}

\nDefinition{Utility-Based Agents}{Agents so far had a single goal, agents may have to juggle conflicting goals. Goal-Based Agents, only consider the binary "Goal reached/Not Reached". Need to optimise utility over a range of goals. \textbf{Utility}: measure of goodness(a real number, internalisation of the Performance score). Combine with probability of success to get expected utility. If the utility function and perofmance measure are in agreement, then the agent will be rational when it maximises its utility(or rather \bd{expected utility})}

\subsubsection{Learning Agents}

Not covered, learn from experience.
\subsection{Environment}

\nDefinition{Environment}{The things an agent has to interact with. An environment can be categorised based on 6 main categories: \bd{D}eterminism, \bd{E}pisodicity, \bd{A}gents, \bd{D}iscreteness, \bd{O}bservabality, \bd{S}taticness : \bd{OS-DEAD}}

\subsubsection{{Fully Observable} vs. {Partially Observable}}

\nDefinition{\textbf{Fully Observable} vs. \textbf{Partially Observable}}{
\bd{full}: agent's sensors describe environment state fully at each point in time. A task environment is \emph{effectively} fully obserbable when the sensors detect all aspects that are \empg{relevant} to the choice of action, relevance depends on performance measure\par 
\bd{Partial}: some parts of environment not visible, perhaps the sensors are noisy.
}

\subsubsection{{Deterministic} vs. {Stochastic}}

\nDefinition{\textbf{Deterministic} vs. \textbf{Stochastic}}
{
\bd{Deterministic}: if the next state of the environment is fully determined by its current state and agent's actions. (maybe a robot in a closed simulation ?) \par
\bd{Stochastic}: even if you know the current state and your agents actions you can't fully predict the new state of the environment with 100\% certainty. (This definition ignores other agents actions!)
}

\subsubsection{{Episodic} vs. {Sequential}}

\nDefinition{\textbf{Episodic} vs. \textbf{Sequential}}{
\bd{Episodic}: the agent's experience is divided into atomic episodes. next action does not depend on previous actions (classification of images?)\par
Mail-sorting robot vs. crossword puzzle.
}

\subsubsection{{Static} vs. {Dynamic}}

\nDefinition{\textbf{Static} vs. \textbf{Dynamic}}{
\bd{Static}: environment unchanged while agent deliberates (freezes). \par
\bd{Semi-dynamic} : while agent deliberates, environment itself doesn't change, while the performance score does. \par
Crossword puzzle vs. chess. \par 
Industrial robot vs. robot car
}

\subsubsection{{Discrete} vs. {Continuous}}

\nDefinition{\textbf{Discrete} vs. \textbf{Continuous}}{
\bd{Discrete}: percepts, actions and episodes are discrete. There is a finite number of distinct states,actions and episodes(think finite outcomes). \par
Chess vs. robot car.}

\subsubsection{{Single Agent} vs. {Multi-Agent}}

\nDefinition{\textbf{Single Agent} vs. \textbf{Multi-Agent}}{
How many objects must be modelled as agents. Weather agent B has to be modelled as an agent is decided by asking "is B's behaviour best described as maximising a performance measure whose value depends on agent A's behaviour?", 2 taxi's can both be seen as agents trying to drive safely, and a collision will affect both of their peformance measures (\bd{cooperative environment}).

\par Crossword vs. poker.
}

\nDefinition{Examples}{insert table from p.45 here}

% DAY - THURSDAY %
% LECTURE 2 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Searching}
\subsection{Problem-solving agents}
\nDefinition{Formulate-Search-Execute agents}{type of goal-based agents which have a one goal which needs to be achieved with a search of some sort. \par
\bd{Action Sequence} - the result of a search, a series of actions required to reach the goal.\par 
\bd{problem} - the model of the problem over which the action sequence is defined \par 
When a problem-solving-agent has nothing in the action sequence it searches for one, and if it has one it follows it (think football)}
\subsection{Problem types}
\subsubsection{Determnistic, fully observable}
\nDefinition{Determnistic, fully observable}{Agent knows exactly which state it will be in; solution is a sequence}
\subsubsection{Non-observable}
\nDefinition{Non-observable}{Agent may have no idea where it is; solution is a sequence}
\subsubsection{Nondeterministic and/or partially observable}
\nDefinition{Nondeterministic and/or partially observable}{contingency problem - percepts provide new information about current state \par
often interleave search, execution}
\subsubsection{Unknown state space}
\nDefinition{Unknown state space}{exploration problem}

\subsection{Problem formulation}
\subsubsection{Problem Definition}
\nDefinition{Problem Definition}{A problem is defined by four items:\par 
\bd{initial state} - where do we start\par 
\bd{actions or successor function} - set of action-state pairs\par
\bd{goal test} - explicit/implicit (in Bucharest or Checkmate(x))\par
\bd{path cost} - the cost of each action in each state\par
\bd{a solution} - sequence of actions leading from the initial state go a goal state}
\subsubsection{Choosing a State Space}
\nDefinition{Choosing a State Space}
{
Real world is absurdly complex - state space must be \empg{abstracted} for problem solving.\par
For guaranteed relizability, \bd{any} real state must get to some real state.\par
Each abstract action should be "easier" than the original problem.\par
\bd{(Abstract) States} - set of real states\par
\bd{(Abstract) Actions} - complex combination of real actions which can be performed from each state/par
}

\subsection{Example problems}
\subsubsection{The 8-puzzle}
\nDefinition{8-puzzle}{
\<diagram\>\par
\bd{states?} locations of tiles\par
\bd{actions} move blank left, right, up, down \par
\bd{goal test?} = goal state(explicit)\par
\bd{path cost?} 1 per move}
\subsubsection{Robotic Assembly}
\nDefinition{Robotic Assembly}{
\<diagram\>\par
\bd{states?} real-valued coordinates of robot joint angles \& parts of the object to be assembled \par
\bd{actions} continuous motions of robot joints\par
\bd{goal test?} complete assembly\par
\bd{path cost?} time to execute
}
\subsection{Basic search algorithms}
\subsubsection{Tree search algorithm}
\nDefinition{}{basic idea: offline, simulated exploration of state space by generating successors of already-explored states (a.k.a expanding states) (think depth-first search)}
\nDefinition{States vs. Nodes}{
A state is a representation of a physical configuration \par
A node is a book-keeping data structure constituting part of a \bd{search tree} includes state, parent node, action, path cost}
% DAY - FRIDAY %
% LECTURE 2 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Tree Search Strategies}
\nDefinition{Definition}{A \bd{search strategy} is defined by picking the order of node expansion - the order in which we pick nodes from the frontier \par
Strategies are evaluated along the following dimensions:\par 
\bd{completeness} - does it always find a solution if one exists? \par
\bd{time complexity} - number of nodes generated \par
\bd{space complexity} - maximum number of nodes in memory \par
\bd{optimality} - does it always find a a least-cost solution? \par
time and space complexity are measured in terms of:\par
\bd{b}: maximum branching factor of a search tree\par
\bd{d}: depth of the least-cost solution\par
\bd{m}: maximum depth of the state space(may be \infty, think loops)}
\subsection{Uninformed tree search strategies}
\nDefinition{Definition}{use only information in problem definition}
\subsubsection{Repeated states Problem}
\nDefinition{Repeated states}{failure to detect repeated states can turn a \bd{linear} problem into an \bd{exponential} one!}
\subsubsection{Graph search}
\nDefinition{Graph Search}{it introduces a set of \bd{explored nodes}, it adds nodes to the frontier set only if they are not in the frontier or the explored set}
\subsubsection{Breadth-first search}
\nDefinition{Breadth-first search}{Expand shallowest unexplored node first. frontier is a FIFO queue, i.e. new successors go at end\par
\bd{complete?} Yes (if b is finite)\par 
\bd{Time?} $b+b^{2} + b^{3} + \hdots + b^{d} = O(b^{d})$ worst case b-ary tree of depth d (not m)\par 
\bd{Space?} $O(b^{d})$ (keeps every node in memory) \par 
\bd{optimal?} Yes (if cost = 1 per step, then a solution is optimal if it is closest to start node)}
\subsubsection{Depth-first Search}
\nDefinition{}{
Expand deepest unexplored node, frontier = LIFO queue, i.e., put successors at front\par
\bd{complete?} No: fails in infinite-depth spaces, spaces with loops - modify to avoid repeated nodes. Complete in finite spaces\par
\bd{Time?} $O(b^{m})$ terrible if m is much larger than d, if solutions are dense, then may be much faster than breadth-first\par 
\bd{Space?} $O(bm)$ i.e. linear spac!\par 
\bd{optimal?} No 
}

\subsubsection{Depth-limited search}
\nDefinition{}{
This is depth-first search with a \bd{depth limit l}, i.e., nodes at depth l have no ancestors\par
\bd{complete?}\par 
\bd{Time?}\par 
\bd{Space?}\par 
\bd{optimal?} 
}
\subsubsection{Iterative deepening search}
\nDefinition{}{Depth-limited search but done over a variety of l-values\par
\bd{complete?} Yes\par 
\bd{Time?} $(d)b + (d-1)b^2 + \hdots + (1)b(^{d})$\par 
\bd{Space?}$O(bd)$\par 
\bd{optimal?} Yes, if step cost = 1\par 
11\% overhead over for repeated operations
}
\subsubsection{Uniform cost search}
%HOMEWORK%
\subsubsection{Summary of algorithms}
\nDefinition{}{
\begin{tabular}{c|c|c|c|c|c}
    Criterion & Breadth-first & Uniform-cost & Depth-first & Depth-limited & Iterative Deepening\\\hline
    Complete? & Yes & Yes & No & No & Yes\\\hline
    Time O(b^{d})& & & & &\\
    Space & & & & &\\ 
    Optimal & Yes & Yes & No & No & Yes
\end{tabular}}
% DAY - TUESDAY %
% LECTURE 3 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Adversarial Search}
\subsection{Games}
\nDefinition{Definition}{We are usually interested in \bd{zero-sum} games of perfect information \par
\begin{itemize}
    \item Deterministic, fully observable
    \item Agents act alternately
    \item Utilities at end of game are equal and opposite
\end{itemize}
"Unpredictable" opponent - specifying a move for every possible opponent reply \par\smallskip
Time limits - unlikely to find goal, must approximate\par
Normal search: optimal decision is a sequence of actions leading ot a goal state (i.e. a winning terminak state)\par
Adversarial search:\par
\begin{itemize}
    \item MIN has a say in game
    \item MAX needs to find a contingent strategy which specifies:\par
    MAX's move in initial state then... MAX's moves in states resulting from every response by MIN to the move then .. MAX's moves in states resulting from every response by MIN to all those moves, etc.
\end{itemize}
}
\subsection{Game Tree}
\nDefinition{Definition}{$<$Min Max diagram$>$}
\nDefinition{Minimax Value}{
definition}
\nDefinition{Minimax example}{
Perfect play for deterministic games\par 
Idea: choose move to position with highest minimax value = best achievable payoff against best play.\par
Example: 2-ply game:
}
\nDefinition{minimax Algorithm}{
$<algorithm>$
\bd{complete?} Yes(if tree is finite)\\
\bd{Optimal?} Yes (against an optimal opponent)\\
\bd{Time complexity?} $O(b^{m})$\\
\bd{Space complexity?} $O(bm)$ (depth first exploration)\\
}
\nDefinition{$\alpha-\beta$ pruning algorithm}{
prune the leaves of the game tree depending on the decisions of its opponent\par 
does not affect final result of the game \par
Good move ordering imporves effectiveness of pruning (How could previous tree be better ?) \par (the order of the leaves' utility matters greatly) \par 
With perfect ordering time complexity: $O(b^{\frac{m}{2}})$\par\bigskip
why alpha and beta pruning ? \par 
alpha is the value of the best choice found so fat at any choice point along the path for MAX\par 
if v is worse than alpha, MAX will avoid it - prune that branch\par 
beta is the same thing for the MIN player\par
$<algorithm>$}
\subsubsection{Resource Limits}
\nDefinition{Definition}{
Suppose we have 100 secs, explore $10^4$ nodes/ sec - $10^6$ nodes per move\par
standard approach: - cutoff test: e.g., depth limit (perhaps add quiescence search, which tries to search interesting positions)}
\nDefinition{Evaluation functions}{
For cheass typically linear weighted sum of features
\begin{equation*}
    EVAL(s) = w_{1}f_{1}(s) + w_{2}f_{2}(s)+ \hdots +w_{n}f_{n}(s) 
\end{equation*}}
\nDefinition{Cutting off search}{
Minimax Cutoff is identical to MinimaxValue except - TERMINAL_TEST is replaced by CUTOFF, UTILITY is replaced by EVAL\par 
does it work in practice? $b^{m} = 10^{6}, b= 35, m = 4$ 
4-ply lookahead is a hopeless chess player}
% DAY - THURSDAY %
% LECTURE 4 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Informed (Heuristic) Search Strategies}
\nDefinition{Definition}{an \bd{informed} strategy - one with problem-specific knowledge beyond the definition of the problem itself.\par
\bd{best-first-search} - instance of the general Tree-Search or Graph-Search algorithm in which a node is selected for expansion based on an \bd{evaluation function}($f(n)$)\par
\bd{heuristic function} ($h(n)$) - estimates the cost of the cheapest path from the given node/state to the goal state/node\par
}
\subsection{Heuristic}
\nDefinition{Definition}{
From the greek word "Heuriskein" meaning "to disover" "to find"\par
A heuristic is any method that is believed or practically proven to be useful for the solution of a given problem.}
\subsection{Greedy best-first search}
\nDefinition{Definition}{tries to expand the node that is 'closest' to the goal in terms, on the grounds this is likely to lead to a solution quickly.\par
so $f(n) = h(n)$\par
\bd{Complete?} No - can get stuck in loops (graph search version is complete in finite space, but not in infinite ones)\par
\bd{Time?} $O(b^{m})$ for tree version, but a good heuristic can give dramatic improvement\par
\bd{Space?} $O(b^{m})$\par
\bd{Optimal?} no}
\subsection{A* search}
\nDefinition{Definition}{
The most widely known form of best-first search. It evaluates nodes by combining $g(n)$, the cost to reach the node, and $h(n)$ the cost to get from the node to the goal - $f(n) = g(n) + h(n)$ e.g. the estimated cost of the cheapest solution through n.\par

\bd{Complete?} Yes(unless there are infinitely many nodes with $f \leq f(G)$)\par
\bd{Time?} Exponential\par
\bd{Space?} Keeps all nodes in memory\par
\bd{Optimal?} Yes 
}
\nTheorem{Optimal A*}{in order to be optimal, the heuristic needs to be:\par
\bd{admissible} - never overestimate the cost to reach the goal.\par
\bd{consistent} - for every node n and every successor $n^{'}$ of n generated by an action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to $n^{'}$ plus the estimated cost of reaching the goal from $n^{'}$
\begin{equation*}
    h(n) \leq c(n,a,n^{'}) + h(n^{'})
\end{equation*}
(\bd{the triangle inequality})\par
$<proof from slides> f(n) < f(G_{2})$
\bd{consistent} - if doing graph-search and the heuristic is consistent, then A* is optimal
}
\subsection{admissible heuristics}
\nDefinition{Definitio}{
8 puzzle example comparing heuristics}
\subsection{Relaxed Problems}
\nDefinition{Defintion}{
A problem with fewer restrictions on the actions is called a \bd{relaxed problem}\par
the cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem.\par
if the rules of the 8-puzzle are relaxed so that a tile can move anywehre then $h_{1}(n)$ gives the shortest solution\par
if the rules are relaxed so that a tile can move to any adjacent square then $h_{2}(n)$ gives the shortest solution}

% DAY - TUESDAY %
% READING - NOT DONE %
% NOTES_COMPLETE -  %

\section{Logical Agents}
\subsection{Knowledge Based Agents}
\nDefinition{Knowledge Base}{Set of sentences in a formal language,which tell the agent what it needs to know, and the agent can follow the knowledge base for answers\par
KB's can be part of an agent or be accessible to many agents.\par
the agent's KB can be viewed at the knowledge level i.e. what it knows, regardless of how implemented\par
on the implementation level, data structures in KB and algorithms that manipulate them.}
\nDefinition{Knowledge-Based Agent}{

\begin{algorithmic}[1]
\Function{KB-Agent}{percept} \Comment{\bd{returns} an action}
\State \bd{persistent}: $KB$ a knwoledge base
\State \hspace*{58pt}$t$, a counter, initially 0, indicating time
\State \Call{Tell}{$KB$,\Call{Make-Percept-Sentence}{$percept,t$}}
\State $action \gets$\Call{Ask}{KB,\Call{Make-Action-Query}{t}}
\State \Call{Tell}{KB,\Call{Make-Action-Sentence}{action,t}}
\State $t \gets t + 1$
\EndFunction
\State \Return{action}

\end{algorithmic}

The agent must be able to: represent states, actions, etc. Incorporate new percepts, update internal representations of the world, deduce hidden properties of the world, deduce appropriate actions}
\subsection{Logic in General}
\nDefinition{Definition}{\bd{Logics} - are formal languages for representing information such that conclusions can be drawn\par
\bd{Syntax} - defines all the well-formed sentences in the language\par
\bd{Semantics} - defines the "meaning" or \bd{truth} of sentences with respect to some \bd{model/world}}
\subsection{Models}
\nDefinition{Definition}{
The models are formally structured worlds with respect to which truth can be evaluated. We say m is a model of a sentence a, if a is true in m. M(a) is the set of all models of a 
\begin{equation*}
    \alpha \models \beta \textit{ iff } M(\alpha) \subseteq M(\beta)
\end{equation*}
}
\subsection{Entailment}
\nDefinition{Entailment}{ $\alpha \models \beta $:
$\alpha$ entails sentence $\beta$ if and only if $\beta$ is true in all worlds where $\alpha$ is true}

\subsection{Inference}
\nDefinition{Inference}{$\alpha \vdash _{i} \beta$ : sentence $\beta$ can be derived from $\alpha$ by an a inference procedure i\par}
\nDefinition{Soundness}{
    i is \bd{sound} if whenever $\alpha \vdash_{i} \beta$ it is also true that $\alpha \models \beta$
}
\nDefinition{Completeness}{
    i is \bd{complete} if whenever $\alpha \models \beta$, it is also true that $\alpha \vdash_{i} \beta$}
\subsection{Propositional Inference}
\nDefinition{Algorithm}{
\begin{algorithmic}[1]
    \Function{TT-Entails?}{KB,\alpha}\Comment{\bd{returns} $true$ or $false $}
    \State \bd{inputs:}
    \State \AIndent[] $KB$, the knowledge base, a sentence in propositional logic.
    \State \AIndent[] $\alpha$, the query, sentence in propositional logic
    \Statex
    \State $symbols \gets $ a list of the proposition symbols in KB and $\alpha$
    \State \Return{\Call{TT-Check-ALL}{KB,$\alpha$,$symbols$, \{\}}}
    \EndFunction
    \Statex
    \Function{TT-Check-All}{KB,$\alpha$,$symbols$,$model$} \Comment{\bd{returns} $true$ or$ false$ }
    \If{\Call{Empty?}{$symbols$}}
        \If{\Call{PL-True?}{$KB,model$}}
            \State \Return{\Call{PL-True?}{$\alpha$,$model$}}
        \Else
            \State \Return{$true$} \Comment{When KB is false, always return true}
        \EndIf
    \Else
    \State $P gets$ \Call{First}{$symbols$}
    \State $rest \gets $ \Call{Rest}{$symbols$}
    \State \Return
    \State \AIndent[] {\Call{TT-Check-All}{$KB,\alpha,rest,model \cup \{P = true\}$}} 
    \State \AIndent[ ]\bd{And}
    \State \AIndent[] \Call{TT-Check-All}{$KB,\alpha ,rest, model \cup \{P = false\}$}
    \EndIf
    \EndFunction
\end{algorithmic}}
% DAY - TUESDAY %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Effective Propositional Inference}
\subsection{Forms}
\subsubsection{CNF}
\nDefinition{CNF}{conjunctive normal form - conjunction of clauses (and of or's)}
\nDefinition{Convert to CNF}{
eliminate bi-implies, eliminate implies, move negations inwards using de Morgan's, apply distributivity law}
\subsubsection{DPLL}
\nDefinition{Definition}{Determine if an input propositional logic sentence (in CNF) is \bd{satisfiable}\par \emph{improvements} over truth table enumeration:\par
Early termination (a clause is true if one of its literals is true, asentence is false if \bd{any} of its clauses is false), Pure symbol heuristic - (\bd{pure symbol} - \bd{always} appears with the same sign or polarity in \bd{all} clauses, make literal containing a pure symbol true (for satisfiability)), Unit clause Heuristic - (only one literal in the clause, e.g.. (A the only literal in a unit clause must be true, also includes clauses where all but one literal is false), tautology deletion (remove clauses where a literal appears both postitively and negatively)}
\subsubsection{WalkSat}
\nDefinition{Definition}{incomplete local search algorithm, Evaluation function: the min-conflict heuristic of minimizing the number of unsatisfied clauses, balance between greediness and randomness\par
checks for satisfiability by randomly flipping the values of variables
}
\subsection{Inference-based agents in the wumpus world}
\nDefinition{Definition}{
A wumpus-world agent using propositional logic, the KB has 64 distinct proposition symbols and 155 sentences. 
$<hybrid wumpus agent>$\par
limitation of propositional logic, explosion of clauses}

%LECTURE - THURSDAY%

\section{Constrained Search (smart search with constraints)}
\nDefinition{Definition}{A more efficient way of searching, which uses the fact that some problems have structure, or constraints which can be used to impose the structure onto a search problem}
\nDefinition{State}{\bd{state} - a black box - any data struture that supports a successor function, a heuristic function and a goal test}
\subsection{Constraint Satisfaction Problems (CSP)}
\nDefinition{Definition}{
X- set of variables,\par
D - set of domains, each domain is a set of possible values for some X\par
C - set of constraints - that specify accepted combinations of values\par
a constraint c consists of a scope- tuple of variables involved in the constraint - and a relation that defines the values that the variables can take}
\nDefinition{Solution}{complete and consistent assignments of values to variables}
\nDefinition{Constraint graph}{
Binary CSP, each constraint relates two variables, nodes are varialbes, edges represent constraints}
\nDefinition{Varieties of CSP}{
DISCRETE VARIABLES\par
finite domains: n variables, domain size d, $\theta(d^{n}$ ) assignments. \par
infinite domains:\par
integers, strings, etc.. \par
CONTINOUS VARIABLES\par
e.g. start/end times for Hubble Space Telescope observations, linear constraints solvable in polynomial time by linear programming}
\nDefinition{Varieties of constraints}{
Unary - constraints involve a single variable\par
Binary - constraints involve pairs of variables\par
Higher-Order constraints- involve 3 or more variables.\par
Global - constraints involve an arbitrary number of variables}
\nDefinition{Standard Search formulation}{
initial state: the empty assignment\par
Successor function: assign a value to an unassigned variable that does not conflict with the current assignment -> fail if no legal assignments\par
goal test: the current assignment is complete\par
for CSP's with n variables, any solution appears at depth n, use depth-first search}
\subsection{Backtracking search for CSP}
\nDefinition{Definition}{
variable assignments are \bd{commutative} e.g. it doesn't matter in which order i assign the values\par
we only need to consider assignments to a single variable at each node. Thus b = d and there are $d^{n}$ leaves\par
Depth-first search for CSP's with single-variable assignments is called backtracking search\par
Backtracking search: the basic uninformed algorithm for CSP\par
can solve the n-queens problem for $n \approx 25$\\
Backtracking search can be modified by: 
\begin{itemize}
    \item changing the order in which we check the assignments of values to the current variable
    \item changing the order in which we pick the variables to be assigned
\end{itemize}}
\subsection{Efficiency}
\nDefinition{Improving Efficiency with variable ordering}{
When picking which node to expand, we can use the following heuristics:\par
\bd{most constrained variable} first (minimum-remaining-values (MRV) heuristic)\par
\bd{constraining variable} first - the variable with the most constraints on remaining variables, thus reducing branching factor\par 
\bd{Least constraining value} - given a variable, choose the least constraining value - the one that rules out the fewest values in the remaining variables. (not helpful if you need all solutions)\par
}
\nDefinition{Improving Efficiency with value ordering}{
After choosing the variable, we also have the opportunity to prune its domain along with its neighbours domain, as well as to pick the order in which we assign the values to that variable
\bd{inference - forward checking} - keep track of remaining legal values for unassigned variables. Terminate the search when a variable has no more legal values\par
\bd{arc consistency} - simplest form of propagation makes each arc \bd{consistent}\par
$X \rightarrow Y$ is consistent iff\par
for every value x in the domain of X, there is some allowed value y in the domain of y. e.g. an edge will be inconsistent if there is a value for one of its nodes that has no allowed counterpart in the other node that doesn't violate a constraint
}
\section{Coursework}
\section{First Order Logic}
\subsection{propositional logic as a language}
\nDefinition{}{\bd{Compared to languages in computer science:}\par
serves as a basis for declarative languages, allows partial/disjunctive/negated information, is compositional(the meaning of a sentence is derived from that of its axioms, unlike some instances of concurrent programming)\par
\bd{Compared to natural languages}\par
meaning is context-dependent, propositional logic has a very limited expressive power (some things require very many sentences to be described)}
\subsection{First Order logic}
\nDefinition{}{propositional logic deals with atomic facts, while FOL brings structure to facts, which can be built from:\par
\begin{itemize}
    \item Objects: people, houses, numbers, colours, football games
    \item Functions: father of, best friend, one more than, plus
    \item Relations: red, round, prime, brother of, bigger than, part of
\end{itemize}}
\subsection{Syntax}
\nDefinition{Signatures}{A first-order \bd{signature} is a pair (F,P)
\begin{itemize}
    \item F - indexed family $(F_{n})_{n\in\mathbb{N}}$ of sets of \bd{function} symbols(operations)
    \item P - indexed family $(P_{n})_{n\in\mathbb{N}}$ of sets of \bd{relation} symbols(predicates)
\end{itemize}
For $\sigma \in F_{n}$ and $\pi \in P_{n},n$ is called \bd{arity}\\
\bd{Constant} symbols are function symbols with arity zero\\
Example:
\begin{itemize}
    \item $F_{0} = \{Richard,John\}, F_{1} =\{LeftLegOf\}$
    \item $P_{1} = \{Crown, King,Person\}$
    \item $P_{2} = \{Brother, OnHead\}$
\end{itemize}
}
\nDefinition{Sentences}{
    sentences are made up of \bd{term}s, \bd{functions} on terms and other sentences connected with logical connectives
}
\nDefinition{Models}{Models are composed of a set of objects: the domain M\par 
the set of functions from each function symbol to the objects: $M_{n}$\par
the set of tuples on the domain for each relation symbol:$ M^{\pi}$.\par
The models decide how the objects are connected to the terms and predicates}
\nDefinition{Satisfaction relation}{links the syntax and the semantics}
\section{Unification}
\nDefinition{Definition}{
A unification of two sentences is the application of substitution in such a way that they become identical}
\nDefinition{Terms over X}{
The function $T_{F}$(X) are the terms from the signature $(F \cap X,P)$}
\nDefinition{Problem}{
An equation is a pair of terms $(t,t^{'})$ with $t,t^{'} \in T_{f}(X)$. we denote the equation $(t,t^{'})$ as $t \stackrel{?}{=} t^{'}$\par\medskip
A unification problem is a set of equations, and a Unifier is a substitution which unifies all of them\\
A unifier $\theta$ is more general than another unifier $\sigma$, if there exists a substitution $\mu$ such that $\sigma = \theta ; \mu$\\
and the most general unifier is one that is more general than any other unifier in Unify(t)}
\nDefinition{Unification Algorithm}{}
\subsection{Modus Ponens}
\nDefinition{Forward Chaining Algorithm}{
find substitutions that match premises with conclusions in your KB and apply generalised modus ponens\\}
\nDefinition{Improving efficiency}{
Incremental search, Database indexing - constant time for retrieving known facts (used in Deductive databases)}
\nDefinition{Backward Chaining Algorithm}{
start with the goal, and prove its premises recursively and collect the substituted terms values as you go}
\section{Planning}
\nDefinition{}{
Planning is the task of coming up with a sequence of actions that will achieve a goal,
\\\\
we are only considering classical planning, in which environments are:
\begin{itemize}
    \item fully observable
    \item deterministic
    \item finite,
    \item static,
    \item discrete
\end{itemize}
\\\\
lifting some of these assumptions will be the subject of the uncertainty part of the course
\\
}
\nDefinition{why planning}{
Consider a serach-based problem-solving agent in a robot shopping world\\\\
task: Go to the supermarket and get milk, bananas and a cordless drill,\\\\
what would a search based agent do ?\\\\
Search is not goal-directed, and it's hard to quantify the complexity of the search space and costs\\\\
logic is good only if the goals are independent of each other, what if there are states which let you achieve only some of the goals and stop you from achieving the others ? (non-linear planning problem), undecidability/semi-decidability problems, cannot tell which solution is better: $A\rightarrow B$ or$ A\rightarrow B\rightarrow A \rightarrow B $?
\\\\
we need to reduce complexity, to allow scaling, and allow reasoning to be guided by efficiency or quality
}
\nDefinition{PDDL}{
\bd{Planning Domain Definition Language}\\
\bd{States} - States represented as conjunctions of propositional function-free first order positive literals (Closed world assumption!)\\
\bd{goal} - partial description of atomic states, allows variables, negations etc\\
\bd{actions} - schemata, containing an action name and parameter list, precondition(defines states in which an action is executable, conjunction of positive and negative literals, where all variables must occur in action name) and effect(defines how literals in the input state get changed, anything not mentioned stays the same, Conjunction of positive and negative literals, with all its variables
also in the preconditions, sometimes divided into an ADD list and REMOVE list), an action is applicatble if you can find a state which satisfies its preconditions with an appropriate substitution (state in which you are at the airport, lets you take the flight action)
}
\section{planning with state-space search}
\nDefinition{}{most straightforward way to think of planning process: search the space of states using action schemata\\\\
since actions are defined both in terms of preconditions and effects we can search in both directions: forward and backwards\\\\
forward- search space is finite in the absence of funtion symbols, any complete graph search algorithm, will be a complete graph planning algorithm. Forward search does not solve problem of irrelevant actions\\\\
backward- excludes irellevant actions, lower branching factor
}
\nDefinition{heuristics for state-space search}{
two possibilities: Divide and Conquer- subgoal decomposition\\
Deriv a relaxed problem\\
\\
subgoal decomposition can be- optimistic (admissible) - if negative interactions exists (if subplan deletes goal achieved by other subplan)\\
pessimistic (inadmissible) if positive interactions exist (e.g. subplans contain redundant actions)\\\\
relaxations:\\
drop all preconditions (all actions, always applicable)\\
remove all negative effects (and count minimum number of actions so that union satisfies goals)\\
empty delete lists approach (involves running a simple planning problem to compute heuristic value)
}
\nDefinition{Partial-order planning}{
some actions can be done in either order, why specify which ? i.e. laziness in chosing between chronologally un-ordered actions,
don't specify which one comes first. Partial-order solution will then correspond to one or several \bd{linearisations} of partial-order plan\\\\
define POP as search problem over plans consisting of: Actions - initial plan contains dummy actions (Start no preconditions, effect = initial state), Finish (no effects, precondition = goal literals)\\
Ordering constraints - on actions A B (A must occur before B)\\
Causal links, between actions A B , express A achieves p for B \\
Open preconditions: set of conditions not yet achieved by the plan\\
consistent plan = plan without cycles in orderings and conflicts with links\\
soution = consistent plan without open preconditions\\
Dealing with unbound variables - don't bind variables untill you have to 
}
\nDefinition{}{
dealing with cycles and non-determinism - Label bit of plan, attempt more than once}
\section{Non-determinism with infinite belief states}
\nDefinition{}{
if you cannot list all possible effects of an action, you cannot perform contingency planning (have a plan for every possible outcome), you have to replan\\\\
execution monitoring - checking whether things are going according to plan: action monitoring - checking weather next action is feasible, plan monitoring - checking whether remainder of plan is feasible\\\\
replannning - ability to find new plan when first one goes wrong (not waiting for future actions)}

\end{document}