\documentclass{article}

\usepackage{Custom_Latex/Summary_Notes/notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}
\title{PWA - Summary Notes}
\author{Maksymilian Mozolewski}
\maketitle
\pagebreak
\tableofcontents
\pagebreak
% WEEK 1 %
% TUTORIAL - DONE %
% HAND-IN - COMPLETE %

% DAY - TUESDAY %
% LECTURE - 1 %
% READING - DONE %
% NOTES_COMPLETE - SAME AS DMMR  %

\section{Introduction}
\subsection{Information}
general information\bigskip\\
new course, restructured.
\section{Counting}
\nTheorem{Product Rule}{if A and B are finite sets then: $|A \times B| = |A| \cdot |B|$}
\nTheorem{General Product Rule}{if $A_{1},A_{2},...,A_{m}$ are finite sets then: $|A_{1},A_{2},...,A_{m}| = |A_{1}|\cdot|A_{2}|\cdots|A_{m}|$}
\nDefinition{Counting Summary}{
\begin{tabular*}{\textwidth}{l|l|l}
    Type & Repetition Allowed ? & Formula\\
    \hline
    r-permutations & No & $\frac{n!}{(n-r)!}$\\[15pt]
    r-combinations & No & $\frac{n!}{r!(n-r)!}$\\[15pt]
    r-permutations & Yes & $n^{r}$\\[15pt]
    r-combinations & Yes & $\frac{(n + r -1)!}{r!(n-1)!}$
\end{tabular*}
}

\nDefinition{Permutations}{A permutation of a set S is an ordered arrangement of the elements
of S.
In other words, it is a sequence containing every element of S exactly
once}

\nTheorem{The Binomial Theorem}{For all $n \geq 0$:
\begin{align*}
    (x + y)^{n} = \sum_{j=0}^{n}{{n \choose j}x^{n-j}y^{j}}
\end{align*}
}

\nTheorem{Multinomial theorem}{for all n $\geq$ 0 and all k $\geq 1$:
\begin{align*}
    ({x_{1}+x_{2}+...+x_{k}})^{n} = \sum_{0\leq n_{1},n_{2},...,n_{k}\leq n}{n\choose n_{1},n_{2},...,n_{k}}x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}
\end{align*}}
% DAY - THURSDAY %
% LECTURE - 2 %
% READING -  DONE %
% NOTES_COMPLETE - DONE %
\section{Axioms of Probability}
\subsubsection{Sample Space * Events}
\nDefinition{Sample Space}{
a set of all possible outcomes}
\nDefinition{Event}{any subset of the \bd{sample space}}
\subsubsection{Basic operations on evnets}
\nDefinition{Basic Event Operations}{
\begin{align*}
    E \cup F = \text{ event where either or both E,F happens}\\
    EF = E \cap F \text{ event where both E and F happen}\\
    E^{c} = S - E = \text{ event where E does \bd{not} happen}
\end{align*}}
\nDefinition{De Morgan's law for events}{
\begin{align*}
    (E_{1}\cup E_{2} \cup \hdots \cup E_{n}) = E_{1}^{c} \cap E_{2}^{c} \cap \hdots \cap E_{n}^{c}\\
    (E_{1}\cap E_{2} \cap \hdots \cap E_{n}) = E_{1}^{c} \cup E_{2}^{c} \cup \hdots \cup E_{n}^{c}
\end{align*}}
\subsubsection{Axioms of Probability}
\nDefinition{Axioms}{
for any sequence  $\{E_{1}\E_{2},E_{3},\hdots\}$ of mutually exclusive events and where for each event $E \subseteq S$ we assign a \bd{probability} \mathbb{P}(E) satisfying: 
\begin{gather}
    0 \leq \mathbb{P}(E)\leq 1\\
    \mathbb{P}(S) = 1\\
     \mathbb{P}(\bigcup_{j=1}^{\infty}E_{j}) = \sum_{j=1}^{\infty}\mathbb{P}(E_{j})
\end{gather}}
\subsubsection{Consequences of probability axioms}
\nDefinition{Null event}{
\begin{gather}
    \mathbb{P}(\emptyset) = 0\\
    \mathbb{P}(\bigcup_{j=j}^{n}E_{j}) = \sum_{j=1}^{n}\mathbb{P}(E_{j})\\
    \mathbb{P}(E^{c}) = 1 - \mathbb{P}(E)
\end{gather}}
\nDefinition{Inclusion-Exclusion principle}{
\begin{align*}
    \mathbb{P}(E_{1}\cup E_{2} \cup \hdots E_{n}) = \sum_{j=1}^{n}\mathbb{P}(E_{j}) - \sum_{j < k}^{n}\mathbb{P}(E_{j}\cap E_{k}) + \sum_{j < k < l}^{n}\mathbb{P}(E_{j} \cap E_{k} \cap E_{l})\\ - \hdots + (-1)^{n+1}\mathbb{P}(E_{1} \cap E_{2} \cap \hdots \cap E_{n})
\end{align*}
}
\subsubsection{Probability Distribution}
\nDefinition{Probability Distribution}{
Suppose S = $\{x_{1},x_{2},\hdots,x_{N}\}$ is finite. Set $p_{j}=\mathbb{P}(\{x_{j}\})$ with $1 \leq j \leq N$ is known as the probability distribution
}
\nDefinition{Uniform distribution}{
equally likely outcomes, so for any event in S: 
\begin{align*}
    \mathbb{P}(E) = \frac{#E}{#S}
\end{align*}}

% DAY - X %
% LECTURE - 3 %
% READING -  %
% NOTES_COMPLETE -  %
\section{Conditional Probability And Independence}
\subsection{Conditional Probability}
\nDefinition{Conditional Probability}{Probability of F given E:
\begin{align*}
    \mathbb{P}(F|E) = \frac{\mathbb{P}(F \cap E)}{\mathbb{P}(E)}
\end{align*}
doesn't depend on original sample space!}
\subsection{Independence of Events}
\nDefinition{}{Two events E,F are said to be \emph{independent} if
\begin{align*}
    \mathbb{P}(F \cap E) = \mathbb{P}(F)\mathbb{P}(E)
\end{align*}
A sequence $E_{1},E_{2},\hdots,E_{n}$ of events is said to be independent if for every $i_{1} < i_{2} < \hdots < i_{r}$ such that $i_{j} \in {1,2,\hdots,n}$ and $1 \leq r \leq n $ we have: \begin{align*}
    \mathbb{P}(E_{i_{1}} \cap E_{i_{2}} \cap \hdots E_{i_{r}}) = \mathbb{P}(E_{i_{1}})\mathbb{P} (E_{i_{2}}) \hdots \mathbb{P}(E_{i_{r}}) = \prod_{j=1}^{r}\mathbb{P}(E_{i_{j}})
\end{align*}}
\subsection{Binomial Distribution}
\nDefinition{Binomial Distribution}{
for repeated independent trials where each trial has a probability p of success (probability 1 - p of failure) then:
\begin{align*}
    \mathbb{P}(\text{k successes in n trials}) = \sum_{J \subseteq(1,2,\hdots,n), \#J = k}\mathbb{P}(E_{j}) = {n \choose k}p^{k}(1-p)^{n-k}   
\end{align*}
}
\subsection{Multiplication rule for probabilities}
\nDefinition{Chain rule for probabilities}{
\begin{align*}
    \mathbb{P}(E_{1}\cap E_{2} \cap \hdots \cap E_{n}) = \mathbb{P}(E_{1})\mathbb{P}({E_{2}|E_{1}})\hdots\mathbb{P}(E_{n}|E_{1}\cap E_{2} \cap \hdots \cap E_{n-1})
\end{align*}
}
\subsection{Law of total probability}
\nDefinition{}{
Represent the sample space S as a union of mutually exclusive events:
\begin{align*}
    S = F_{1}\cup F_{2} \cup \hdots \cup F_{n}
\end{align*}
then any event E can be represented as 
\begin{align*}
  E = EF_{1} \cup EF_{2} \cup \hdots \cup EF_{n}  
\end{align*}
so that
\begin{align*}
    \mathbb{P}(E) = \mathbb{P}(E|F_{1})\mathbb{P}(F_{1}) + \hdots + \mathbb{P}(E|F_{n})\mathbb{P}(F_{n})
\end{align*}
}
\subsection{Baye's formula}
\nTheorem{Generalised Baye's Theorem}{
Suppose that $E, F_{1},..., F_{n}$ are events from sample space $\Omega$, and
that $P : \Omega \rightarrow [0, 1]$ is a probability distribution on $\Omega$. Suppose that $\cup_{i=1}^{n}F_{j} = \Omega$ and that $F_{i} \cap F_{j} = \emptyset$ for all $i \not = j$.\\
Suppose $P(E) > 0$, and $P(F_{j}) > 0$ for all j. Then for all j:
\begin{align*}
    P(F_{j}|E) = \frac{P(E|F_{j})P(F_{j})}{\sum_{i=1}^{n}P(E|F_{i})P(F_{i}))}
\end{align*}
}
\subsection{Expectation}
\nTheorem{Better expression for Expectation}{
For a random variable $X : \Omega \rightarrow \mathbb{R}$
\begin{align*}
    E(X) = \sum_{r\in range(X)}P(X = r)\cdot r
\end{align*}}
\end{document}