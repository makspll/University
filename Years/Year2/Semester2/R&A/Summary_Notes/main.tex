\documentclass{article}

\usepackage{Custom_Latex/Summary_Notes/notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{tabularx}
\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}
\title{Reasoning \& Agents - Summary Notes}
\author{Maksymilian Mozolewski}
\maketitle
\pagebreak
\tableofcontents
\pagebreak

% WEEK 1 %
% TUTORIAL - NONE %

% DAY - TUESDAY %
% LECTURE - 1 %
% READING - DONE %
% NOTES_COMPLETE - 80% %
\section{Introduction}
\subsection{Book}
\paragraph{Artificial Intelligence: A Modern Approach}

\section{Intelligent Agents}

\subsection{Agents}

\nDefinition{Agent}{\bd{Perceives} its \bd{environment}, through its \bd{sensors}, then achieves its \bd{goals}, by acting on its environment via \bd{actuators} \par
\bd{Rational Agent} - always makes decisions which maximize its \bd{performance measure}, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has. A `rational` agent doesn't necessarily pick the best outcome, but the best outcome given its state/percepts etc. which might involve \bd{information-gathering} or \bd{exploration}, and \bd{learning on past experiences} (crossing the road requires a look around!/a roomba which forgets where the kitchen is is not rational) \par
\bd{omniscence} - an omniscent agent knows the actual outcome of its actions and can act accordingly, very different to a rational agent.\par
\bd{agent function} - maps any given \bd{percept} sequence to an action (abstract mathematical concept, basically a big table) \par
the process of writing an agent function is called \bd{tabulating} \par
\bd{agent program} - concrete implementation of the agent function running within some physical system \par
\bd{autonomy} - an autonomous agent relies more on its own percepts rather than initial knowledge from the designer}
\nDefinition{Percept}{An agent's perceptual inputs at any given instant. An agent's \bd{percept sequence} is the complete history of everything the agent has ever perceived \par f}
\nDefinition{Task Environment}{
PEAS - \bd{P}erformance measure, \bd{E}nvironment, \bd{A}ctuators, \bd{S}ensors}
\nDefinition{Examples}{
\small{
\begin{tabularx}{\textwidth}{ X | X | l | X | X }
    Agent Type & Performance Measure & Environment & Actuators & Sensors \\\hline
    Medical diagnosis system & Healthy patient, reduced costs & Patient,hospital,staff & Display of questions, tests, diagnoses, treatments, referrals & Keyboard entry of symptoms, findings, patient's answers\\\hline
    Satellite image analysis system & Correct image categorization & Downlink from orbiting satellite & Display of scene categorisation & Color pixel arrays\\\hline
    Part-picking robot & Percentage of parts in correct bins & Conveyor belt with parts; bins & jointed arm and hand & Camera, joint angle sensors \\\hline
    Refinery controller & Purity, yield, safety & Refinery, operators & Valves, pumps, heaters, displays & Temperature, pressure, chemical sensors\\\hline
    Interactive English tutor & Student's score on test & Set of students, testing agency & Display of exercises suggestions corrections & Keyboard entry\\\hline
\end{tabularx}
}}

\subsubsection{Simple Reflex Agents}

\nDefinition{Simple Reflex Agents}{
action depends only on \emph{current} percepts(they are the state), implement by condition-action rules, e.g.:\par
\bd{if} car-in-front-is-braking \bd{then} initiate-braking. \par
work well in fully-observable environments. Can be made to function better in partially-observable environments with \bd{randomisation}}

\subsubsection{Model-Based Reflex Agents}

\nDefinition{Model-Based Reflex Agents}{
Build on simple reflex agents, by adding persistent \bd{state}, and making decisions based on percept history as well as current percepts.\\
Need to maintain internal world model - needs to understand how it affect the environment with its actions and the `rules` of the world - the \bd{model}.
}

\subsubsection{Goal-Based Agents}

\nDefinition{Goal-Based Agents}{Simple Reflex agents don't really have an explicit 'goal', they have functions telling them what to do at each point in time. Goal-Based-Agents, determine what to do from their goal as well as the model of the environment.}

\subsubsection{Utility-Based Agents}

\nDefinition{Utility-Based Agents}{Agents so far had a single goal, agents may have to juggle conflicting goals. Goal-Based Agents, only consider the binary "Goal reached/Not Reached". Need to optimise utility over a range of goals. \textbf{Utility}: measure of goodness(a real number, internalisation of the Performance score). Combine with probability of success to get expected utility. If the utility function and perofmance measure are in agreement, then the agent will be rational when it maximises its utility(or rather \bd{expected utility})}

\subsubsection{Learning Agents}

Not covered, learn from experience.
\subsection{Environment}

\nDefinition{Environment}{The things an agent has to interact with. An environment can be categorised based on 6 main categories: \bd{D}eterminism, \bd{E}pisodicity, \bd{A}gents, \bd{D}iscreteness, \bd{O}bservabality, \bd{S}taticness : \bd{OS-DEAD}}

\subsubsection{{Fully Observable} vs. {Partially Observable}}

\nDefinition{\textbf{Fully Observable} vs. \textbf{Partially Observable}}{
\bd{full}: agent's sensors describe environment state fully at each point in time. A task environment is \emph{effectively} fully obserbable when the sensors detect all aspects that are \empg{relevant} to the choice of action, relevance depends on performance measure\par 
\bd{Partial}: some parts of environment not visible, perhaps the sensors are noisy.
}

\subsubsection{{Deterministic} vs. {Stochastic}}

\nDefinition{\textbf{Deterministic} vs. \textbf{Stochastic}}
{
\bd{Deterministic}: if the next state of the environment is fully determined by its current state and agent's actions. (maybe a robot in a closed simulation ?) \par
\bd{Stochastic}: even if you know the current state and your agents actions you can't fully predict the new state of the environment with 100\% certainty. (This definition ignores other agents actions!)
}

\subsubsection{{Episodic} vs. {Sequential}}

\nDefinition{\textbf{Episodic} vs. \textbf{Sequential}}{
\bd{Episodic}: the agent's experience is divided into atomic episodes. next action does not depend on previous actions (classification of images?)\par
Mail-sorting robot vs. crossword puzzle.
}

\subsubsection{{Static} vs. {Dynamic}}

\nDefinition{\textbf{Static} vs. \textbf{Dynamic}}{
\bd{Static}: environment unchanged while agent deliberates (freezes). \par
\bd{Semidynamic} : while agent deliberates, environment itself doesn't change, while the performance score does. \par
Crossword puzzle vs. chess. \par 
Industrial robot vs. robot car
}

\subsubsection{{Discrete} vs. {Continuous}}

\nDefinition{\textbf{Discrete} vs. \textbf{Continuous}}{
\bd{Discrete}: percepts, actions and episodes are discrete. There is a finite number of distinct states,actions and episodes(think finite outcomes). \par
Chess vs. robot car.}

\subsubsection{{Single Agent} vs. {Multi-Agent}}

\nDefinition{\textbf{Single Agent} vs. \textbf{Multi-Agent}}{
How many objects must be modelled as agents. Weather agent B has to be modelled as an agent is decided by asking "is B's behaviour best described as maximising a performance measure whose value depends on agent A's behaviour?", 2 taxi's can both be seen as agents trying to drive safely, and a collision will affect both of their peformance measures (\bd{cooperative environment}).

\par Crossword vs. poker.
}

\nDefinition{Examples}{insert table from p.45 here}

% DAY - THURSDAY %
% LECTURE 2 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Searching}
\subsection{Problem-solving agents}
\nDefinition{Formulate-Search-Execute agents}{type of goal-based agents which have a one goal which needs to be achieved with a search of some sort. \par
\bd{Action Sequence} - the result of a search, a series of actions required to reach the goal.\par 
\bd{problem} - the model of the problem over which the action sequence is defined \par 
When a problem-solving-agent has nothing in the action sequence it searches for one, and if it has one it follows it (think football)}
\subsection{Problem types}
\subsubsection{Determnistic, fully observable}
\nDefinition{Determnistic, fully observable}{Agent knows exactly which state it will be in; solution is a sequence}
\subsubsection{Non-observable}
\nDefinition{Non-observable}{Agent may have no idea where it is; solution is a sequence}
\subsubsection{Nondeterministic and/or partially observable}
\nDefinition{Nondeterministic and/or partially observable}{contingency problem - percepts provide new information about current state \par
often interleave search, execution}
\subsubsection{Unknown state space}
\nDefinition{Unknown state space}{exploration problem}

\subsection{Problem formulation}
\subsubsection{Problem Definition}
\nDefinition{Problem Definition}{A problem is defined by four items:\par 
\bd{initial state} - where do we start\par 
\bd{actions or successor function} - set of action-state pairs\par
\bd{goal test} - explicit/implicit (in Bucharest or Checkmate(x))\par
\bd{path cost} - the cost of each action in each state\par
\bd{a solution} - sequence of actions leading from the initial state go a goal state}
\subsubsection{Choosing a State Space}
\nDefinition{Choosing a State Space}
{
Real world is absurdly complex - state space must be \empg{abstracted} for problem solving.\par
For guaranteed relizability, \bd{any} real state must get to some real state.\par
Each abstract action should be "easier" than the original problem.\par
\bd{(Abstract) States} - set of real states\par
\bd{(Abstract) Actions} - complex combination of real actions which can be performed from each state/par
}

\subsection{Example problems}
\subsubsection{The 8-puzzle}
\nDefinition{8-puzzle}{
\<diagram\>\par
\bd{states?} locations of tiles\par
\bd{actions} move blank left, right, up, down \par
\bd{goal test?} = goal state(explicit)\par
\bd{path cost?} 1 per move}
\subsubsection{Robotic Assembly}
\nDefinition{Robotic Assembly}{
\<diagram\>\par
\bd{states?} real-valued coordinates of robot joint angles \& parts of the object to be assembled \par
\bd{actions} continuous motions of robot joints\par
\bd{goal test?} complete assembly\par
\bd{path cost?} time to execute
}
\subsection{Basic search algorithms}
\subsubsection{Tree search algorithm}
\nDefinition{}{basic idea: offline, simulated exploration of state space by generating successors of already-explored states (a.k.a expanding states) (think depth-first search)}
\nDefinition{States vs. Nodes}{
A state is a representation of a physical configuration \par
A node is a book-keeping data structure constituting part of a \bd{search tree} includes state, parent node, action, path cost}
% DAY - FRIDAY %
% LECTURE 2 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Tree Search Strategies}
\nDefinition{Definition}{A \bd{search strategy} is defined by picking the order of node expansion - the order in which we pick nodes from the frontier \par
Strategies are evaluated along the following dimensions:\par 
\bd{completeness} - does it always find a solution if one exists? \par
\bd{time complexity} - number of nodes generated \par
\bd{space complexity} - maximum number of nodes in memory \par
\bd{optimality} - does it always finda a least-cost solution? \par
time and space complexity are measured in terms of:\par
\bd{b}: maximum branching factor of a search tree\par
\bd{d}: depth of the least-cost solution\par
\bd{m}: maximum depth of the state space(may be \infty, think loops)}
\subsection{Uninformed tree search strategies}
\nDefinition{Definition}{use only information in problem definition}
\subsubsection{Repeated states Problem}
\nDefinition{Repeated states}{failure to detect repeated states can turn a \bd{linear} problem into an \bd{exponential} one!}
\subsubsection{Graph search}
\nDefinition{Graph Search}{it introduces a set of \bd{explored nodes}, it adds nodes to the frontier set only if they are not in the frontier or the explored set}
\subsubsection{Breadth-first search}
\nDefinition{Breadth-first search}{Expand shallowest unexplored node first. frontier is a FIFO queue, i.e. new successors go at end\par
\bd{complete?} Yes (if b is finite)\par 
\bd{Time?} $b+b^{2} + b^{3} + \hdots + b^{d} = O(b^{d})$ worst case b-ary tree of depth d (not m)\par 
\bd{Space?} $O(b^{d})$ (keeps every node in memory) \par 
\bd{optimal?} Yes (if cost = 1 per step, then a solution is optimal if it is closest to start node)}
\subsubsection{Depth-first Search}
\nDefinition{}{
Expand deepest unexplored node, frontier = LIFO queue, i.e., put successors at front\par
\bd{complete?} No: fails in infinite-depth spaces, spaces with loops - modify to avoid repeated nodes. Complete in finite spaces\par
\bd{Time?} $O(b^{m})$ terrible if m is much larger than d, if solutions are dense, then may be much faster than breadth-first\par 
\bd{Space?} $O(bm)$ i.e. linear spac!\par 
\bd{optimal?} No 
}

\subsubsection{Depth-limited search}
\nDefinition{}{
This is depth-first search with a \bd{depth limit l}, i.e., nodes at depth l have no ancestors\par
\bd{complete?}\par 
\bd{Time?}\par 
\bd{Space?}\par 
\bd{optimal?} 
}
\subsubsection{Iterative deepening search}
\nDefinition{}{Depth-limited search but done over a variety of l-values\par
\bd{complete?} Yes\par 
\bd{Time?} $(d)b + (d-1)b^2 + \hdots + (1)b(^{d})$\par 
\bd{Space?}$O(bd)$\par 
\bd{optimal?} Yes, if step cost = 1\par 
11\% overhead over for repeated operations
}
\subsubsection{Uniform cost search}
%HOMEWORK%
\subsubsection{Summary of algorithms}
\nDefinition{}{
\begin{tabular}{c|c|c|c|c|c}
    Criterion & Breadth-first & Uniform-cost & Depth-first & Depth-limited & Iterative Deepening\\\hline
    Complete? & Yes & Yes & No & No & Yes\\\hline
    Time O(b^{d})& & & & &\\
    Space & & & & &\\ 
    Optimal & Yes & Yes & No & No & Yes
\end{tabular}}
\end{document}