\documentclass{article}

\usepackage{Custom_Latex/Summary_Notes/notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{tabularx}
\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}
\title{Reasoning \& Agents - Summary Notes}
\author{Maksymilian Mozolewski}
\maketitle
\pagebreak
\tableofcontents
\pagebreak

% WEEK 1 %
% TUTORIAL - NONE %

% DAY - TUESDAY %
% LECTURE - 1 %
% READING - DONE %
% NOTES_COMPLETE - 80% %
\section{Introduction}
\subsection{Book}
\paragraph{Artificial Intelligence: A Modern Approach}

\section{Intelligent Agents}

\subsection{Agents}

\nDefinition{Agent}{\bd{Perceives} its \bd{environment}, through its \bd{sensors}, then achieves its \bd{goals}, by acting on its environment via \bd{actuators} \par
\bd{Rational Agent} - always makes decisions which maximize its \bd{performance measure}, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has. A `rational` agent doesn't necessarily pick the best outcome, but the best outcome given its state/percepts etc. which might involve \bd{information-gathering} or \bd{exploration}, and \bd{learning on past experiences} (crossing the road requires a look around!/a roomba which forgets where the kitchen is is not rational) \par
\bd{omniscence} - an omniscent agent knows the actual outcome of its actions and can act accordingly, very different to a rational agent.\par
\bd{agent function} - maps any given \bd{percept} sequence to an action (abstract mathematical concept, basically a big table) \par
the process of writing an agent function is called \bd{tabulating} \par
\bd{agent program} - concrete implementation of the agent function running within some physical system \par
\bd{autonomy} - an autonomous agent relies more on its own percepts rather than initial knowledge from the designer}
\nDefinition{Percept}{An agent's perceptual inputs at any given instant. An agent's \bd{percept sequence} is the complete history of everything the agent has ever perceived \par f}
\nDefinition{Task Environment}{
PEAS - \bd{P}erformance measure, \bd{E}nvironment, \bd{A}ctuators, \bd{S}ensors}
\nDefinition{Examples}{
\small{
\begin{tabularx}{\textwidth}{ X | X | l | X | X }
    Agent Type & Performance Measure & Environment & Actuators & Sensors \\\hline
    Medical diagnosis system & Healthy patient, reduced costs & Patient,hospital,staff & Display of questions, tests, diagnoses, treatments, referrals & Keyboard entry of symptoms, findings, patient's answers\\\hline
    Satellite image analysis system & Correct image categorization & Downlink from orbiting satellite & Display of scene categorisation & Color pixel arrays\\\hline
    Part-picking robot & Percentage of parts in correct bins & Conveyor belt with parts; bins & jointed arm and hand & Camera, joint angle sensors \\\hline
    Refinery controller & Purity, yield, safety & Refinery, operators & Valves, pumps, heaters, displays & Temperature, pressure, chemical sensors\\\hline
    Interactive English tutor & Student's score on test & Set of students, testing agency & Display of exercises suggestions corrections & Keyboard entry\\\hline
\end{tabularx}
}}

\subsubsection{Simple Reflex Agents}

\nDefinition{Simple Reflex Agents}{
action depends only on \emph{current} percepts(they are the state), implement by condition-action rules, e.g.:\par
\bd{if} car-in-front-is-braking \bd{then} initiate-braking. \par
work well in fully-observable environments. Can be made to function better in partially-observable environments with \bd{randomisation}}

\subsubsection{Model-Based Reflex Agents}

\nDefinition{Model-Based Reflex Agents}{
Build on simple reflex agents, by adding persistent \bd{state}, and making decisions based on percept history as well as current percepts.\\
Need to maintain internal world model - needs to understand how it affect the environment with its actions and the `rules` of the world - the \bd{model}.
}

\subsubsection{Goal-Based Agents}

\nDefinition{Goal-Based Agents}{Simple Reflex agents don't really have an explicit 'goal', they have functions telling them what to do at each point in time. Goal-Based-Agents, determine what to do from their goal as well as the model of the environment.}

\subsubsection{Utility-Based Agents}

\nDefinition{Utility-Based Agents}{Agents so far had a single goal, agents may have to juggle conflicting goals. Goal-Based Agents, only consider the binary "Goal reached/Not Reached". Need to optimise utility over a range of goals. \textbf{Utility}: measure of goodness(a real number, internalisation of the Performance score). Combine with probability of success to get expected utility. If the utility function and perofmance measure are in agreement, then the agent will be rational when it maximises its utility(or rather \bd{expected utility})}

\subsubsection{Learning Agents}

Not covered, learn from experience.
\subsection{Environment}

\nDefinition{Environment}{The things an agent has to interact with. An environment can be categorised based on 6 main categories: \bd{D}eterminism, \bd{E}pisodicity, \bd{A}gents, \bd{D}iscreteness, \bd{O}bservabality, \bd{S}taticness : \bd{OS-DEAD}}

\subsubsection{{Fully Observable} vs. {Partially Observable}}

\nDefinition{\textbf{Fully Observable} vs. \textbf{Partially Observable}}{
\bd{full}: agent's sensors describe environment state fully at each point in time. A task environment is \emph{effectively} fully obserbable when the sensors detect all aspects that are \empg{relevant} to the choice of action, relevance depends on performance measure\par 
\bd{Partial}: some parts of environment not visible, perhaps the sensors are noisy.
}

\subsubsection{{Deterministic} vs. {Stochastic}}

\nDefinition{\textbf{Deterministic} vs. \textbf{Stochastic}}
{
\bd{Deterministic}: if the next state of the environment is fully determined by its current state and agent's actions. (maybe a robot in a closed simulation ?) \par
\bd{Stochastic}: even if you know the current state and your agents actions you can't fully predict the new state of the environment with 100\% certainty. (This definition ignores other agents actions!)
}

\subsubsection{{Episodic} vs. {Sequential}}

\nDefinition{\textbf{Episodic} vs. \textbf{Sequential}}{
\bd{Episodic}: the agent's experience is divided into atomic episodes. next action does not depend on previous actions (classification of images?)\par
Mail-sorting robot vs. crossword puzzle.
}

\subsubsection{{Static} vs. {Dynamic}}

\nDefinition{\textbf{Static} vs. \textbf{Dynamic}}{
\bd{Static}: environment unchanged while agent deliberates (freezes). \par
\bd{Semi-dynamic} : while agent deliberates, environment itself doesn't change, while the performance score does. \par
Crossword puzzle vs. chess. \par 
Industrial robot vs. robot car
}

\subsubsection{{Discrete} vs. {Continuous}}

\nDefinition{\textbf{Discrete} vs. \textbf{Continuous}}{
\bd{Discrete}: percepts, actions and episodes are discrete. There is a finite number of distinct states,actions and episodes(think finite outcomes). \par
Chess vs. robot car.}

\subsubsection{{Single Agent} vs. {Multi-Agent}}

\nDefinition{\textbf{Single Agent} vs. \textbf{Multi-Agent}}{
How many objects must be modelled as agents. Weather agent B has to be modelled as an agent is decided by asking "is B's behaviour best described as maximising a performance measure whose value depends on agent A's behaviour?", 2 taxi's can both be seen as agents trying to drive safely, and a collision will affect both of their peformance measures (\bd{cooperative environment}).

\par Crossword vs. poker.
}

\nDefinition{Examples}{insert table from p.45 here}

% DAY - THURSDAY %
% LECTURE 2 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Searching}
\subsection{Problem-solving agents}
\nDefinition{Formulate-Search-Execute agents}{type of goal-based agents which have a one goal which needs to be achieved with a search of some sort. \par
\bd{Action Sequence} - the result of a search, a series of actions required to reach the goal.\par 
\bd{problem} - the model of the problem over which the action sequence is defined \par 
When a problem-solving-agent has nothing in the action sequence it searches for one, and if it has one it follows it (think football)}
\subsection{Problem types}
\subsubsection{Determnistic, fully observable}
\nDefinition{Determnistic, fully observable}{Agent knows exactly which state it will be in; solution is a sequence}
\subsubsection{Non-observable}
\nDefinition{Non-observable}{Agent may have no idea where it is; solution is a sequence}
\subsubsection{Nondeterministic and/or partially observable}
\nDefinition{Nondeterministic and/or partially observable}{contingency problem - percepts provide new information about current state \par
often interleave search, execution}
\subsubsection{Unknown state space}
\nDefinition{Unknown state space}{exploration problem}

\subsection{Problem formulation}
\subsubsection{Problem Definition}
\nDefinition{Problem Definition}{A problem is defined by four items:\par 
\bd{initial state} - where do we start\par 
\bd{actions or successor function} - set of action-state pairs\par
\bd{goal test} - explicit/implicit (in Bucharest or Checkmate(x))\par
\bd{path cost} - the cost of each action in each state\par
\bd{a solution} - sequence of actions leading from the initial state go a goal state}
\subsubsection{Choosing a State Space}
\nDefinition{Choosing a State Space}
{
Real world is absurdly complex - state space must be \empg{abstracted} for problem solving.\par
For guaranteed relizability, \bd{any} real state must get to some real state.\par
Each abstract action should be "easier" than the original problem.\par
\bd{(Abstract) States} - set of real states\par
\bd{(Abstract) Actions} - complex combination of real actions which can be performed from each state/par
}

\subsection{Example problems}
\subsubsection{The 8-puzzle}
\nDefinition{8-puzzle}{
\<diagram\>\par
\bd{states?} locations of tiles\par
\bd{actions} move blank left, right, up, down \par
\bd{goal test?} = goal state(explicit)\par
\bd{path cost?} 1 per move}
\subsubsection{Robotic Assembly}
\nDefinition{Robotic Assembly}{
\<diagram\>\par
\bd{states?} real-valued coordinates of robot joint angles \& parts of the object to be assembled \par
\bd{actions} continuous motions of robot joints\par
\bd{goal test?} complete assembly\par
\bd{path cost?} time to execute
}
\subsection{Basic search algorithms}
\subsubsection{Tree search algorithm}
\nDefinition{}{basic idea: offline, simulated exploration of state space by generating successors of already-explored states (a.k.a expanding states) (think depth-first search)}
\nDefinition{States vs. Nodes}{
A state is a representation of a physical configuration \par
A node is a book-keeping data structure constituting part of a \bd{search tree} includes state, parent node, action, path cost}
% DAY - FRIDAY %
% LECTURE 2 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Tree Search Strategies}
\nDefinition{Definition}{A \bd{search strategy} is defined by picking the order of node expansion - the order in which we pick nodes from the frontier \par
Strategies are evaluated along the following dimensions:\par 
\bd{completeness} - does it always find a solution if one exists? \par
\bd{time complexity} - number of nodes generated \par
\bd{space complexity} - maximum number of nodes in memory \par
\bd{optimality} - does it always find a a least-cost solution? \par
time and space complexity are measured in terms of:\par
\bd{b}: maximum branching factor of a search tree\par
\bd{d}: depth of the least-cost solution\par
\bd{m}: maximum depth of the state space(may be \infty, think loops)}
\subsection{Uninformed tree search strategies}
\nDefinition{Definition}{use only information in problem definition}
\subsubsection{Repeated states Problem}
\nDefinition{Repeated states}{failure to detect repeated states can turn a \bd{linear} problem into an \bd{exponential} one!}
\subsubsection{Graph search}
\nDefinition{Graph Search}{it introduces a set of \bd{explored nodes}, it adds nodes to the frontier set only if they are not in the frontier or the explored set}
\subsubsection{Breadth-first search}
\nDefinition{Breadth-first search}{Expand shallowest unexplored node first. frontier is a FIFO queue, i.e. new successors go at end\par
\bd{complete?} Yes (if b is finite)\par 
\bd{Time?} $b+b^{2} + b^{3} + \hdots + b^{d} = O(b^{d})$ worst case b-ary tree of depth d (not m)\par 
\bd{Space?} $O(b^{d})$ (keeps every node in memory) \par 
\bd{optimal?} Yes (if cost = 1 per step, then a solution is optimal if it is closest to start node)}
\subsubsection{Depth-first Search}
\nDefinition{}{
Expand deepest unexplored node, frontier = LIFO queue, i.e., put successors at front\par
\bd{complete?} No: fails in infinite-depth spaces, spaces with loops - modify to avoid repeated nodes. Complete in finite spaces\par
\bd{Time?} $O(b^{m})$ terrible if m is much larger than d, if solutions are dense, then may be much faster than breadth-first\par 
\bd{Space?} $O(bm)$ i.e. linear spac!\par 
\bd{optimal?} No 
}

\subsubsection{Depth-limited search}
\nDefinition{}{
This is depth-first search with a \bd{depth limit l}, i.e., nodes at depth l have no ancestors\par
\bd{complete?}\par 
\bd{Time?}\par 
\bd{Space?}\par 
\bd{optimal?} 
}
\subsubsection{Iterative deepening search}
\nDefinition{}{Depth-limited search but done over a variety of l-values\par
\bd{complete?} Yes\par 
\bd{Time?} $(d)b + (d-1)b^2 + \hdots + (1)b(^{d})$\par 
\bd{Space?}$O(bd)$\par 
\bd{optimal?} Yes, if step cost = 1\par 
11\% overhead over for repeated operations
}
\subsubsection{Uniform cost search}
%HOMEWORK%
\subsubsection{Summary of algorithms}
\nDefinition{}{
\begin{tabular}{c|c|c|c|c|c}
    Criterion & Breadth-first & Uniform-cost & Depth-first & Depth-limited & Iterative Deepening\\\hline
    Complete? & Yes & Yes & No & No & Yes\\\hline
    Time O(b^{d})& & & & &\\
    Space & & & & &\\ 
    Optimal & Yes & Yes & No & No & Yes
\end{tabular}}
% DAY - TUESDAY %
% LECTURE 3 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Adversarial Search}
\subsection{Games}
\nDefinition{Definition}{We are usually interested in \bd{zero-sum} games of perfect information \par
\begin{itemize}
    \item Deterministic, fully observable
    \item Agents act alternately
    \item Utilities at end of game are equal and opposite
\end{itemize}
"Unpredictable" opponent - specifying a move for every possible opponent reply \par\smallskip
Time limits - unlikely to find goal, must approximate\par
Normal search: optimal decision is a sequence of actions leading ot a goal state (i.e. a winning terminak state)\par
Adversarial search:\par
\begin{itemize}
    \item MIN has a say in game
    \item MAX needs to find a contingent strategy which specifies:\par
    MAX's move in initial state then... MAX's moves in states resulting from every response by MIN to the move then .. MAX's moves in states resulting from every response by MIN to all those moves, etc.
\end{itemize}
}
\subsection{Game Tree}
\nDefinition{Definition}{$<$Min Max diagram$>$}
\nDefinition{Minimax Value}{
definition}
\nDefinition{Minimax example}{
Perfect play for deterministic games\par 
Idea: choose move to position with highest minimax value = best achievable payoff against best play.\par
Example: 2-ply game:
}
\nDefinition{minimax Algorithm}{
$<algorithm>$
\bd{complete?} Yes(if tree is finite)\\
\bd{Optimal?} Yes (against an optimal opponent)\\
\bd{Time complexity?} $O(b^{m})$\\
\bd{Space complexity?} $O(bm)$ (depth first exploration)\\
}
\nDefinition{$\alpha-\beta$ pruning algorithm}{
prune the leaves of the game tree depending on the decisions of its opponent\par 
does not affect final result of the game \par
Good move ordering imporves effectiveness of pruning (How could previous tree be better ?) \par (the order of the leaves' utility matters greatly) \par 
With perfect ordering time complexity: $O(b^{\frac{m}{2}})$\par\bigskip
why alpha and beta pruning ? \par 
alpha is the value of the best choice found so fat at any choice point along the path for MAX\par 
if v is worse than alpha, MAX will avoid it - prune that branch\par 
beta is the same thing for the MIN player\par
$<algorithm>$}
\subsubsection{Resource Limits}
\nDefinition{Definition}{
Suppose we have 100 secs, explore $10^4$ nodes/ sec - $10^6$ nodes per move\par
standard approach: - cutoff test: e.g., depth limit (perhaps add quiescence search, which tries to search interesting positions)}
\nDefinition{Evaluation functions}{
For cheass typically linear weighted sum of features
\begin{equation*}
    EVAL(s) = w_{1}f_{1}(s) + w_{2}f_{2}(s)+ \hdots +w_{n}f_{n}(s) 
\end{equation*}}
\nDefinition{Cutting off search}{
Minimax Cutoff is identical to MinimaxValue except - TERMINAL_TEST is replaced by CUTOFF, UTILITY is replaced by EVAL\par 
does it work in practice? $b^{m} = 10^{6}, b= 35, m = 4$ 
4-ply lookahead is a hopeless chess player}
% DAY - THURSDAY %
% LECTURE 4 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Informed (Heuristic) Search Strategies}
\nDefinition{Definition}{an \bd{informed} strategy - one with problem-specific knowledge beyond the definition of the problem itself.\par
\bd{best-first-search} - instance of the general Tree-Search or Graph-Search algorithm in which a node is selected for expansion based on an \bd{evaluation function}($f(n)$)\par
\bd{heuristic function} ($h(n)$) - estimates the cost of the cheapest path from the given node/state to the goal state/node\par
}
\subsection{Heuristic}
\nDefinition{Definition}{
From the greek word "Heuriskein" meaning "to disover" "to find"\par
A heuristic is any method that is believed or practically proven to be useful for the solution of a given problem.}
\subsection{Greedy best-first search}
\nDefinition{Definition}{tries to expand the node that is 'closest' to the goal in terms, on the grounds this is likely to lead to a solution quickly.\par
so $f(n) = h(n)$\par
\bd{Complete?} No - can get stuck in loops (graph search version is complete in finite space, but not in infinite ones)\par
\bd{Time?} $O(b^{m})$ for tree version, but a good heuristic can give dramatic improvement\par
\bd{Space?} $O(b^{m})$\par
\bd{Optimal?} no}
\subsection{A* search}
\nDefinition{Definition}{
The most widely known form of best-first search. It evaluates nodes by combining $g(n)$, the cost to reach the node, and $h(n)$ the cost to get from the node to the goal - $f(n) = g(n) + h(n)$ e.g. the estimated cost of the cheapest solution through n.\par

\bd{Complete?} Yes(unless there are infinitely many nodes with $f \leq f(G)$)\par
\bd{Time?} Exponential\par
\bd{Space?} Keeps all nodes in memory\par
\bd{Optimal?} Yes 
}
\nTheorem{Optimal A*}{in order to be optimal, the heuristic needs to be:\par
\bd{admissible} - never overestimate the cost to reach the goal.\par
\bd{consistent} - for every node n and every successor $n^{'}$ of n generated by an action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to $n^{'}$ plus the estimated cost of reaching the goal from $n^{'}$
\begin{equation*}
    h(n) \leq c(n,a,n^{'}) + h(n^{'})
\end{equation*}
(\bd{the triangle inequality})\par
$<proof from slides> f(n) < f(G_{2})$
\bd{consistent} - if doing graph-search and the heuristic is consistent, then A* is optimal
}
\subsection{admissible heuristics}
\nDefinition{Definitio}{
8 puzzle example comparing heuristics}
\subsection{Relaxed Problems}
\nDefinition{Defintion}{
A problem with fewer restrictions on the actions is called a \bd{relaxed problem}\par
the cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem.\par
if the rules of the 8-puzzle are relaxed so that a tile can move anywehre then $h_{1}(n)$ gives the shortest solution\par
if the rules are relaxed so that a tile can move to any adjacent square then $h_{2}(n)$ gives the shortest solution}
\section{Logical Agents}
\subsection{Knowledge bases}
\nDefinition{Knowledge Base}{Set of sentences in a formal language,which tell the agent what it needs to know, and the agent can follow the knowledge base for answers\par
KB's can be part of an agent or be accessible to many agents.\par
the agent's KB can be viewed at the knowledge level i.e. what it knows, regardless of how implemented\par
on the implementation level, data structures in KB and algorithms that manipulate them.}
\nDefinition{ knowledge-based agent}{
$<pseudocode>$ the agent must be able to:\par
represent states, actions, etc.\par
incorporate new percepts\par
update internal representations of the world\par
deduce hidden properties of the world \par
deduce appropriate actions}
\nDefinition{WUMPUS world example}{
\bd{Performance measure} - gold+1000, death -1000, -1 per step, -10 for using arrow\par
\bd{Environment} - Squares adjacent to Wumpus are smelly, Squares adjacent to pits are breezy, Glitter iff gold is in the same square, Shooting kills wumpus if you are facing it, shooting uses up the only arrow,grabbing picks up gold if in same square, releasing drops the gold in same square\par
\bd{Actuators} - Left turn, Right turn, Forward, Grab, Release, Shoot \par
\bd{Sensors} - Stench, breeze, Glitter, Bump, Scream}
\subsection{Logic in General}
\nDefinition{Definition}{\bd{Logics} - are formal languages for representing information such taht conclusions can be drawn\par
\bd{Syntax} - defines the sentences in the language\par
\bd{Semantics} - defines the "meaning" of sentences (define truth of a sentence in a world)}
\subsection{Entailment}
\nDefinition{Entailment}{Entailment means that one thing follows from another: $\models$\par
Knowledge base KB entails sentence a if and only if a is true in all worlds where KB is true}
\subsection{Models}
\nDefinition{Definition}{
The models are formally structured worlds with respect to which truth can be evaluated. We say m is a model of a sentence a if a is true in m. M(a) is the set of all models of a \begin{equation*}
    KB \entails a iff M(KB) \subseteq M(a)
\end{equation*}
}
\subsection{Inference}
\nDefinition{Inference}{$KB \inferes_{i} a$ : sentence a can be derived from KB by an a inference procedure i\par
\bd{Soundness} - i is sound if whenever KB \inferes_{i} a it is also true that KB \models a\par
\nd{completeness} i is complete if whenever KB \models a, it is also true that KB \inferes_{i} a}
\subsection{Propositional Logic}
\subsection{Inference bu enumeration}
% DAY - TUESDAY %
% LECTURE 5 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\section{Effective Propositional Inference}
\subsection{Forms}
\subsubsection{CNF}
\nDefinition{CNF}{conjunctive normal form - conjunction of clauses (and of or's)}
\nDefinition{Convert to CNF}{
eliminate bi-implies, eliminate implies, move negations inwards using de Morgan's, apply distributivity law}
\subsubsection{DPLL}
\nDefinition{Definition}{Determine if an input propositional logic sentence (in CNF) is \bd{satisfiable}\par \emph{improvements} over truth table enumeration:\par
Early termination (a clause is true if one of its literals is true, asentence is false if \bd{any} of its clauses is false), Pure symbol heuristic - (\bd{pure symbol} - \bd{always} appears with the same sign or polarity in \bd{all} clauses, make literal containing a pure symbol true (for satisfiability)), Unit clause Heuristic - (only one literal in the clause, e.g.. (A the only literal in a unit clause must be true, also includes clauses where all but one literal is false), tautology deletion (remove clauses where a literal appears both postitively and negatively)}
\subsubsection{WalkSat}
\nDefinition{Definition}{incomplete local search algorithm, Evaluation function: the min-conflict heuristic of minimizing the number of unsatisfied clauses, balance between greediness and randomness\par
checks for satisfiability by randomly flipping the values of variables
}
\subsection{Inference-based agents in the wumpus world}
\nDefinition{Definition}{
A wumpus-world agent using propositional logic, the KB has 64 distinct proposition symbols and 155 sentences. 
$<hybrid wumpus agent>$\par
limitation of propositional logic, explosion of clauses}
\end{document}