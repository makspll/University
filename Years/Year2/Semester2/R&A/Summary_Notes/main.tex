\documentclass{article}

\usepackage{Custom_Latex/Summary_Notes/notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{tabularx}
\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}
\title{Reasoning \& Agents - Summary Notes}
\author{Maksymilian Mozolewski}
\maketitle
\pagebreak
\tableofcontents
\pagebreak

% WEEK 1 %
% TUTORIAL - NONE %

% DAY - TUESDAY %
% LECTURE - 1 %
% READING - DONE %
% NOTES_COMPLETE - 80% %
\section{Introduction}
\subsection{Book}
\paragraph{Artificial Intelligence: A Modern Approach}

\section{Intelligent Agents}

\subsection{Agents}

\nDefinition{Agent}{\bd{Perceives} its \bd{environment}, through its \bd{sensors}, then achieves its \bd{goals}, by acting on its environment via \bd{actuators} \par
\bd{Rational Agent} - always makes decisions which maximize its \bd{performance measure}, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has. A `rational` agent doesn't necessarily pick the best outcome, but the best outcome given its state/percepts etc. which might involve \bd{information-gathering} or \bd{exploration}, and \bd{learning on past experiences} (crossing the road requires a look around!/a roomba which forgets where the kitchen is is not rational) \par
\bd{omniscence} - an omniscent agent knows the actual outcome of its actions and can act accordingly, very different to a rational agent.\par
\bd{agent function} - maps any given \bd{percept} sequence to an action (abstract mathematical concept, basically a big table) \par
the process of writing an agent function is called \bd{tabulating} \par
\bd{agent program} - concrete implementation of the agent function running within some physical system \par
\bd{autonomy} - an autonomous agent relies more on its own percepts rather than initial knowledge from the designer}
\nDefinition{Percept}{An agent's perceptual inputs at any given instant. An agent's \bd{percept sequence} is the complete history of everything the agent has ever perceived \par f}
\nDefinition{Task Environment}{
PEAS - \bd{P}erformance measure, \bd{E}nvironment, \bd{A}ctuators, \bd{S}ensors}
\nDefinition{Examples}{
\small{
\begin{tabularx}{\textwidth}{ X | X | l | X | X }
    Agent Type & Performance Measure & Environment & Actuators & Sensors \\\hline
    Medical diagnosis system & Healthy patient, reduced costs & Patient,hospital,staff & Display of questions, tests, diagnoses, treatments, referrals & Keyboard entry of symptoms, findings, patient's answers\\\hline
    Satellite image analysis system & Correct image categorization & Downlink from orbiting satellite & Display of scene categorisation & Color pixel arrays\\\hline
    Part-picking robot & Percentage of parts in correct bins & Conveyor belt with parts; bins & jointed arm and hand & Camera, joint angle sensors \\\hline
    Refinery controller & Purity, yield, safety & Refinery, operators & Valves, pumps, heaters, displays & Temperature, pressure, chemical sensors\\\hline
    Interactive English tutor & Student's score on test & Set of students, testing agency & Display of exercises suggestions corrections & Keyboard entry\\\hline
\end{tabularx}
}}

\subsubsection{Simple Reflex Agents}

\nDefinition{Simple Reflex Agents}{
action depends only on \emph{current} percepts(they are the state), implement by condition-action rules, e.g.:\par
\bd{if} car-in-front-is-braking \bd{then} initiate-braking. \par
work well in fully-observable environments. Can be made to function better in partially-observable environments with \bd{randomisation}}

\subsubsection{Model-Based Reflex Agents}

\nDefinition{Model-Based Reflex Agents}{
Build on simple reflex agents, by adding persistent \bd{state}, and making decisions based on percept history as well as current percepts.\\
Need to maintain internal world model - needs to understand how it affect the environment with its actions and the `rules` of the world - the \bd{model}.
}

\subsubsection{Goal-Based Agents}

\nDefinition{Goal-Based Agents}{Simple Reflex agents don't really have an explicit 'goal', they have functions telling them what to do at each point in time. Goal-Based-Agents, determine what to do from their goal as well as the model of the environment.}

\subsubsection{Utility-Based Agents}

\nDefinition{Utility-Based Agents}{Agents so far had a single goal, agents may have to juggle conflicting goals. Goal-Based Agents, only consider the binary "Goal reached/Not Reached". Need to optimise utility over a range of goals. \textbf{Utility}: measure of goodness(a real number, internalisation of the Performance score). Combine with probability of success to get expected utility. If the utility function and perofmance measure are in agreement, then the agent will be rational when it maximises its utility(or rather \bd{expected utility})}

\subsubsection{Learning Agents}

Not covered, learn from experience.
\subsection{Environment}

\nDefinition{Environment}{The things an agent has to interact with. An environment can be categorised based on 6 main categories: \bd{D}eterminism, \bd{E}pisodicity, \bd{A}gents, \bd{D}iscreteness, \bd{O}bservabality, \bd{S}taticness : \bd{OS-DEAD}}

\subsubsection{{Fully Observable} vs. {Partially Observable}}

\nDefinition{\textbf{Fully Observable} vs. \textbf{Partially Observable}}{
\bd{full}: agent's sensors describe environment state fully at each point in time. A task environment is \emph{effectively} fully obserbable when the sensors detect all aspects that are \empg{relevant} to the choice of action, relevance depends on performance measure\par 
\bd{Partial}: some parts of environment not visible, perhaps the sensors are noisy.
}

\subsubsection{{Deterministic} vs. {Stochastic}}

\nDefinition{\textbf{Deterministic} vs. \textbf{Stochastic}}
{
\bd{Deterministic}: if the next state of the environment is fully determined by its current state and agent's actions. (maybe a robot in a closed simulation ?) \par
\bd{Stochastic}: even if you know the current state and your agents actions you can't fully predict the new state of the environment with 100\% certainty. (This definition ignores other agents actions!)
}

\subsubsection{{Episodic} vs. {Sequential}}

\nDefinition{\textbf{Episodic} vs. \textbf{Sequential}}{
\bd{Episodic}: the agent's experience is divided into atomic episodes. next action does not depend on previous actions (classification of images?)\par
Mail-sorting robot vs. crossword puzzle.
}

\subsubsection{{Static} vs. {Dynamic}}

\nDefinition{\textbf{Static} vs. \textbf{Dynamic}}{
\bd{Static}: environment unchanged while agent deliberates (freezes). \par
\bd{Semidynamic} : while agent deliberates, environment itself doesn't change, while the performance score does. \par
Crossword puzzle vs. chess. \par 
Industrial robot vs. robot car
}

\subsubsection{{Discrete} vs. {Continuous}}

\nDefinition{\textbf{Discrete} vs. \textbf{Continuous}}{
\bd{Discrete}: percepts, actions and episodes are discrete. There is a finite number of distinct states,actions and episodes(think finite outcomes). \par
Chess vs. robot car.}

\subsubsection{{Single Agent} vs. {Multi-Agent}}

\nDefinition{\textbf{Single Agent} vs. \textbf{Multi-Agent}}{
How many objects must be modelled as agents. Weather agent B has to be modelled as an agent is decided by asking "is B's behaviour best described as maximising a performance measure whose value depends on agent A's behaviour?", 2 taxi's can both be seen as agents trying to drive safely, and a collision will affect both of their peformance measures (\bd{cooperative environment}).

\par Crossword vs. poker.
}

\nDefinition{Examples}{insert table from p.45 here}

% DAY - THURSDAY %
% LECTURE 2 %
% READING - NOT DONE %
% NOTES_COMPLETE -  %
\end{document}