\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{IAML Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{IAML}

\nSection{Introduction}

\nDefinition{Machine Learning}{
    A machine learning model \textbf{takes in} data, \textbf{outputs} predictions. It's a function of data really together with a set of training data.

    Learning = Representation + Evaluation + Optimisation
}
\nSection{Thinking about data}

\nDefinition{Classification}{
    Sort data points into discrete buckets based on training data 
}

\nDefinition{Regression}{
    Output a continuous/real value for each data point based on training data. 
}

\nDefinition{Clustering}{
    Detect which data points are related to which other data points, find outliers.
}

\nDefinition{Data representation}{
    What format do we feed the data in ? Most likely as a \textbf{bag of features}. I.e. collection of attribute-value pairs, every data point must have an attribute-value pair for each property (in most cases)
    
    Data representation has more impact on the performance of your ML algorithm than anything.

    \nHeader{Types of attributes}

    \begin{itemize}[noitemsep]
        \item \textbf{Categorical}
            \subitem- e.g. red/blue/brown
            \subitem- a set of possible \textbf{mutually exclusive} values
            \subitem- meaningful operators: equality comparison
            \subitem- usually represented as numbers 
            \subitem- problems: {\color{red} synnonymy is a major challenge e.g. some values might mean the same thing to a human but not to the machine (country == folk?)}   
        \item \textbf{Ordinal}
            \subitem- e.g. poor $<$ satisfactory $<$ good $<$ excellent
            \subitem- a set of possible \textbf{mutually exclusive} values, but with a \textbf{natural ordering}
            \subitem- meaningful operators: equality comparison, sorting
            \subitem- problems: {\color{red} sometimes hard to differentiate from categorical (single $<$ divorced)?}    
        \item \textbf{Numeric}
            \subitem- e.g. 3.1/5
            \subitem- meaningful operators: arithmetic, distance metrics, equality, sorting 
            \subitem- problems:
                \subsubitem- {\color{red} sensitive to extreme outliers (handle these \textbf{before normalization})}
                \subsubitem- {\color{red} skewed distributions (assymetric) - outliers might actually be real data (e.g. personal wealth data)}
                \subsubitem- {\color{red} Non-monotonic effect of attributes - e.g. predicting someone is going to win a marathon, here the relationship is not monotonic i.e. norect correlatio din, might be a curve with a "sweet spot"}
            \subitem- solutions:
                \subsubitem- \textit{Deal with outliers, maybe trim them for training phase only?}
                \subsubitem- \textit{use a log/atan scale to make data more linear}
                \subsubitem- \textit{discretize data into buckets}
    \end{itemize}
}
\nDefinition{Normalisation}{
    Normalization is the process of converting all the data such that each different attribute is roughly in the same range, and comparable.

    Normalization is mostly necessary for linear methods.
}


\nDefinition{Picking attributes}{
    We want:
    \begin{itemize}[noitemsep]
        \item all our attributes to have similar values if the data points that posses them are similar themselves!    
        \item small change in input $\rightarrow$ small change in values
    \end{itemize} 
    
    \nHeader{Images}

    For images, can we use pixel data as attributes directly ? it depends, if the pixel 20,20 always corresponds to the middle of a letter then yes, this is a meaningful attribute which might help us discern whether digits given have strokes going through the middle of the image!
    What if the pixel is always something random ? This could happen whenever we are looking for an object which can be anywhere in the image, in which case the attribute would be gibberish!
    In the case of image classification, we want attributes which are:
    \begin{itemize}
        \item invariant to size 
        \item invariant to rotation 
        \item invariant to illumination
    \end{itemize}
    
    How can we classify whether an image contains a desired object ? In general we do the following:
    \begin{itemize}
        \item \textbf{Segment} the image, into regions of pixels which we believe are related (by colour, texture, position etc..)
        \item Pick a number of attributes which you will use to describe each of the regions
        \item Then use those features in your machine learning algorithm!
    \end{itemize}
    Keep in mind though, the segmentation \textbf{will} make errors, we can hope that these will be consistent across all images (all images will have their legs chopped off). Sometimes we can segment using a grid
    
    \nHeader{Text}
        For textual data, often we can use the \textbf{bag-of-words} approach. I.e. we can form an attribute vector which counts the amount of occurrences of each word, regardless of position. This is invariant to word shuffling for example.
    \nHeader{Sound}
    For sound, the data is sound waves. How can we select attributes here? We can count the number of different frequencies occurring in the piece, (using Fourier's transform) and treat this as a feature vector!
    \begin{center}
        \nImg{attributes_sound}
    \end{center}
}



\nDefinition{Supervised Learning}{
    Supervised learning algorithms have some sort of "performance" metric they can use, i.e. test labels they can validate their guesses on.
    When the algorithm can measure accuracy directly it's a supervised algorithm.
}

\nDefinition{Unsupervised Learning}{
    Learning without a specific accuracy measure available. Algorithms in this area usually look for structure/patterns/information in the data which can be helpful in other ways.
    There is nothing specific the algorithm is looking for.
    Can be \textbf{direct} when the algorithm helps to make sense of the data directly, or \textbf{indirect} when it is "plugged" into another machine learning aglorithm as an attribute itself. 
}

\nDefinition{Semi-supervised Learning}{
    Using unsupervised methods to improve supervised algorithms. Usually have a few labelled examples but lots more unlabelled. 
}

\nDefinition{Multi-class classification}{
    Classification with multiple mutually exclusive labels/classes.
    
    Might be hard to tell when something belongs to none of the available classes.
}
\nDefinition{Binary classification}{
    Classification with 2 mutually exclusive labels/classes in each "run". 
    This way of classification can be applied to multiple-classes classification but with a "One-vs-Rest" meta-strategy (a vs not a, b vs not b).
    In this way a sample may belong to multiple classes but never to two sides of the one-vs-rest structure simultaneously in each run.
    
    In this classification method we can actually tell when something doesn't belong to any class!
}

\nDefinition{Analysing data}{
    We have to check for a number of things in our data sets:
    \begin{itemize}[noitemsep]
        \item Are there any dominating classes ? what would the best "dummy" model do ? always predicting no ?
        \item What should we use as the appropriate error metric ? How important are the false positives vs the false negatives ? 
    \end{itemize}
}

\nDefinition{Generative model}{
    A generative model, develops a probabilistic model of each class, i.e. tries to "model" the underlying probability distribution directly.
    The decision boundary becomes implicitly defined by the probabilities of each input being in each class.
}

\nDefinition{Discriminative model}{
    A discriminative model ignores the underlying model and tries to "separate" the data, i.e. it tries to model the boundaries that divide the classes.
    \textbf{Not designed to use unlabeled data} so cannot be used for unsupervised tasks.
}

\nDefinition{Dealing with data structure}{
    What do we do if the data input has some sort of hierarchical structure ?  Where the position of occurrence of a node affects its meaning?
    We can encode as attributes the existence of root-to-leaf paths in the entire tree, and use this bag-of-words approach to perform machine learning 
    \begin{center}
        \nImg{structure_input}
    \end{center}

    What if we need to predict the output structure from the input structure ? This is very difficult, but we can "trick" our classifier and turn this more into a search problem by embedding the possible outputs with each input and classifying on that instead:

    \begin{center}
        \nImg[0.9]{structure_output}
    \end{center}
    This of course means we have to search for all possible output structures!

    }

\nDefinition{Dealing with outliers}{
    \textbf{Outliers} are isolated instances of a class that are unlike any other instance of that class. These affect all learning models to some degree.

    There are some ways we can deal with outliers. One method is to remove the outliers just before we perform any sort of normalisation on the data, (ONLY FOR THE PURPOSES OF TRAINING!!)
    We can also put a confidence interval around our data, and removing values outside of those intervals (with x,y values outside of a normal range).
    Some data points might still be outliers even though they are within expected x,y ranges! (second figure)
    
    \begin{center}
        \nImg{outliers}    
    \end{center}

    Best way to deal with outliers ? \textbf{VISUALISE YOUR DATA}
    }



\nSection{Naive Bayes}

\nTheorem{Bayes rule}{
    \begin{equation}
        P(y|x) = \frac{P(x|y) P(y)}{\sum_{y'} P(x|y')P(y')}
    \end{equation}

    where:
    \begin{itemize}[noitemsep]
        \item y : one of the classes
        \item x : the input data, feature vector
        \item P(y) : \textbf{prior}, the probability of seeing elements of class y in general prior to making any observations
        \item $P(x|y)$ : \textbf{class model/likelihood}, given the data is in class y, how likely are the features that i am seeing
        \item P(x) = $\sum_{y'}P(x|y')P(y')$ : \textbf{normalizer}, does not affect the probabilities, but without this term we cannot compare data in terms of probability of being in each class (i.e. for confidence values)
    \end{itemize}

    \textit{Note: this works with probability densities too (which we're using almost exclusively)}
}

\nDefinition{Naive bayes}{
    Bayesian classifier, which assumes \textbf{conditional independence} between different attributes. It attempts to model the underlying probability distribution so it's a \textbf{generative} model.

    I.e.:
    \begin{equation}
        P(\vec{x}|y) = \prod_{i=1}^{d} P(x_{i}|x_{1} \hdots x_{i-1},y)=  P(x_{1}|y) \cdot P(x_{2}|y) \cdot P(x_{3}|y) \hdots
    \end{equation}
    It assumes that there are no correlations between the variables themselves, and any correlations are explained by the fact they belong to the same class.

    \nHeader{Example}
    The probability of going to the beach and getting a heat stroke are not independent: $P(B,S) > P(B)P(S)$.
    The may be independent if we know the weather is hot (i.e. external factor is what actually causes the dependence, which we can factor into the equation):
    $P(B,S|H) = P(B|H)P(S|H)$ This hidden factor in naive bayes is \textbf{the class}
}

\nDefinition{Gaussian Naive bayes}{
    A Naive Bayes classifier which uses the gaussian distribution as a class model for each class.

    \begin{equation}
        \LARGE
        p(x|y) = \frac{1}{\sqrt{2\pi \sigma_{x,y} ^{2}}}\exp^{-\frac{1}{2} \frac{(x - \mu_{x,y})^{2}}{\sigma_{x,y}^{2}}}
    \end{equation}

    With:
    \begin{itemize}[noitemsep]
        \item $\mu_{x,y}$ : the mean of the gaussian modelling class y's attribute x
        \item $\sigma_{x,y}^{2}$ : variance of the gaussian modelling class y's attribute x
        \item $x$ : the value of the attribute
        \item $y$ : the class 
    \end{itemize}
    \textit{Note: this is not actually a probability per-se, this is a p.d.f function which can be higher than 1}
    }

\nDefinition{Problems with Naive Bayes}{
    \nHeader{Covariance}
    Naive bayes cannot model covariance of the attributes, i.e.:
    \begin{center}
        \nImg{covariance_nb}
    \end{center}
    in the above case the only thing differentiating the classes is their relationship between x and y, i.e. the two attributes. Naive bayes can only model 
    \textbf{spherical} distributions
    
    \nHeader{Zero frequency problem}
        Should any data never appear under any of the classes the probability of it belonging to that class is always going to be zero.
        So say we are classifying emails as spam or non-spam, and the word "now" never occurred in non-spam emails, but had occurred in some spam emails. Any sentence containing the word "now" would always be classified as spam no matter the actual probability of it being spam!

        Solution, add a small epsilon to all "feature counts", or perform laplace smoothing: 
        \begin{equation}
            P(w,c) = \frac{num(w,c) + \epsilon}{num(c) + 2\epsilon}
        \end{equation}
        
        Zipf's law: 50\% of words occur once 
        \nHeader{Independence assumption}

        Continuing with the spam example, every word contributes to the total probability independently, easy to fool. Simply stuff lots of non-spammy words into email
    
        }

\nDefinition{Deaing with missing data}{

    Suppose we have missing data for some attribute i of a sample x. How can we compute the likelihood of this sample belonging to any class ? 

    We simply ignore it:
    \begin{equation}
        P(x_{1}\hdots x_{i} \hdots x_{d}|y) = \prod_{k \neq i}^{d} P(x_{i}|y)
    \end{equation}

    There is no need to fill in the gaps artificially because missing attribute data essentially has a probability of 1: 
    
    \begin{center}
        \nImg[0.8]{nb_missing_data}
    \end{center}
}


\nSection{Decision Trees}

\nDefinition{Method}{
    Decision trees split the data based on the label of each data point, in a way which maximizes the entropy of each split. This means we are splitting on attributes which have the most "discriminatory" power at each step
\begin{center}
    \nImg{dt_example}
\end{center}
    We classify each new sample by following the splits in the tree untill we hit a pure subset (or non-pure), and select the label based on the most frequent label in that leaf. 
}

\nDefinition{Quinlan's ID3 Algorithm}{
    \begin{itemize}[noitemsep]
        \item A <- best attribute for splitting  (\textbf{needs a heuristic})
        \item Decision attribute for this node <- A 
        \item For each value of A, create new child node
        \item Split training data at this node into child nodes
        \item For each child node/subset 
            \subitem if subset is \textbf{pure}, stop
            \subitem otherwise repeat on this subnode  
    \end{itemize}
}

\nDefinition{Purity measures}{
    In order to pick the best attribute we need a purity measure
    
    Even splits do not give us any information, we want ones biased towards the +'ve or -'ve labels

    i.e. the purity for a pure set (4y/0n) is 100\%
    but the purity for an impure set (3y/3n) is 50\% (uncertain)
    
    \nHeader{Entropy}

    \begin{equation}
        H(S) = -p_{+}\log_{2}p_{+} -p_{-}\log_{2}p_{-}
    \end{equation}

    where: 
    \begin{itemize}[noitemsep]
        \item S is the subset of training examples
        \item p's refer to the proportion of positive and negatives in the subset S
    \end{itemize}

    \begin{center}
        \nImg{entropy}
    \end{center}

    we interpret this measure as the number of bits of information needed to tell if a random item in S is a + or a -, we dont need any information at all if the subset is entirely composed of +'s or -'s, and all the information if it's evenly split
}

\nDefinition{Information gain}{
    once we have a measure of purity, we can establish the heuristic which will define which split will be the best:

    \begin{equation}
        Gain(S,A) = H(S) - \sum_{V\in Values(A)}\frac{|S_{v}|}{|S|}H(S_{v})
    \end{equation}

    where:
    \begin{itemize}[noitemsep]
        \item \textbf{S} : subset of data points
        \item \textbf{A} : attribute we are splitting on
        \item \textbf{V} : possible values of attribute A
        \item \textbf{$S_{v}$} : subset of S with data points which have label v  
    \end{itemize}

    This measures the "information" we gain by performing the split. 
    Notice how we take a weighted average (weighted by the number of data points) of the entropy of each new subset formed in the split. 
    If the entropy in the new subsets is on average higher than the entropy in the original subset, 
    we have lost information (note this is not possible in this scenario) and if it's lower, we have indeed gained information. The split which maximises this quantity is the best.
    
    \textit{
    Note: that picking the split on this criterion, implies that no attribute will be picked twice (since if we split on it once, all the subsets in nodes below will have the same value for this attribute)}
    \nHeader{Gain ratio}

    Using this heuristic will lead the algorithm to pick splits which lead to more subsets naturally (if allowed by shape of attributes and data)
    We can use the entropy of the split itself to counteract this instead:
    \begin{equation}
        \begin{aligned}
            &SplitEntropy(S,A) = - \sum_{V \in Values(A)} \frac{|S_{v}|}{|S|}log \frac{|S_{v}|}{|S|}\\
            &GainRatio(S,A) = \frac{Gain(S,A)}{SplitEntropy(S,A)}
        \end{aligned}
    \end{equation}

    not how the gain will be lower when we have too many or too few subsets, 
    the split ratio will be the highest with higher information gains which result in 2 subsets that each take half of the data. 
    }

\nDefinition{Overfitting}{
    Overfitting happens when the model learns the "noise" in the training data, and becomes too specific.

    Decision trees will always classify training examples perfectly if we make them big enough (because eventually we will split the data into singletons, and those instances will always be labelled as the correct instances)

    This is an example of \textbf{overfitting}, we need to limit the size of the tree in order to counteract this, i.e. force some subsets to end up being non-pure in order to stop the model from becoming too specific to our training data.


    \begin{center}
        \nImg[0.6]{dt_overfitting}
    \end{center}

    \nHeader{Solutions}

    \begin{itemize}
        \item Stop splitting when not statistically significant
        \item Grow the tree fully and then prune in a way which maximises the accuracy (using a validation set)
            \subitem- \textbf{Sub-tree replacement pruning}: pretend you remove a node and all its children, measure the performance on validation set. Remove node with highest performance gain, repeat untill removal is harmful to performance(greedy)
    \end{itemize}
}

\nDefinition{Numeric attributes}{
    How can we apply this to numeric attributes? Split based on \textbf{thresholds}, threshold can be picked via some process.
    The easy solution is to take all the values for the attribute we are evaluating a split on, sort them and go through the split on each of those one-by-one.

    \textit{Note: this will lead to one attribute possibly being split multiple times since we can always divide the values into smaller subsets}
}

\nDefinition{Multi-class classification}{
    We can apply decision trees to multi-class classification by predicting the most frequent class in the subset we land on. 
    The entropy calculation becomes:
    \begin{equation}
        H(S) = - \sum_{c} p_{c} \log_{2}p_{c}
    \end{equation}

    where $p_{c}$ is the fraction of samples of class c in the subset
}
\nDefinition{Regression}{
    We can also apply this algorithm to regression by taking the average of the training examples in the subset we land on as the predicted output value.
    In which case instead of maximising gain we grow the tree by minimising variance in the subsets.
    (pick split which reduces variance in the output the most!)
    We can also replace the average by linear regression at the leaves.

}

\nDefinition{Random decision forest}{
    \begin{itemize}[noitemsep]
        \item Pick a random subset $S_{r}$ of training examples 
        \item grow a full tree $T_{r}$ (without pruning)
        \item when splitting pick from $d << D$ random attributes (i.e. hide some attributes for some trees)
        \item compute gain based on $S_{r}$ instead of full set
        \item repeat K times to make K trees
    \end{itemize}

    Given a new data point X we can then:
    \begin{itemize}[noitemsep]
        \item classify X using each of the trees $T_{1}\hdots T_{k}$
        \item use majority vote: class predicted most often as the label!
    \end{itemize}

    \textit{\color{green} State-of-the-art performance in many domains, very good stuff}
}
\nDefinition{Evaluation of decision trees}{
    \begin{itemize}[noitemsep]
        \item[+] \textbf{interpretable} - humans can understand the underlying decisions! (ONLY ALGORITHM IN THIS COURSE TO HAVE THIS PROPERTY)
        \item[+] easilly handles irrelevant attributes (gain = 0)
        \item[+] can handle missing data 
        \item[+] very compact: nodes $<<$ D after pruning
        \item[+] very fast at testing time O(depth)
        \item[-] only axis-aligned splits are possible (because we split only taking into account one dimension at a step)
            \nImg[0.4]{dt_axis_aligned} 
        \item[-] greedy (may not find best tree globally)
        \item[-] exponentially many possible trees       
    \end{itemize}

    \nHeader{Why trees?}
    Can model non-linear, weirdly shaped data which does not really follow a nice probability distribution (i.e. \textbf{non-monotonic} data!)
    \begin{center}
        \nImg{nb_vs_dt}
    \end{center}
}

\nSection{Generalisation \& Evaluation}

\nDefinition{Over \& Under fitting}{
    \nHeader{Overfitting}
    If we can find a model which makes more mistakes on the training data but fewer mistakes on unseen future data, our model has overfitted.

    This can happen for many reasons:
    \begin{itemize}[noitemsep]
        \item Predictor is too "complex" (too flexible)
        \item It fits to the "noise" in the training data
        \item Captures patterns which will not-re appear in future data
    \end{itemize}

    \nHeader{Underfitting}

    If we can find another predictor with smaller training error and smaller error on future unseen data, our model has underfitted.

    This can happen for many reasons:
    \begin{itemize}[noitemsep]
        \item Predictor is too simplistic (too rigid)
        \item Not powerful enough to capture salient patterns in data 
    \end{itemize}

    \nHeader{Examples}

    \begin{center}
        \nImg[0.7]{under_over_fitting}
    \end{center}
}

\nDefinition{Flexible vs Inflexible}{
    Every dataset requires a different level of flexibility. How much depends on the complexity of the task and the available data. We need a "knob" to vary the "flexibility" of our predictor.

    Most learning algorithms have such knobs:
    \begin{itemize}[noitemsep]
        \item Regression: order of the polynomial 
        \item Naive Bayes: number of attributes, limits on model parameters
        \item Decision Trees: \#nodes in the tree / prunning confidence
        \item kNN: number of nearest neighbours
        \item Support Vector Machines: kernel type, cost parameter  
    \end{itemize}

    With a small amount of training data we require more rigit models (less chance for overfitting).
    For simpler tasks we also pick more rigid models. For complex tasks we want a more flexible/complex model
}

\nDefinition{Training vs Generalization error}{
    The training error can be defined as the average error over the training data (error here can be any error metric):
    \begin{equation}
        E_{train} = \frac{1}{n}\sum_{i=1}^{n} error(f_{D}(\mathbf{X_{i}}),y_{i})
    \end{equation}
    The generalization error however, is the general error on future data.
    Problem is we don't know what the future data will be, we do however know the possible range of input data.

    Generalization error is defined as follows:
    \begin{equation}
        E_{gen} = \int error(f_{D}(\mathbf{X}),y)p(y,\mathbf{X}) \; d\mathbf{x}
    \end{equation}

    i.e. the sum over all possible input values of the error multiplied by the probability of seeing each of those input values given the class.

    We can never compute this as we do not know $p(y,\mathbf{X})$, since that is what we're trying to model!

    We can however estimate the generalization error, by treating the \textbf{testing} error as an estimate of it! (NOTE NOT THE TRAINING ERROR, NEVER USE THE TRAINING ERROR AS YOUR ACCURACY)
    
}

\nDefinition{Estimating Generalization error}{
    The testing error can be used to estimate generalization error:
    \begin{equation}
        E_{test} = \frac{1}{n}\sum_{i=1}^{n} error(f_{D}(\mathbf{X_{i}}),y_{i})
    \end{equation}

    If the testing set is an \textbf{unbiased} sample from the population of input data, i.e. unbiased sample of input values, then the following holds:
    \begin{equation}
        \lim_{n\rightarrow \infty}E_{test} = E_{gen}
    \end{equation}

    I.e. as our sample size grows to infinity, our testing error converges onto the generalization error!
    How close are the two together ? this depends on n

    \nHeader{Confidence interval}

    We can never actually gain the value for $E_{gen}$ but we can estimate it using a previously unseen sample of the data, i.e. the test set.
    This estimate error $E_{test}$ is going to be within some interval of $E_{gen}$.
    We can try to estimate how close to the real error we are using a confidence interval.

    We can think of the error we measure as a sample from a distribution of all possible errors centered around the true error. From there we want to find an interval around the measured error describing our certainty of how close we are to the true error: 
    \begin{equation}
        E_{test} \pm \Delta E_{gen}
    \end{equation}

    $E_{gen} = E$ here is the probability that our system will misclassify a random instance (i.e. true error)

    The confidence interval with p confidence is the range of values your error will be within p\% of the time
    \begin{equation}
        \begin{aligned}
            &CI = E \pm \sigma \cdot \phi^{-1} \left( \frac{1-p}{2} \right)\\
            &\phi^{-1}(x) = \sqrt{2} erf^{-1}(-p)
        \end{aligned}
    \end{equation}
    With phi calculating the number of standard deviations of the normal distribution which covers p\% of the future test sets.

    we can now:
    \begin{itemize}[noitemsep]
        \item take a random set of n instances, how many will be miss-classified ?
        \item flip E-biased coin n times, how many heads will we get ?
        \item this is the binomial distribution with mean = nE, variance = n E (1-E)
        \item $E_{future} = \# misclassified / n$, this can be approximated with a gaussian: with mean E, variance E(1-E)/n
        \item $\frac{2}{3}$ future tests will have error in $E \pm \sqrt{E(1-E)/n} = \sigma^{2}$
        \item Assuming E = 0.25 the confidence interval for future error will then be:
            \subitem- for n=100 examples, p=0.95: 
                \begin{equation}
                    \begin{aligned}
                        &CI = 0.25 \pm 1.96 \cdot \sigma = 0.25 \pm 0.08
                    \end{aligned}
                \end{equation}   
            \subitem- for n=1000 samples, p = 0.99
            \begin{equation}
                CI = 0.25 \pm 0.11 
            \end{equation}
            \subitem- for n=10000, p=0.95
            \begin{equation}
                CI = 0.25 \pm 0.008
            \end{equation}
    \end{itemize}

    \textit{Note: the confidence interval varies roughly with the square root of n (if p is constant)}
    }

\nDefinition{Data set splits}{
    The test + training set split may not be enough, we also need a validation set to pick the right algorithm and "fine-tune" its parameters:
    \begin{itemize}[noitemsep]
        \item Training set: construct basic classifier (Naive Bayes: count frequencies, DT: pick attributes to split on)
        \item Validation set: pick the algorithm + fine-tune settings
            \subitem- select best-performing algorithm
        \item Testing set: Estimate future error rate. Cannot be same as validation set, since the algorithm would overfit to the validation set after working out the parameters!
            \subitem- {\color{red} never report best of many runs, run once, or report results of every run} 
    \end{itemize}
}

\nDefinition{Holdout method}{
    The most natural way to split up your data set is to pick a fraction of it as the training set, and the rest as the test/validation set, this is known as the \textbf{holdout} method
}

\nDefinition{K-fold Cross-validation}{
    \textbf{Cross-validation} is a technique for dealing with small data set sizes. 
    Ideally we want to balance many goals:
    \begin{itemize}[noitemsep]
        \item We want to estimate future error as accurately as possible - keep $n_{train}$ as big as possible
        \item We want to learn the classifier as accurately as possible - keep $n_{test}$ as big as possible 
    \end{itemize}

    But both the training and test sets cannot overlap! 
    We can however, split and alternate the sets in such a way to cover more ground:
    \begin{itemize}[noitemsep]
        \item Split dadta into k sets
        \item test on each k in turn, while training on k-1 others 
        \item average the result over k folds (why can we do that? because we never use one instance as both training/testing instance at the same time, also we are not averaging the performance of one model over number of runs, we build k different models!) 
    \end{itemize}
    \textit{Note: while we call the sets test/train, we can use them for whatever purposes, the test set is actually likely going to be our validation set, and using cross-validation does not really set aside data for a full test set}
    }
\nDefinition{Leave-one-out}{
    Same thing as cross-validation ,but with K = n, i.e. each data point becomes a test set and the rest the training set.

    \begin{itemize}
        \item[+] best possible classifier is learned: n-1 training examples
        \item[-] high computational cost: re-learn everything n times 
        \item[-] classes are not balanced in training / testing sets  
            \subitem- extreme example: say we have 2 equ-probable classes and \textbf{zero attributes}, this will lead to training set of n/2 samples of B and n/2 - 1 of A. Then the model must estimate that B is always the most likely, but the instance in the test set is of class A (same thing happens if B is in the test set, model is then biased towards A), so the model will always be wrong.    
    \end{itemize}
}

\nDefinition{Stratification}{
    All the above splitting methods disregard the class balance in the splits, this might be a problem.

    The process of ensuring each split is always populated with equal proportion of class instances, is called \textbf{stratification}
}

\nDefinition{Basics of classification measures}{
    \nImg[0.9]{classification_measures}
}

\nDefinition{Accuracy/Classification Error}{
    The percentage of correct classifications, or distance from real value to the predicted value in regression.
    This is not a great measure when the A priori are uneven, i.e. one class is more frequent

    \begin{equation}
        Acc = \frac{errors}{total} = \frac{FP + FN}{TP + TN + FP + FN}
    \end{equation}

    \begin{center}
        \nImg{accuracy_bad}
    \end{center}
    
}
\nDefinition{Misses and false alarms}{
    We can devise error metrics which aim to take into account the false positives and negatives more:

    \begin{equation}
        \begin{aligned}
            &False Alarm Rate = False Positive Rate = \frac{FP}{FP + TN}\\
            &\text{\% of negatives we misclassified as positive}\\
            &Miss rate = False Negative Rate = \frac{FN}{TP+FN}\\
            &\text{\% of positives we misclassified as negative}\\
            &Recall = True Positive Rate = \frac{TP}{TP+FN}\\
            &\text{\% of positives we classified correctly}\\
            &Precision = \frac{TP}{TP+FP}\\
            &\text{\% of the things we predicted positive which were actually positive (dummy classifiers could be realy bad here)}\\
        \end{aligned}
    \end{equation}

    We can use a combination of these to quantify the performance of a model, depending on what's more important and on the domain.
    i.e. it's trivial to get 100\% recall or 0\% false alarm () 

}

\nDefinition{Utility and cost}{
    We can combine these metrics into a single utility measure. If we know the cost of false positives, or negatives (i.e. how costly is it when our system misclassifies something, preventive measures, evacuation, or cost of recovery, reconstruction)
    We can also evaluate how costly our model is in our domain.

    \begin{itemize}
        \item Detection cost: weighted average of FP, FN rates, good for event detection (earthquakes)
            \begin{equation}
                Cost = C_{FP} * FP + C_{FN} * FN
            \end{equation}
        \item F-measure: harmonic mean of recall, precision, good for information retrieval, internet searches (takes out true negatives of the equation)
            \begin{equation}
                F1 = 2/ (1/Recall + 1/ Precision)
            \end{equation}
        \item Domain-specific measures:
            \subitem- e.g. observed profit/loss from +/- market prediction
    \end{itemize}
}

\nDefinition{Classification thresholds}{
    Say two systems have the following performance:
    \begin{itemize}
        \item A: True positive = 50\%, False Positive = 20\%
        \item A: True positive = 100\%, False Positive = 60\%
    \end{itemize}

    which is better ? (assume no-apriori utility)
    \begin{itemize}
        \item very misleading question, A and B could be \textbf{THE EXACT SAME SYSTEM} but operating at different \textbf{thresholds}
    \end{itemize}

    \nHeader{ROC curves}
    many algorithms computeu "confidence" c(x)
    i.e. the threshold above which they classify something as a positive. For naive bayes this is naturally 0.5

    this threshold determines the error rates of the model.
    FP rate = $P(c(x) > t | ham)$, TP rate = $P(c(x) > t| spam)$

    We can evaluate the performance of the model across the entire spectrum of t's using \textbf{Receiver Operating Characteristic} curves.
    Simply plot the TPR vs FPR as t varies from $\infty $to $-\infty$

    \begin{center}
        \nImg[0.9]{roc_curves}
    \end{center}

    The AUC is a good metric for comparing different classifiers which can operate over a variety of thresholds.
}

\nDefinition{Evaluating regression}{
    All the above methods deal with discrete, classifiers. How do we evaluate regression ?
    There are many error measures for regression:

    \nHeader{Mean Squared Error}
    \begin{equation}
        \frac{1}{n}\sum_{i=1}^{n}f(x_{i}-y_{i})^{2}
    \end{equation}

    \textit{popular, well understood, differentiable but {\color{red}sensitive to outliers, asingle prediction which is off by 10, is equivalent to 100 predictions off by 1.}}
    \textit{\color{red}also sensitive to mean and scale}

    \begin{center}
        \nImg[0.4]{error_offset}
    \end{center}
    \begin{center}
        here orange predictor is not as bad, but will yield high error
    \end{center}

    \nHeader{Mean Absolute Error}
    \begin{equation}
        \frac{1}{n}\sum_{i=1}^{n}|f(x_{i}-y_{i})|
    \end{equation}
    \textit{less sensitive to outliers, many small errors = one large error. {\color{red}Sensitive to mean and scale}}

    \nHeader{Median Absolute Deviation}
    \begin{equation}
        median(|f(x_{i}-y_{i})|)
    \end{equation}

    \textit{robust, completely ignores outliers. {\color{red} difficult to wor with - cannot take derivatives, sensitive to mean and scale}}
    \nHeader{Correlation coefficient}
    \begin{equation}
        \frac{n \sum_{i=1}^{n}f(x_{i}-\mu_{f})(y_{i}-\mu_{y})}
        {\sqrt{n \sum_{i=1}^{n}f(x_{i}-\mu_{f})^{2}(y_{i}-\mu_{y})^{2}}}
    \end{equation}
    Basically ignores the "position" of your prediction function and likes predictors of similar shape to the data (without considering mean and scale)
    \textit{insensitive to mean \& scale, good for ranking tasks. Intuition: is your output small as target is small and vice-versa?}
}

\nSection{Linear regression}

\nDefinition{Linear model}{
    The idea is to assume the output value is a linear combination of the attributes, i.e.:
    \begin{equation}
        \begin{aligned}
            &y = f(\vec{x}) = w_{0} + w_{1}x_{1} + \hdots + w_{D}x_{D} = \phi(\vec{x})\vec{w} \\
            &\phi(x) = (1,x_{0},x_{1},\hdots,x_{D})
        \end{aligned}
    \end{equation}


    where:
    \begin{itemize}[noitemsep]
        \item \textbf{w} are the weights, or model parameters (think gradient of the line)
        \item \textbf{x} is the observation
        \item \textbf{$\phi(x)$} is just a generalization of x which appends a 1 to the beginning (and performs transformations on the input data)  
    \end{itemize}

    Once we have the weights vector, we can predict new outputs by simply following the line defined by our model:

    \begin{center}
        \nImg[0.4]{linear_reg}
    \end{center}

    \textit{Note: if $\vec{x}$ is the matrix of all input data (i.e. each row is an input data point), $\phi(\vec{x})$ becomes a matrix and $\vec{y}$ becomes a vector or predictions for each data point}   
}

\nDefinition{Calculating w - least squares}{
    We need to find the parameters $\vec{w}$ given the \textbf{design matrix} $\phi$ (where each training instance is a row). 
    Many methods, simplest is to minimise loss function:
    \begin{equation}
       O(\vec{w}) = \sum_{i=1}^{n}(y_{i} - \vec{w}^{t}x_{i})^{2}
    \end{equation}

    i.e. we are finding the weights which give us a model with \textit{the least squared error} - or minimises sum of \textbf{residuals} squared.

    This problem is exactly the same as the problem of finding a solution to tall matrices of equations $Ax = b$ which minimizes $|Ax - b|^{2}$. The standard way of dealing with tall matrices is using the \textbf{pseudo-inverse}:
    \begin{equation}
        \vec{w} = (\phi^{T}\phi)^{-1} \phi^{T}\vec{y}
    \end{equation}
    \nHeader{Probabilistic intepretation of O(w)}

    This method can be interpreted probabisticially by assuming: $y = \vec{w}^{T}\vec{x} + e$
    
    where e is just some arbitrary gaussian noise $N(0,\sigma^{2})$.
    This means, we are assuming that our data is scattered around the hyperplane of best fit according to some gaussian noise.
    This implies that $y|\vec{x}_{i} \approx N(\vec{w}^{T}\vec{x}_{i},\sigma^{2}) $
    So minimising $O(\vec{w})$ is equivalent to maximising the likelihood of this model! (probability output value is drawn from this gaussian)
    
    Then $\vec{w}^{T}\vec{x}$ can be seen as the expectation $E(y|\vec{X})$
    and the squared residuals give us the estimation of the "noise" model:
    \begin{equation}
        \sigma^{2} = \frac{1}{n}\sum_{i=1}^{n}(y_{i} - \vec{w}^{T}\vec{x}_{i})^{2}
    \end{equation}
    
    }

\nDefinition{Linear regression with multiple outputs}{
    what if for each piece of input data we want to predict more than one value ? Introduce a $\vec{w_{y}}$ for each y required and perform \textbf{multiple regression}, i.e. a separate regression for each target.
}

\nDefinition{Non-linear data}{
    We can expand linear regression to work with non-linear data, by transforming it first!
    \begin{equation}
        \phi = \begin{bmatrix}
            \phi_{1}(\vec{x}_{1}) & \phi_{2}(\vec{x}_{1}) & \hdots & \phi_{3}(\vec{x}_{1})\\
            \phi_{1}(\vec{x}_{2}) & \phi_{2}(\vec{x}_{2}) & \hdots & \phi_{3}(\vec{x}_{2})\\
            \vdots &\vdots & \hdots & \vdots\\
            \phi_{1}(\vec{x}_{d}) & \phi_{2}(\vec{x}_{d}) & \hdots & \phi_{d}(\vec{x}_{2})\\
        \end{bmatrix}
    \end{equation}

    The solution to w stays the same and this way we can establish linear relationships with non-linear transforms of the data!

    \nHeader{Polynomial regression}

    an example of this is polynomial regression of the form (with 1D data):
    \begin{equation}
        \begin{aligned}
            &\phi_{1}(x) = x \\
            &\phi_{2}(x) = x^{2}\\
            &\phi = (1,x,x^{2})
        \end{aligned}
    \end{equation}

    this would yield solutions of the form:

    \begin{equation}
        y = w_{0} + w_{1}x + w_{2}x^{2}
    \end{equation}

    I.e. the model would find a hyperplane in higher dimensional space which actually expresses the polynomial form we're looking for!

    \begin{center}
        \nImg[0.35]{polynomial_reg}
    \end{center}

    \nHeader{Radial basis functions}

    Another common transformation is \textbf{RBF} defined as:
    \begin{equation}
        \phi_{i}(\vec{x}) = \exp(\frac{-\frac{1}{2}\left| \vec{x} - \vec{c}_{i} \right|^{2}}{\alpha^{2}})
    \end{equation}
    
    where: 
    \begin{itemize}[noitemsep]
        \item $\alpha$ is the "width" of the basis function, akin to variance in a gaussian distribution
        \item $\vec{c_{i}}$ is the "center" of the basis function, akin to the mean in a gaussian distribution
    \end{itemize}
    Notice how this is equivalent to a gaussian distribution, just without the normalizer term (i.e. this does not sum to 1)

    Example with 1D data and 2 radial basis:
    \begin{center}
        \nImg[0.35]{rbf_functions}
    \end{center}

    hard to imagine, but this way we express the output as a linear combination of distances to each centre, and this can give us a nice linear relationship.
    
    \textit{\color{red} These are really good, but with more dimensions you'll need way too many of them and that probably means overfitting, also hard to learn the rbf parameters}
}

\nDefinition{one-hot-encoding}{
    How do we translate categorical features to work with linear regression ? 
    can we give a different numeric value to each category like so?:
    \begin{equation}
        x_{1} = 
        \begin{cases}
            1 & \text{if intel}\\
            2 & \text{if AMD}\\
            3 & \text{if Apple}\\
            4 & \text{if Motorola}
        \end{cases}
    \end{equation}

    No! HELL NO. EW!

    Think about it, say we have only this attribute and no bias:
    \begin{equation}
        y = x_{1}\cdot w_{1}
    \end{equation}
    maybe intel produces twice as good processors as AMD (based). We cannot express that in this form, we only get one weight for the attribute, so changing $w_{1}$ equally changes the \textbf{contribution} of each manufacturer to the output (clock speed or whatever).

    We can however split up each value of this attribute into its own attribute:
    \begin{equation}
        \begin{aligned}
            &x_{1} = \text{1 if intel or 0 otherwise}\\
            &x_{2} = \text{1 if AMD or 0 otherwise}\\
            \vdots
        \end{aligned}
    \end{equation}
    We can then express the superiority of intel with weights $w_{1} = 1$,$w_{2} = 0.5$ (exact values, scale etc.. irrelevant rn.)like so:
    \begin{equation}
        y = x_{1} + x_{2} \cdot 0.5 + \hdots
    \end{equation}


}
\nDefinition{Evaluation of Linear regression}{

    \begin{itemize}
        \item[+] very simple yet powerful
        \item[+] \textbf{invariant to scaling of attributes} (no need for normalization) 
        \item[-] sensitive to outliers
            \subitem \nImg[0.2]{linear_reg_outliers}
    \end{itemize}
}
\nSection{Logistic regression}

\nDefinition{Binary classification}{
    While the machine learning mafia would like you to believe this algorithm is used for regression, this is shockingly not the case, we use it to perform binary classificatin like so:
    \begin{equation}
        y = f(\vec{x}) =
        \begin{cases}
            1 & \text{if} \quad \vec{w}^{T}\vec{x} + w_{0} > 0\\ 
            0 & \text{otherwise}
        \end{cases}
    \end{equation}
    where:
    \begin{itemize}[noitemsep]
        \item $\vec{w}$ is the weight vector perpendicular to the decision boundary (pointing to the "side" of class 1)
        \item $w_{0}$ is the bias term (extracted from w for convenience)
    \end{itemize}

    \begin{center}
        \nImg[0.3]{logistic_decision}
    \end{center}

    I.e. we model the decision boundary instead of the distributions of the classes (\textbf{discriminative} model)
    Decision boundary (hyperplane) is defined as $\vec{w}^{T}\vec{x} + w_{0} = 0$ (think with $w_{0} = 0$ dot product of x with w must be zero if x is on the line, $w_{0}$ offsets this linearly)

    \nHeader{Probabilistic version}
    We can do a bit better, and give ourselves the benefit of a probability value so we can say how confident we are in our prediction, by "squashing" the distances from the hyperplane so that they form probabilities:
    $P(y=1|\vec{x}) = \vec{w}^{T}\vec{x}$ does not work, because the value is unbounded and breaks probability axioms, instead we use a sigmoid:
    \begin{equation}
        \begin{aligned}
            & P(y=1|\vec{x}) = f(\vec{w}^{T}\vec{x})\\
            & P(y=0|\vec{x}) = 1 - P(y=1|\vec{x})\\
            &f(z) = \sigma(z) = \frac{1}{1 + \exp(-z)}\\  
        \end{aligned}
    \end{equation}

    The sigmoid function maps extreme distance values to the range [0,1]:
       
    % \begin{center}
    %     \nImg[0.3]{sigmoid}
    % \end{center}

    since $\sigma(z) = 0.5$ when $z = 0$, the \textbf{decision boundary} is given by $\vec{w}^{T}\vec{x} = 0$

    The scale of $\vec{w}$ affects the shape of the sigmoid, larger $\vec{w}$ inflate the values, scaling the sigmoid on the x axis by the scale factor effectively "squashing" it.
    \begin{itemize}
        \item larger $\vec{w} \rightarrow$ probabilties increased (mostly near decision boundary)
        \item smaller $\vec{w} \rightarrow$ probabilities decreased (mostly near decision boundary)
    \end{itemize}
    \begin{center}
        \nImg[1]{sigmoid_variations}
    \end{center}
}

\nDefinition{Calculating w}{
    To find the model parameters we derive probabilistic expression for likelihood and maximise it.

    Let dataset $D = \{ (\vec{x}_{1},y_{1}),(\vec{x}_{2},y_{2}), \hdots , (\vec{x}_{n},y_{n})\}$ 
    Then the likelihood of our data (probability of seeing this data with w being the weight vector) is:
    \begin{equation}
        \begin{aligned}
            &p(D|\vec{w}) = \prod_{i=1}^{n} p(y = y_{i}|\vec{x}_{i})\\
            &= \prod_{i=1}^{n}p(y = 1 |\vec{x}_{i})^{y_{i}}(1- p(y=1|\vec{x}_{i}))^{1-y_{i}} \quad \text{\#split by cases}\\
            &L(\vec{w}) = \sum_{i=1}^{n} y_{i}\log p(y = 1 |\vec{x}_{i}) + (1 - y_{i})\log(1 - p(y = 1 |\vec{x}_{i})) \quad \text{\#log likelihood}\\
            &L(\vec{w}) = \sum_{i=1}^{n} y_{i}\log \sigma(\vec{w}^{T}\vec{x}_{i}) + (1 - y_{i})\log(1 - \sigma(\vec{w}^{T}\vec{x}_{i}))
        \end{aligned}
    \end{equation}

    Now to maximise we find gradient, and find maxima, cannot do that \textbf{analytically} however, need to use a \textbf{numerical} technique.
    The gradient (worked out by simple application of multivariate calculus) is:
    \begin{equation}
        \nabla L = 
        \begin{bmatrix}
            \frac{\partial L}{\partial w_{1}} \\
            \vdots \\
            \frac{\partial L}{\partial w_{d}} \\
        \end{bmatrix}
        \quad
        \frac{\partial L}{\partial w_{j}} = \sum_{i=1}^{n}(y_{i} - \sigma(\vec{w,x}^{T}\vec{x}_{i}))x_{ij}
    \end{equation} 

    The gradient points in direction of steepest ascent, i.e. direction of change to w which will increase likelihood 

    \nHeader{Gradient descent}
    We can apply \textbf{gradient descent} to find the local maximum - which turns out to be the global maximum because the gradient is a \textbf{convex} function!
    \begin{equation}
        \vec{w}_{t+1} = \vec{w}_{t} + \alpha \nabla L(\vec{w_{t},\vec{x}})
    \end{equation}
    with $\alpha$ here standing for the learning parameter which balances the accuracy vs speed of the descent
}

\nDefinition{Multi class classification}{
    To apply this algorithm to a multi-class classification problem:
    \begin{itemize}[noitemsep]
        \item Introduce a weight vector $\vec{w}_{k}$ for each class k, which does k vs not-k classification.
        \item Apply softmax to pick the most probable class:
            \begin{equation}
                p(y = k | \vec{x}) = \frac{\exp(\vec{w}_{k}^{T}\vec{x})}{\sum_{j=1}^{C}\exp(\vec{w}_{j}^{T}\vec{x})}
            \end{equation}
    \end{itemize}
    Note that: $0 \leq p(y = k | \vec{x}) \leq 1$ and $\sum_{j=1}^{C}p(y=j|\vec{x}) = 1$, i.e. this is another probability distribution but over all the classes.
    \textbf{softmax} is a natural generalization of the sigmoid function to more than 2 classes, and will "split" the probability between classes. We could simply take the sigmoid outputs for each class if our classes are not \textbf{mutually exclusive} (for example, if classes are: \{has leg, has arm\}) 
}
\nDefinition{Linear Separability}{
    Logical regresion, divides the space by a hyper plane and therefore can only perform well on problems which are linearly separable (classes can be divided by a plane).
    i.e. problems where we can find a $\vec{w}$ such that:
    \begin{equation}
        \begin{aligned}
            &\vec{w}^{T}\vec{x} + w_{0} > 0 \quad \text{for all postitive classes} \\
            &\vec{w}^{T}\vec{x} + w_{0} \leq 0 \quad \text{for all negative classes} 
        \end{aligned}
    \end{equation}

    However, we can apply \textbf{non-linear} transformations to the input data (similarly as in linear regression), to make the data \textbf{linearly separable}.
    \begin{center}
        \nImg[0.6]{non_linearly_separable}
    \end{center}
    }

\nDefinition{Evaluation of logical regression}{
    \begin{itemize}[noitemsep]
        \item[+] \textbf{discriminative advantage}: avoids spending time on generating probability model, models boundary directly (models $p(y|\vec{x})$ directly vs $p(\vec{x}|y)$)
        \item[+] handles outliers better than linear regression
            \nImg[0.6]{logistic_vs_linear_reg} 
        \item[+] less assumptions about data (doesn't assume any distribution or underlying model) apart from separability
        \item[+] handles correlation relatively well
        \item[-] doesn't handle missing data as gracefuly as NB (generative advantage) 
    \end{itemize}
}
\nSection{Optimisation \& Regularisation}

\nDefinition{Error function}{
    We can think of optimising the weight vector in terms of the error function defined against the weights: $E(\vec{w})$ which defines a surface:

    \begin{center}
        \nImg[0.8]{error_surface}
    \end{center}

    Learning = descending this curve
}

\nDefinition{Role of smoothness}{
    If E is completely unconstrained, minimization is impossible, best we could do is search through all possible values of $\vec{w}$

    \begin{center}
        \nImg[0.5]{not_smooth_error}
    \end{center}
    }

\nDefinition{Role of derivatives}{
    The gradient of the error function tells us the direction we need to push $\vec{w}$ in to increase error:
    \begin{equation}
        \frac{\partial E}{\partial \vec{w}}=
        \nabla E = \begin{pmatrix}
            \frac{\partial E}{\partial w_{1}}\\
            \frac{\partial E}{\partial w_{2}}\\
            \vdots \\
            \frac{\partial E}{\partial w_{d}}\\
        \end{pmatrix}
    \end{equation} 

    Multi-variate calculus is cool, think of each partial derivative as "wiggling" that component of w and seeing how it affects the function, each direction of wiggling is independent when it comes to the gradient!
}

\nDefinition{Numerical optimization}{
    Numerical optimization tries to solve the general problem:
    \begin{equation}
        \min_{\vec{w}}E(\vec{w})
    \end{equation}

    Most commonly this is dune using the gradient (sometimes using more information, like higher order derivatives)

    Most basic optimization algorithm is:
    
    \begin{enumerate}[noitemsep]
        \item Initialize $\vec{w}$
        \item while $E(\vec{w})$ too high, calculate gradient $\vec{g} = \nabla E$
        \item pick direction d based on $\vec{g}$,$\vec{w}$,$E(\vec{w})$
        \item set $\vec{w} = w - n\vec{d}$. Repeat from 2.
    \end{enumerate}

    note $n$ is the learning rate or step size.
    }

\nDefinition{Choice of direction}{
    Simplest choise for $\vec{d}$ is the current gradient $\nabla E$ (because we are moving in direction of -d)
}

\nDefinition{Gradient descent}{
    Numerical optimizatio where at each step we choose $\vec{d} = \nabla E$.

    If the \textbf{step size} is too small, the algorithm will be slow, and if too large, it won't be stable
    
    \nHeader{Bold Driver gradient descent}

    Simple heuristic for choosing n which you can use if desperate:
    \begin{itemize}[noitemsep]
        \item Perform gradient descent but: we repeat while n is greater than zero
        \item At each step if the error has increased from the previous step, we set $n = n /2$
        \item If the error has not increased, we increase n (say by factor of $1.01$) 
    \end{itemize}
    }
\nDefinition{Batch vs Online learning}{
    \nHeader{Batch}
    \textbf{Batch} learning uses all patterns in the training set, and updates the weights after
    \begin{equation}
        \nabla E = \sum_{i} \nabla E_{i}
    \end{equation}
    \textit{Note $E_{i}$ signifies the error due to $i$'th sample}.

    Batch methods are easier to analyze and more powerful.
    \nHeader{Online learning}
    \textbf{Online} learning uses a single instance at each iteration and updates weights after each
    \begin{equation}
        \nabla E = \nabla E_{i}
    \end{equation}
    Online methods are more feasible for huge or continually growing datasets.
    Online learning allows the possibility of jumping over local optima ocasionally
    }

\nDefinition{Problems with gradient descent}{
    \nHeader{Shallow valleys}
    gradient descent can be made very slow if the shape of the error function looks like a valley:
    The algorithm will travel very slowly once it hits the valley.

    \begin{center}
        \nImg{valley}
    \end{center}

    One solution is to use momentum in place of the gradient:
    \begin{equation}
       \vec{d}_{t} = \beta \vec{d}_{t-1} + (1- \beta)n \nabla E(\vec{w}_{t})  
    \end{equation}
    Here $\beta$ varies how much momentum we inherit from the previous iteration.
    But this increases the complexity of the algorithm, and you have to find both optimal values for $n$ and $\beta$

    \nHeader{Curved error surfaces}

    The gradient always points in the direction of steepest ascent, but this might not actually be any local maximum due to curvature. This means gradient descent is not always very fast.

    We can measure local curvature using the hessian matrix:
    \begin{equation}
        H_{ij} = \frac{\partial^{2}E}{\partial w_{i}w_{j}}
    \end{equation}

    \nHeader{Local minima}

        While following the gradient guarantees that we hit a local maximum, this maximum is not necessarily global. Meaning that we can get stuck in a solution which is sub-optimal.
        If the error function is not \textbf{convex} we are not guaranteed to find the global maximum.

        Best you can do is run the algorithm from different points and pick best run.
    }
\nSection{Support Vector Machines}
\nSection{Ethics}
\nSection{Nearest Neighbours}
\nSection{K-Means}
\nSection{Gaussian mixture models}
\nSection{Principal components analysis}
\nSection{Hierarchical Clustering}
\nSection{Perceptrons}
\nSection{Neural networks}

















\end{document}
