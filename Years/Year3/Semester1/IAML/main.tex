\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{IAML Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{IAML}

\nSection{Introduction}

\nDefinition{Machine Learning}{
    A machine learning model \textbf{takes in} data, \textbf{outputs} predictions. It's a function of data really together with a set of training data.

    Learning = Representation + Evaluation + Optimisation
}
\nSection{Thinking about data}

\nDefinition{Classification}{
    Sort data points into discrete buckets based on training data 
}

\nDefinition{Regression}{
    Output a continuous/real value for each data point based on training data. 
}

\nDefinition{Clustering}{
    Detect which data points are related to which other data points, find outliers.
}

\nDefinition{Data representation}{
    What format do we feed the data in ? Most likely as a \textbf{bag of features}. I.e. collection of attribute-value pairs, every data point must have an attribute-value pair for each property (in most cases)
    
    Data representation has more impact on the performance of your ML algorithm than anything.

    \nHeader{Types of attributes}

    \begin{itemize}[noitemsep]
        \item \textbf{Categorical}
            \subitem- e.g. red/blue/brown
            \subitem- a set of possible \textbf{mutually exclusive} values
            \subitem- meaningful operators: equality comparison
            \subitem- usually represented as numbers 
            \subitem- problems: {\color{red} synnonymy is a major challenge e.g. some values might mean the same thing to a human but not to the machine (country == folk?)}   
        \item \textbf{Ordinal}
            \subitem- e.g. poor $<$ satisfactory $<$ good $<$ excellent
            \subitem- a set of possible \textbf{mutually exclusive} values, but with a \textbf{natural ordering}
            \subitem- meaningful operators: equality comparison, sorting
            \subitem- problems: {\color{red} sometimes hard to differentiate from categorical (single $<$ divorced)?}    
        \item \textbf{Numeric}
            \subitem- e.g. 3.1/5
            \subitem- meaningful operators: arithmetic, distance metrics, equality, sorting 
            \subitem- problems:
                \subsubitem- {\color{red} sensitive to extreme outliers (handle these \textbf{before normalization})}
                \subsubitem- {\color{red} skewed distributions (assymetric) - outliers might actually be real data (e.g. personal wealth data)}
                \subsubitem- {\color{red} Non-monotonic effect of attributes - e.g. predicting someone is going to win a marathon, here the relationship is not monotonic i.e. norect correlatio din, might be a curve with a "sweet spot"}
            \subitem- solutions:
                \subsubitem- \textit{Deal with outliers, maybe trim them for training phase only?}
                \subsubitem- \textit{use a log/atan scale to make data more linear}
                \subsubitem- \textit{discretize data into buckets}
    \end{itemize}
}
\nDefinition{Normalisation}{
    Normalization is the process of converting all the data such that each different attribute is roughly in the same range, and comparable.

    Normalization is mostly necessary for linear methods.
}


\nDefinition{Picking attributes}{
    We want:
    \begin{itemize}[noitemsep]
        \item all our attributes to have similar values if the data points that posses them are similar themselves!    
        \item small change in input $\rightarrow$ small change in values
    \end{itemize} 
    
    \nHeader{Images}

    For images, can we use pixel data as attributes directly ? it depends, if the pixel 20,20 always corresponds to the middle of a letter then yes, this is a meaningful attribute which might help us discern whether digits given have strokes going through the middle of the image!
    What if the pixel is always something random ? This could happen whenever we are looking for an object which can be anywhere in the image, in which case the attribute would be gibberish!
    In the case of image classification, we want attributes which are:
    \begin{itemize}
        \item invariant to size 
        \item invariant to rotation 
        \item invariant to illumination
    \end{itemize}
    
    How can we classify whether an image contains a desired object ? In general we do the following:
    \begin{itemize}
        \item \textbf{Segment} the image, into regions of pixels which we believe are related (by colour, texture, position etc..)
        \item Pick a number of attributes which you will use to describe each of the regions
        \item Then use those features in your machine learning algorithm!
    \end{itemize}
    Keep in mind though, the segmentation \textbf{will} make errors, we can hope that these will be consistent across all images (all images will have their legs chopped off). Sometimes we can segment using a grid
    
    \nHeader{Text}
        For textual data, often we can use the \textbf{bag-of-words} approach. I.e. we can form an attribute vector which counts the amount of occurrences of each word, regardless of position. This is invariant to word shuffling for example.
    \nHeader{Sound}
    For sound, the data is sound waves. How can we select attributes here? We can count the number of different frequencies occurring in the piece, (using Fourier's transform) and treat this as a feature vector!
    \begin{center}
        \nImg{attributes_sound}
    \end{center}
}



\nDefinition{Supervised Learning}{
    Supervised learning algorithms have some sort of "performance" metric they can use, i.e. test labels they can validate their guesses on.
    When the algorithm can measure accuracy directly it's a supervised algorithm.
}

\nDefinition{Unsupervised Learning}{
    Learning without a specific accuracy measure available. Algorithms in this area usually look for structure/patterns/information in the data which can be helpful in other ways.
    There is nothing specific the algorithm is looking for.
    Can be \textbf{direct} when the algorithm helps to make sense of the data directly, or \textbf{indirect} when it is "plugged" into another machine learning aglorithm as an attribute itself. 
}

\nDefinition{Semi-supervised Learning}{
    Using unsupervised methods to improve supervised algorithms. Usually have a few labelled examples but lots more unlabelled. 
}

\nDefinition{Multi-class classification}{
    Classification with multiple mutually exclusive labels/classes.
    
    Might be hard to tell when something belongs to none of the available classes.
}
\nDefinition{Binary classification}{
    Classification with 2 mutually exclusive labels/classes in each "run". 
    This way of classification can be applied to multiple-classes classification but with a "One-vs-Rest" meta-strategy (a vs not a, b vs not b).
    In this way a sample may belong to multiple classes but never to two sides of the one-vs-rest structure simultaneously in each run.
    
    In this classification method we can actually tell when something doesn't belong to any class!
}

\nDefinition{Analysing data}{
    We have to check for a number of things in our data sets:
    \begin{itemize}[noitemsep]
        \item Are there any dominating classes ? what would the best "dummy" model do ? always predicting no ?
        \item What should we use as the appropriate error metric ? How important are the false positives vs the false negatives ? 
    \end{itemize}
}

\nDefinition{Generative model}{
    A generative model, develops a probabilistic model of each class, i.e. tries to "model" the underlying probability distribution directly.
    The decision boundary becomes implicitly defined by the probabilities of each input being in each class.
}

\nDefinition{Discriminative model}{
    A discriminative model ignores the underlying model and tries to "separate" the data, i.e. it tries to model the boundaries that divide the classes.
    \textbf{Not designed to use unlabeled data} so cannot be used for unsupervised tasks.
}

\nDefinition{Dealing with data structure}{
    What do we do if the data input has some sort of hierarchical structure ?  Where the position of occurrence of a node affects its meaning?
    We can encode as attributes the existence of root-to-leaf paths in the entire tree, and use this bag-of-words approach to perform machine learning 
    \begin{center}
        \nImg{structure_input}
    \end{center}

    What if we need to predict the output structure from the input structure ? This is very difficult, but we can "trick" our classifier and turn this more into a search problem by embedding the possible outputs with each input and classifying on that instead:

    \begin{center}
        \nImg[0.9]{structure_output}
    \end{center}
    This of course means we have to search for all possible output structures!

    }

\nDefinition{Dealing with outliers}{
    \textbf{Outliers} are isolated instances of a class that are unlike any other instance of that class. These affect all learning models to some degree.

    There are some ways we can deal with outliers. One method is to remove the outliers just before we perform any sort of normalisation on the data, (ONLY FOR THE PURPOSES OF TRAINING!!)
    We can also put a confidence interval around our data, and removing values outside of those intervals (with x,y values outside of a normal range).
    Some data points might still be outliers even though they are within expected x,y ranges! (second figure)
    
    \begin{center}
        \nImg{outliers}    
    \end{center}

    Best way to deal with outliers ? \textbf{VISUALISE YOUR DATA}
    }



\nSection{Naive Bayes}

\nTheorem{Bayes rule}{
    \begin{equation}
        P(y|x) = \frac{P(x|y) P(y)}{\sum_{y'} P(x|y')P(y')}
    \end{equation}

    where:
    \begin{itemize}[noitemsep]
        \item y : one of the classes
        \item x : the input data, feature vector
        \item P(y) : \textbf{prior}, the probability of seeing elements of class y in general prior to making any observations
        \item $P(x|y)$ : \textbf{class model/likelihood}, given the data is in class y, how likely are the features that i am seeing
        \item P(x) = $\sum_{y'}P(x|y')P(y')$ : \textbf{normalizer}, does not affect the probabilities, but without this term we cannot compare data in terms of probability of being in each class (i.e. for confidence values)
    \end{itemize}

    \textit{Note: this works with probability densities too (which we're using almost exclusively)}
}

\nDefinition{Naive bayes}{
    Bayesian classifier, which assumes \textbf{conditional independence} between different attributes. It attempts to model the underlying probability distribution so it's a \textbf{generative} model.

    I.e.:
    \begin{equation}
        P(\vec{x}|y) = \prod_{i=1}^{d} P(x_{i}|x_{1} \hdots x_{i-1},y)=  P(x_{1}|y) \cdot P(x_{2}|y) \cdot P(x_{3}|y) \hdots
    \end{equation}
    It assumes that there are no correlations between the variables themselves, and any correlations are explained by the fact they belong to the same class.

    \nHeader{Example}
    The probability of going to the beach and getting a heat stroke are not independent: $P(B,S) > P(B)P(S)$.
    The may be independent if we know the weather is hot (i.e. external factor is what actually causes the dependence, which we can factor into the equation):
    $P(B,S|H) = P(B|H)P(S|H)$ This hidden factor in naive bayes is \textbf{the class}
}

\nDefinition{Gaussian Naive bayes}{
    A Naive Bayes classifier which uses the gaussian distribution as a class model for each class.

    \begin{equation}
        \LARGE
        p(x|y) = \frac{1}{\sqrt{2\pi \sigma_{x,y} ^{2}}}\exp^{-\frac{1}{2} \frac{(x - \mu_{x,y})^{2}}{\sigma_{x,y}^{2}}}
    \end{equation}

    With:
    \begin{itemize}[noitemsep]
        \item $\mu_{x,y}$ : the mean of the gaussian modelling class y's attribute x
        \item $\sigma_{x,y}^{2}$ : variance of the gaussian modelling class y's attribute x
        \item $x$ : the value of the attribute
        \item $y$ : the class 
    \end{itemize}
    \textit{Note: this is not actually a probability per-se, this is a p.d.f function which can be higher than 1}
    }

\nDefinition{Problems with Naive Bayes}{
    \nHeader{Covariance}
    Naive bayes cannot model covariance of the attributes, i.e.:
    \begin{center}
        \nImg{covariance_nb}
    \end{center}
    in the above case the only thing differentiating the classes is their relationship between x and y, i.e. the two attributes. Naive bayes can only model 
    \textbf{spherical} distributions
    
    \nHeader{Zero frequency problem}
        Should any data never appear under any of the classes the probability of it belonging to that class is always going to be zero.
        So say we are classifying emails as spam or non-spam, and the word "now" never occurred in non-spam emails, but had occurred in some spam emails. Any sentence containing the word "now" would always be classified as spam no matter the actual probability of it being spam!

        Solution, add a small epsilon to all "feature counts", or perform laplace smoothing: 
        \begin{equation}
            P(w,c) = \frac{num(w,c) + \epsilon}{num(c) + 2\epsilon}
        \end{equation}
        
        Zipf's law: 50\% of words occur once 
        \nHeader{Independence assumption}

        Continuing with the spam example, every word contributes to the total probability independently, easy to fool. Simply stuff lots of non-spammy words into email
    
        }

\nDefinition{Deaing with missing data}{

    Suppose we have missing data for some attribute i of a sample x. How can we compute the likelihood of this sample belonging to any class ? 

    We simply ignore it:
    \begin{equation}
        P(x_{1}\hdots x_{i} \hdots x_{d}|y) = \prod_{k \neq i}^{d} P(x_{i}|y)
    \end{equation}

    There is no need to fill in the gaps artificially because missing attribute data essentially has a probability of 1: 
    
    \begin{center}
        \nImg[0.8]{nb_missing_data}
    \end{center}
}


\nSection{Decision Trees}

\nDefinition{Method}{
    Decision trees split the data based on the label of each data point, in a way which maximizes the entropy of each split. This means we are splitting on attributes which have the most "discriminatory" power at each step
\begin{center}
    \nImg{dt_example}
\end{center}
    We classify each new sample by following the splits in the tree untill we hit a pure subset (or non-pure), and select the label based on the most frequent label in that leaf. 
}

\nDefinition{Quinlan's ID3 Algorithm}{
    \begin{itemize}[noitemsep]
        \item A <- best attribute for splitting  (\textbf{needs a heuristic})
        \item Decision attribute for this node <- A 
        \item For each value of A, create new child node
        \item Split training data at this node into child nodes
        \item For each child node/subset 
            \subitem if subset is \textbf{pure}, stop
            \subitem otherwise repeat on this subnode  
    \end{itemize}
}

\nDefinition{Purity measures}{
    In order to pick the best attribute we need a purity measure
    
    Even splits do not give us any information, we want ones biased towards the +'ve or -'ve labels

    i.e. the purity for a pure set (4y/0n) is 100\%
    but the purity for an impure set (3y/3n) is 50\% (uncertain)
    
    \nHeader{Entropy}

    \begin{equation}
        H(S) = -p_{+}\log_{2}p_{+} -p_{-}\log_{2}p_{-}
    \end{equation}

    where: 
    \begin{itemize}[noitemsep]
        \item S is the subset of training examples
        \item p's refer to the proportion of positive and negatives in the subset S
    \end{itemize}

    \begin{center}
        \nImg{entropy}
    \end{center}

    we interpret this measure as the number of bits of information needed to tell if a random item in S is a + or a -, we dont need any information at all if the subset is entirely composed of +'s or -'s, and all the information if it's evenly split
}

\nDefinition{Information gain}{
    once we have a measure of purity, we can establish the heuristic which will define which split will be the best:

    \begin{equation}
        Gain(S,A) = H(S) - \sum_{V\in Values(A)}\frac{|S_{v}|}{|S|}H(S_{v})
    \end{equation}

    where:
    \begin{itemize}[noitemsep]
        \item \textbf{S} : subset of data points
        \item \textbf{A} : attribute we are splitting on
        \item \textbf{V} : possible values of attribute A
        \item \textbf{$S_{v}$} : subset of S with data points which have label v  
    \end{itemize}

    This measures the "information" we gain by performing the split. 
    Notice how we take a weighted average (weighted by the number of data points) of the entropy of each new subset formed in the split. 
    If the entropy in the new subsets is on average higher than the entropy in the original subset, 
    we have lost information (note this is not possible in this scenario) and if it's lower, we have indeed gained information. The split which maximises this quantity is the best.
    
    \textit{
    Note: that picking the split on this criterion, implies that no attribute will be picked twice (since if we split on it once, all the subsets in nodes below will have the same value for this attribute)}
    \nHeader{Gain ratio}

    Using this heuristic will lead the algorithm to pick splits which lead to more subsets naturally (if allowed by shape of attributes and data)
    We can use the entropy of the split itself to counteract this instead:
    \begin{equation}
        \begin{aligned}
            &SplitEntropy(S,A) = - \sum_{V \in Values(A)} \frac{|S_{v}|}{|S|}log \frac{|S_{v}|}{|S|}\\
            &GainRatio(S,A) = \frac{Gain(S,A)}{SplitEntropy(S,A)}
        \end{aligned}
    \end{equation}

    not how the gain will be lower when we have too many or too few subsets, 
    the split ratio will be the highest with higher information gains which result in 2 subsets that each take half of the data. 
    }

\nDefinition{Overfitting}{
    Overfitting happens when the model learns the "noise" in the training data, and becomes too specific.

    Decision trees will always classify training examples perfectly if we make them big enough (because eventually we will split the data into singletons, and those instances will always be labelled as the correct instances)

    This is an example of \textbf{overfitting}, we need to limit the size of the tree in order to counteract this, i.e. force some subsets to end up being non-pure in order to stop the model from becoming too specific to our training data.


    \begin{center}
        \nImg[0.6]{dt_overfitting}
    \end{center}

    \nHeader{Solutions}

    \begin{itemize}
        \item Stop splitting when not statistically significant
        \item Grow the tree fully and then prune in a way which maximises the accuracy (using a validation set)
            \subitem- \textbf{Sub-tree replacement pruning}: pretend you remove a node and all its children, measure the performance on validation set. Remove node with highest performance gain, repeat untill removal is harmful to performance(greedy)
    \end{itemize}
}

\nDefinition{Numeric attributes}{
    How can we apply this to numeric attributes? Split based on \textbf{thresholds}, threshold can be picked via some process.
    The easy solution is to take all the values for the attribute we are evaluating a split on, sort them and go through the split on each of those one-by-one.

    \textit{Note: this will lead to one attribute possibly being split multiple times since we can always divide the values into smaller subsets}
}

\nDefinition{Multi-class classification}{
    We can apply decision trees to multi-class classification by predicting the most frequent class in the subset we land on. 
    The entropy calculation becomes:
    \begin{equation}
        H(S) = - \sum_{c} p_{c} \log_{2}p_{c}
    \end{equation}

    where $p_{c}$ is the fraction of samples of class c in the subset
}
\nDefinition{Regression}{
    We can also apply this algorithm to regression by taking the average of the training examples in the subset we land on as the predicted output value.
    In which case instead of maximising gain we grow the tree by minimising variance in the subsets.
    (pick split which reduces variance in the output the most!)
    We can also replace the average by linear regression at the leaves.

}

\nDefinition{Random decision forest}{
    \begin{itemize}[noitemsep]
        \item Pick a random subset $S_{r}$ of training examples 
        \item grow a full tree $T_{r}$ (without pruning)
        \item when splitting pick from $d << D$ random attributes (i.e. hide some attributes for some trees)
        \item compute gain based on $S_{r}$ instead of full set
        \item repeat K times to make K trees
    \end{itemize}

    Given a new data point X we can then:
    \begin{itemize}[noitemsep]
        \item classify X using each of the trees $T_{1}\hdots T_{k}$
        \item use majority vote: class predicted most often as the label!
    \end{itemize}

    \textit{\color{green} State-of-the-art performance in many domains, very good stuff}
}
\nDefinition{Evaluation of decision trees}{
    \begin{itemize}[noitemsep]
        \item[+] \textbf{interpretable} - humans can understand the underlying decisions! (ONLY ALGORITHM IN THIS COURSE TO HAVE THIS PROPERTY)
        \item[+] easilly handles irrelevant attributes (gain = 0)
        \item[+] can handle missing data 
        \item[+] very compact: nodes $<<$ D after pruning
        \item[+] very fast at testing time O(depth)
        \item[-] only axis-aligned splits are possible (because we split only taking into account one dimension at a step)
            \nImg[0.4]{dt_axis_aligned} 
        \item[-] greedy (may not find best tree globally)
        \item[-] exponentially many possible trees       
    \end{itemize}

    \nHeader{Why trees?}
    Can model non-linear, weirdly shaped data which does not realy follow a nice probability distribution (i.e. \textbf{non-monotonic} data!)
    \begin{center}
        \nImg{nb_vs_dt}
    \end{center}
}

\nSection{Generalisation \& Evaluation}

\nDefinition{}{}

\nSection{Linear regression}
\nSection{Logistic regression}
\nSection{Optimisation \& Regularisation}
\nSection{Support Vector Machines}
\nSection{Ethics}
\nSection{Nearest Neighbours}
\nSection{K-Means}
\nSection{Gaussian mixture models}
\nSection{Principal components analysis}
\nSection{Hierarchical Clustering}
\nSection{Perceptrons}
\nSection{Neural networks}

















\end{document}
