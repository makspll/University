\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{IAML Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{IAML}

\nSection{Introduction}

\nDefinition{Machine Learning}{
    A machine learning model \textbf{takes in} data, \textbf{outputs} predictions. It's a function of data really together with a set of training data.

    Learning = Representation + Evaluation + Optimisation
}
\nSection{Thinking about data}

\nDefinition{Classification}{
    Sort data points into discrete buckets based on training data 
}

\nDefinition{Regression}{
    Output a continuous/real value for each data point based on training data. 
}

\nDefinition{Clustering}{
    Detect which data points are related to which other data points, find outliers.
}

\nDefinition{Data representation}{
    What format do we feed the data in ? Most likely as a \textbf{bag of features}. I.e. collection of attribute-value pairs, every data point must have an attribute-value pair for each property (in most cases)
    
    Data representation has more impact on the performance of your ML algorithm than anything.

    \nHeader{Types of attributes}

    \begin{itemize}[noitemsep]
        \item \textbf{Categorical}
            \subitem- e.g. red/blue/brown
            \subitem- a set of possible \textbf{mutually exclusive} values
            \subitem- meaningful operators: equality comparison
            \subitem- usually represented as numbers 
            \subitem- problems: {\color{red} synnonymy is a major challenge e.g. some values might mean the same thing to a human but not to the machine (country == folk?)}   
        \item \textbf{Ordinal}
            \subitem- e.g. poor $<$ satisfactory $<$ good $<$ excellent
            \subitem- a set of possible \textbf{mutually exclusive} values, but with a \textbf{natural ordering}
            \subitem- meaningful operators: equality comparison, sorting
            \subitem- problems: {\color{red} sometimes hard to differentiate from categorical (single $<$ divorced)?}    
        \item \textbf{Numeric}
            \subitem- e.g. 3.1/5
            \subitem- meaningful operators: arithmetic, distance metrics, equality, sorting 
            \subitem- problems:
                \subsubitem- {\color{red} sensitive to extreme outliers (handle these \textbf{before normalization})}
                \subsubitem- {\color{red} skewed distributions (assymetric) - outliers might actually be real data (e.g. personal wealth data)}
                \subsubitem- {\color{red} Non-monotonic effect of attributes - e.g. predicting someone is going to win a marathon, here the relationship is not monotonic i.e. norect correlatio din, might be a curve with a "sweet spot"}
            \subitem- solutions:
                \subsubitem- \textit{Deal with outliers, maybe trim them for training phase only?}
                \subsubitem- \textit{use a log/atan scale to make data more linear}
                \subsubitem- \textit{discretize data into buckets}
    \end{itemize}
}
\nDefinition{Normalisation}{
    Normalization is the process of converting all the data such that each different attribute is roughly in the same range, and comparable.

    Normalization is mostly necessary for linear methods.
}


\nDefinition{Picking attributes}{
    We want:
    \begin{itemize}[noitemsep]
        \item all our attributes to have similar values if the data points that posses them are similar themselves!    
        \item small change in input $\rightarrow$ small change in values
    \end{itemize} 
    
    \nHeader{Images}

    For images, can we use pixel data as attributes directly ? it depends, if the pixel 20,20 always corresponds to the middle of a letter then yes, this is a meaningful attribute which might help us discern whether digits given have strokes going through the middle of the image!
    What if the pixel is always something random ? This could happen whenever we are looking for an object which can be anywhere in the image, in which case the attribute would be gibberish!
    In the case of image classification, we want attributes which are:
    \begin{itemize}
        \item invariant to size 
        \item invariant to rotation 
        \item invariant to illumination
    \end{itemize}
    
    How can we classify whether an image contains a desired object ? In general we do the following:
    \begin{itemize}
        \item \textbf{Segment} the image, into regions of pixels which we believe are related (by colour, texture, position etc..)
        \item Pick a number of attributes which you will use to describe each of the regions
        \item Then use those features in your machine learning algorithm!
    \end{itemize}
    Keep in mind though, the segmentation \textbf{will} make errors, we can hope that these will be consistent across all images (all images will have their legs chopped off). Sometimes we can segment using a grid
    
    \nHeader{Text}
        For textual data, often we can use the \textbf{bag-of-words} approach. I.e. we can form an attribute vector which counts the amount of occurrences of each word, regardless of position. This is invariant to word shuffling for example.
    \nHeader{Sound}
    For sound, the data is sound waves. How can we select attributes here? We can count the number of different frequencies occurring in the piece, (using Fourier's transform) and treat this as a feature vector!
    \begin{center}
        \nImg{attributes_sound}
    \end{center}
}



\nDefinition{Supervised Learning}{
    Supervised learning algorithms have some sort of "performance" metric they can use, i.e. test labels they can validate their guesses on.
    When the algorithm can measure accuracy directly it's a supervised algorithm.
}

\nDefinition{Unsupervised Learning}{
    Learning without a specific accuracy measure available. Algorithms in this area usually look for structure/patterns/information in the data which can be helpful in other ways.
    There is nothing specific the algorithm is looking for.
    Can be \textbf{direct} when the algorithm helps to make sense of the data directly, or \textbf{indirect} when it is "plugged" into another machine learning aglorithm as an attribute itself. 
}

\nDefinition{Semi-supervised Learning}{
    Using unsupervised methods to improve supervised algorithms. Usually have a few labelled examples but lots more unlabelled. 
}

\nDefinition{Multi-class classification}{
    Classification with multiple mutually exclusive labels/classes.
    
    Might be hard to tell when something belongs to none of the available classes.
}
\nDefinition{Binary classification}{
    Classification with 2 mutually exclusive labels/classes in each "run". 
    This way of classification can be applied to multiple-classes classification but with a "One-vs-Rest" meta-strategy (a vs not a, b vs not b).
    In this way a sample may belong to multiple classes but never to two sides of the one-vs-rest structure simultaneously in each run.
    
    In this classification method we can actually tell when something doesn't belong to any class!
}

\nDefinition{Analysing data}{
    We have to check for a number of things in our data sets:
    \begin{itemize}[noitemsep]
        \item Are there any dominating classes ? what would the best "dummy" model do ? always predicting no ?
        \item What should we use as the appropriate error metric ? How important are the false positives vs the false negatives ? 
    \end{itemize}
}

\nDefinition{Generative model}{
    A generative model, develops a probabilistic model of each class, i.e. tries to "model" the underlying probability distribution directly.
    The decision boundary becomes implicitly defined by the probabilities of each input being in each class.
}

\nDefinition{Discriminative model}{
    A discriminative model ignores the underlying model and tries to "separate" the data, i.e. it tries to model the boundaries that divide the classes.
    \textbf{Not designed to use unlabeled data} so cannot be used for unsupervised tasks.
}

\nDefinition{Dealing with data structure}{
    What do we do if the data input has some sort of hierarchical structure ?  Where the position of occurrence of a node affects its meaning?
    We can encode as attributes the existence of root-to-leaf paths in the entire tree, and use this bag-of-words approach to perform machine learning 
    \begin{center}
        \nImg{structure_input}
    \end{center}

    What if we need to predict the output structure from the input structure ? This is very difficult, but we can "trick" our classifier and turn this more into a search problem by embedding the possible outputs with each input and classifying on that instead:

    \begin{center}
        \nImg[0.9]{structure_output}
    \end{center}
    This of course means we have to search for all possible output structures!

    }

\nDefinition{Dealing with outliers}{
    \textbf{Outliers} are isolated instances of a class that are unlike any other instance of that class. These affect all learning models to some degree.

    There are some ways we can deal with outliers. One method is to remove the outliers just before we perform any sort of normalisation on the data, (ONLY FOR THE PURPOSES OF TRAINING!!)
    We can also put a confidence interval around our data, and removing values outside of those intervals (with x,y values outside of a normal range).
    Some data points might still be outliers even though they are within expected x,y ranges! (second figure)
    
    \begin{center}
        \nImg{outliers}    
    \end{center}

    Best way to deal with outliers ? \textbf{VISUALISE YOUR DATA}
    }



\nSection{Naive Bayes}

\nTheorem{Bayes rule}{
    \begin{equation}
        P(y|x) = \frac{P(x|y) P(y)}{\sum_{y'} P(x|y')P(y')}
    \end{equation}

    where:
    \begin{itemize}[noitemsep]
        \item y : one of the classes
        \item x : the input data, feature vector
        \item P(y) : \textbf{prior}, the probability of seeing elements of class y in general prior to making any observations
        \item $P(x|y)$ : \textbf{class model/likelihood}, given the data is in class y, how likely are the features that i am seeing
        \item P(x) = $\sum_{y'}P(x|y')P(y')$ : \textbf{normalizer}, does not affect the probabilities, but without this term we cannot compare data in terms of probability of being in each class (i.e. for confidence values)
    \end{itemize}

    \textit{Note: this works with probability densities too (which we're using almost exclusively)}
}

\nDefinition{Naive bayes}{
    Bayesian classifier, which assumes \textbf{conditional independence} between different attributes. It attempts to model the underlying probability distribution so it's a \textbf{generative} model.

    I.e.:
    \begin{equation}
        P(\vec{x}|y) = \prod_{i=1}^{d} P(x_{i}|x_{1} \hdots x_{i-1},y)=  P(x_{1}|y) \cdot P(x_{2}|y) \cdot P(x_{3}|y) \hdots
    \end{equation}
    It assumes that there are no correlations between the variables themselves, and any correlations are explained by the fact they belong to the same class.

    \nHeader{Example}
    The probability of going to the beach and getting a heat stroke are not independent: $P(B,S) > P(B)P(S)$.
    The may be independent if we know the weather is hot (i.e. external factor is what actually causes the dependence, which we can factor into the equation):
    $P(B,S|H) = P(B|H)P(S|H)$ This hidden factor in naive bayes is \textbf{the class}
}

\nDefinition{Gaussian Naive bayes}{
    A Naive Bayes classifier which uses the gaussian distribution as a class model for each class.

    \begin{equation}
        \LARGE
        p(x|y) = \frac{1}{\sqrt{2\pi \sigma_{x,y} ^{2}}}\exp^{-\frac{1}{2} \frac{(x - \mu_{x,y})^{2}}{\sigma_{x,y}^{2}}}
    \end{equation}

    With:
    \begin{itemize}[noitemsep]
        \item $\mu_{x,y}$ : the mean of the gaussian modelling class y's attribute x
        \item $\sigma_{x,y}^{2}$ : variance of the gaussian modelling class y's attribute x
        \item $x$ : the value of the attribute
        \item $y$ : the class 
    \end{itemize}
    \textit{Note: this is not actually a probability per-se, this is a p.d.f function which can be higher than 1}
    }

\nDefinition{Problems with Naive Bayes}{
    \nHeader{Covariance}
    Naive bayes cannot model covariance of the attributes, i.e.:
    \begin{center}
        \nImg{covariance_nb}
    \end{center}
    in the above case the only thing differentiating the classes is their relationship between x and y, i.e. the two attributes. Naive bayes can only model 
    \textbf{spherical} distributions
    
    \nHeader{Zero frequency problem}
        Should any data never appear under any of the classes the probability of it belonging to that class is always going to be zero.
        So say we are classifying emails as spam or non-spam, and the word "now" never occurred in non-spam emails, but had occurred in some spam emails. Any sentence containing the word "now" would always be classified as spam no matter the actual probability of it being spam!

        Solution, add a small epsilon to all "feature counts", or perform laplace smoothing: 
        \begin{equation}
            P(w,c) = \frac{num(w,c) + \epsilon}{num(c) + 2\epsilon}
        \end{equation}
        
        Zipf's law: 50\% of words occur once 
        \nHeader{Independence assumption}

        Continuing with the spam example, every word contributes to the total probability independently, easy to fool. Simply stuff lots of non-spammy words into email
    
        }

\nDefinition{Deaing with missing data}{

    Suppose we have missing data for some attribute i of a sample x. How can we compute the likelihood of this sample belonging to any class ? 

    We simply ignore it:
    \begin{equation}
        P(x_{1}\hdots x_{i} \hdots x_{d}|y) = \prod_{k \neq i}^{d} P(x_{i}|y)
    \end{equation}

    There is no need to fill in the gaps artificially because missing attribute data essentially has a probability of 1: 
    
    \begin{center}
        \nImg[0.8]{nb_missing_data}
    \end{center}
}


\nSection{Decision Trees}

\nDefinition{Method}{
    Decision trees split the data based on the label of each data point, in a way which maximizes the entropy of each split. This means we are splitting on attributes which have the most "discriminatory" power at each step
\begin{center}
    \nImg{dt_example}
\end{center}
    We classify each new sample by following the splits in the tree untill we hit a pure subset (or non-pure), and select the label based on the most frequent label in that leaf. 
}

\nDefinition{Quinlan's ID3 Algorithm}{
    \begin{itemize}[noitemsep]
        \item A <- best attribute for splitting  (\textbf{needs a heuristic})
        \item Decision attribute for this node <- A 
        \item For each value of A, create new child node
        \item Split training data at this node into child nodes
        \item For each child node/subset 
            \subitem if subset is \textbf{pure}, stop
            \subitem otherwise repeat on this subnode  
    \end{itemize}
}

\nDefinition{Purity measures}{
    In order to pick the best attribute we need a purity measure
    
    Even splits do not give us any information, we want ones biased towards the +'ve or -'ve labels

    i.e. the purity for a pure set (4y/0n) is 100\%
    but the purity for an impure set (3y/3n) is 50\% (uncertain)
    
    \nHeader{Entropy}

    \begin{equation}
        H(S) = -p_{+}\log_{2}p_{+} -p_{-}\log_{2}p_{-}
    \end{equation}

    where: 
    \begin{itemize}[noitemsep]
        \item S is the subset of training examples
        \item p's refer to the proportion of positive and negatives in the subset S
    \end{itemize}

    \begin{center}
        \nImg{entropy}
    \end{center}

    we interpret this measure as the number of bits of information needed to tell if a random item in S is a + or a -, we dont need any information at all if the subset is entirely composed of +'s or -'s, and all the information if it's evenly split
}

\nDefinition{Information gain}{
    once we have a measure of purity, we can establish the heuristic which will define which split will be the best:

    \begin{equation}
        Gain(S,A) = H(S) - \sum_{V\in Values(A)}\frac{|S_{v}|}{|S|}H(S_{v})
    \end{equation}

    where:
    \begin{itemize}[noitemsep]
        \item \textbf{S} : subset of data points
        \item \textbf{A} : attribute we are splitting on
        \item \textbf{V} : possible values of attribute A
        \item \textbf{$S_{v}$} : subset of S with data points which have label v  
    \end{itemize}

    This measures the "information" we gain by performing the split. 
    Notice how we take a weighted average (weighted by the number of data points) of the entropy of each new subset formed in the split. 
    If the entropy in the new subsets is on average higher than the entropy in the original subset, 
    we have lost information (note this is not possible in this scenario) and if it's lower, we have indeed gained information. The split which maximises this quantity is the best.
    
    \textit{
    Note: that picking the split on this criterion, implies that no attribute will be picked twice (since if we split on it once, all the subsets in nodes below will have the same value for this attribute)}
    \nHeader{Gain ratio}

    Using this heuristic will lead the algorithm to pick splits which lead to more subsets naturally (if allowed by shape of attributes and data)
    We can use the entropy of the split itself to counteract this instead:
    \begin{equation}
        \begin{aligned}
            &SplitEntropy(S,A) = - \sum_{V \in Values(A)} \frac{|S_{v}|}{|S|}log \frac{|S_{v}|}{|S|}\\
            &GainRatio(S,A) = \frac{Gain(S,A)}{SplitEntropy(S,A)}
        \end{aligned}
    \end{equation}

    not how the gain will be lower when we have too many or too few subsets, 
    the split ratio will be the highest with higher information gains which result in 2 subsets that each take half of the data. 
    }

\nDefinition{Overfitting}{
    Overfitting happens when the model learns the "noise" in the training data, and becomes too specific.

    Decision trees will always classify training examples perfectly if we make them big enough (because eventually we will split the data into singletons, and those instances will always be labelled as the correct instances)

    This is an example of \textbf{overfitting}, we need to limit the size of the tree in order to counteract this, i.e. force some subsets to end up being non-pure in order to stop the model from becoming too specific to our training data.


    \begin{center}
        \nImg[0.6]{dt_overfitting}
    \end{center}

    \nHeader{Solutions}

    \begin{itemize}
        \item Stop splitting when not statistically significant
        \item Grow the tree fully and then prune in a way which maximises the accuracy (using a validation set)
            \subitem- \textbf{Sub-tree replacement pruning}: pretend you remove a node and all its children, measure the performance on validation set. Remove node with highest performance gain, repeat untill removal is harmful to performance(greedy)
    \end{itemize}
}

\nDefinition{Numeric attributes}{
    How can we apply this to numeric attributes? Split based on \textbf{thresholds}, threshold can be picked via some process.
    The easy solution is to take all the values for the attribute we are evaluating a split on, sort them and go through the split on each of those one-by-one.

    \textit{Note: this will lead to one attribute possibly being split multiple times since we can always divide the values into smaller subsets}
}

\nDefinition{Multi-class classification}{
    We can apply decision trees to multi-class classification by predicting the most frequent class in the subset we land on. 
    The entropy calculation becomes:
    \begin{equation}
        H(S) = - \sum_{c} p_{c} \log_{2}p_{c}
    \end{equation}

    where $p_{c}$ is the fraction of samples of class c in the subset
}
\nDefinition{Regression}{
    We can also apply this algorithm to regression by taking the average of the training examples in the subset we land on as the predicted output value.
    In which case instead of maximising gain we grow the tree by minimising variance in the subsets.
    (pick split which reduces variance in the output the most!)
    We can also replace the average by linear regression at the leaves.

}

\nDefinition{Random decision forest}{
    \begin{itemize}[noitemsep]
        \item Pick a random subset $S_{r}$ of training examples 
        \item grow a full tree $T_{r}$ (without pruning)
        \item when splitting pick from $d << D$ random attributes (i.e. hide some attributes for some trees)
        \item compute gain based on $S_{r}$ instead of full set
        \item repeat K times to make K trees
    \end{itemize}

    Given a new data point X we can then:
    \begin{itemize}[noitemsep]
        \item classify X using each of the trees $T_{1}\hdots T_{k}$
        \item use majority vote: class predicted most often as the label!
    \end{itemize}

    \textit{\color{green} State-of-the-art performance in many domains, very good stuff}
}
\nDefinition{Evaluation of decision trees}{
    \begin{itemize}[noitemsep]
        \item[+] \textbf{interpretable} - humans can understand the underlying decisions! (ONLY ALGORITHM IN THIS COURSE TO HAVE THIS PROPERTY)
        \item[+] easilly handles irrelevant attributes (gain = 0)
        \item[+] can handle missing data 
        \item[+] very compact: nodes $<<$ D after pruning
        \item[+] very fast at testing time O(depth)
        \item[-] only axis-aligned splits are possible (because we split only taking into account one dimension at a step)
            \nImg[0.4]{dt_axis_aligned} 
        \item[-] greedy (may not find best tree globally)
        \item[-] exponentially many possible trees       
    \end{itemize}

    \nHeader{Why trees?}
    Can model non-linear, weirdly shaped data which does not really follow a nice probability distribution (i.e. \textbf{non-monotonic} data!)
    \begin{center}
        \nImg{nb_vs_dt}
    \end{center}
}

\nSection{Generalisation \& Evaluation}

\nDefinition{Over \& Under fitting}{
    \nHeader{Overfitting}
    If we can find a model which makes more mistakes on the training data but fewer mistakes on unseen future data, our model has overfitted.

    This can happen for many reasons:
    \begin{itemize}[noitemsep]
        \item Predictor is too "complex" (too flexible)
        \item It fits to the "noise" in the training data
        \item Captures patterns which will not-re appear in future data
    \end{itemize}

    \nHeader{Underfitting}

    If we can find another predictor with smaller training error and smaller error on future unseen data, our model has underfitted.

    This can happen for many reasons:
    \begin{itemize}[noitemsep]
        \item Predictor is too simplistic (too rigid)
        \item Not powerful enough to capture salient patterns in data 
    \end{itemize}

    \nHeader{Examples}

    \begin{center}
        \nImg[0.7]{under_over_fitting}
    \end{center}
}

\nDefinition{Flexible vs Inflexible}{
    Every dataset requires a different level of flexibility. How much depends on the complexity of the task and the available data. We need a "knob" to vary the "flexibility" of our predictor.

    Most learning algorithms have such knobs:
    \begin{itemize}[noitemsep]
        \item Regression: order of the polynomial 
        \item Naive Bayes: number of attributes, limits on model parameters
        \item Decision Trees: \#nodes in the tree / prunning confidence
        \item kNN: number of nearest neighbours
        \item Support Vector Machines: kernel type, cost parameter  
    \end{itemize}

    With a small amount of training data we require more rigit models (less chance for overfitting).
    For simpler tasks we also pick more rigid models. For complex tasks we want a more flexible/complex model
}

\nDefinition{Training vs Generalization error}{
    The training error can be defined as the average error over the training data (error here can be any error metric):
    \begin{equation}
        E_{train} = \frac{1}{n}\sum_{i=1}^{n} error(f_{D}(\mathbf{X_{i}}),y_{i})
    \end{equation}
    The generalization error however, is the general error on future data.
    Problem is we don't know what the future data will be, we do however know the possible range of input data.

    Generalization error is defined as follows:
    \begin{equation}
        E_{gen} = \int error(f_{D}(\mathbf{X}),y)p(y,\mathbf{X}) \; d\mathbf{x}
    \end{equation}

    i.e. the sum over all possible input values of the error multiplied by the probability of seeing each of those input values given the class.

    We can never compute this as we do not know $p(y,\mathbf{X})$, since that is what we're trying to model!

    We can however estimate the generalization error, by treating the \textbf{testing} error as an estimate of it! (NOTE NOT THE TRAINING ERROR, NEVER USE THE TRAINING ERROR AS YOUR ACCURACY)
    
}

\nDefinition{Estimating Generalization error}{
    The testing error can be used to estimate generalization error:
    \begin{equation}
        E_{test} = \frac{1}{n}\sum_{i=1}^{n} error(f_{D}(\mathbf{X_{i}}),y_{i})
    \end{equation}

    If the testing set is an \textbf{unbiased} sample from the population of input data, i.e. unbiased sample of input values, then the following holds:
    \begin{equation}
        \lim_{n\rightarrow \infty}E_{test} = E_{gen}
    \end{equation}

    I.e. as our sample size grows to infinity, our testing error converges onto the generalization error!
    How close are the two together ? this depends on n

    \nHeader{Confidence interval}

    We can never actually gain the value for $E_{gen}$ but we can estimate it using a previously unseen sample of the data, i.e. the test set.
    This estimate error $E_{test}$ is going to be within some interval of $E_{gen}$.
    We can try to estimate how close to the real error we are using a confidence interval.
    Stipulate that we are within $\Delta E$ of the generalization error such that 95\% of future test sets fall within interval :
    \begin{equation}
        E_{test} \pm \Delta E
    \end{equation}

    E here is the probability that our system will misclassify a random instance (i.e. true error)

    We can estabilish the confidence interval into which p\% of future tests will fall within,  as:
    \begin{equation}
        \begin{aligned}
            &CI = E \pm \sigma \cdot \phi^{-1} \left( \frac{1-p}{2} \right)\\
            &\phi^{-1}(x) = \sqrt{2} erf^{-1}(-p)
        \end{aligned}
    \end{equation}
    With phi calculating the number of standard deviations of the normal distribution which covers p\% of the future test sets.

    we can now:
    \begin{itemize}[noitemsep]
        \item take a random set of n instances, how many will be miss-classified ?
        \item flip E-biased coin n times, how many heads will we get ?
        \item this is the binomial distribution with mean = nE, variance = n E (1-E)
        \item $E_{future} = \# misclassified / n$, this can be approximated with a gaussian: with mean E, variance E(1-E)/n
        \item $\frac{2}{3}$ future tests will have error in $E \pm \sqrt{E(1-E)/n} = \sigma^{2}$
        \item Assuming E = 0.25 the confidence interval for future error will then be:
            \subitem- for n=100 examples, p=0.95: 
                \begin{equation}
                    \begin{aligned}
                        &CI = 0.25 \pm 1.96 \cdot \sigma = 0.25 \pm 0.08
                    \end{aligned}
                \end{equation}   
            \subitem- for n=1000 samples, p = 0.99
            \begin{equation}
                CI = 0.25 \pm 0.11 
            \end{equation}
            \subitem- for n=10000, p=0.95
            \begin{equation}
                CI = 0.25 \pm 0.008
            \end{equation}
    \end{itemize}
    }

\nDefinition{Data set splits}{
    The test + training set split may not be enough, we also need a validation set to pick the right algorithm and "fine-tune" its parameters:
    \begin{itemize}[noitemsep]
        \item Training set: construct basic classifier (Naive Bayes: count frequencies, DT: pick attributes to split on)
        \item Validation set: pick the algorithm + fine-tune settings
            \subitem- select best-performing algorithm
        \item Testing set: Estimate future error rate. Cannot be same as validation set, since the algorithm would overfit to the validation set after working out the parameters!
            \subitem- {\color{red} never report best of many runs, run once, or report results of every run} 
    \end{itemize}
}

\nDefinition{Holdout method}{
    The most natural way to split up your data set is to pick a fraction of it as the training set, and the rest as the test/validation set, this is known as the \textbf{holdout} method
}

\nDefinition{K-fold Cross-validation}{
    \textbf{Cross-validation} is a technique for dealing with small data set sizes. 
    Ideally we want to balance many goals:
    \begin{itemize}[noitemsep]
        \item We want to estimate future error as accurately as possible - keep $n_{train}$ as big as possible
        \item We want to learn the classifier as accurately as possible - keep $n_{test}$ as big as possible 
    \end{itemize}

    But both the training and test sets cannot overlap! 
    We can however, split and alternate the sets in such a way to cover more ground:
    \begin{itemize}[noitemsep]
        \item Split dadta into k sets
        \item test on each k in turn, while training on k-1 others 
        \item average the result over k folds (why can we do that? because we never use one instance as both training/testing instance at the same time, also we are not averaging the performance of one model over number of runs, we build k different models!) 
    \end{itemize}
    \textit{Note: while we call the sets test/train, we can use them for whatever purposes, the test set is actually likely going to be our validation set, and using cross-validation does not really set aside data for a full test set}
    }
\nDefinition{Leave-one-out}{
    Same thing as cross-validation ,but with K = n, i.e. each data point becomes a test set and the rest the training set.

    \begin{itemize}
        \item[+] best possible classifier is learned: n-1 training examples
        \item[-] high computational cost: re-learn everything n times 
        \item[-] classes are not balanced in training / testing sets  
            \subitem- extreme example: say we have 2 equ-probable classes and \textbf{zero attributes}, this will lead to training set of n/2 samples of B and n/2 - 1 of A. Then the model must estimate that B is always the most likely, but the instance in the test set is of class A (same thing happens if B is in the test set, model is then biased towards A), so the model will always be wrong.    
    \end{itemize}
}

\nDefinition{Stratification}{
    All the above splitting methods disregard the class balance in the splits, this might be a problem.

    The process of ensuring each split is always populated with equal proportion of class instances, is called \textbf{stratification}
}

\nDefinition{Basics of classification measures}{
    \nImg[0.9]{classification_measures}
}

\nDefinition{Accuracy/Classification Error}{
    The percentage of correct classifications, or distance from real value to the predicted value in regression.
    This is not a great measure when the A priori are uneven, i.e. one class is more frequent

    \begin{equation}
        Acc = \frac{errors}{total} = \frac{FP + FN}{TP + TN + FP + FN}
    \end{equation}

    \begin{center}
        \nImg{accuracy_bad}
    \end{center}
    
}
\nDefinition{Misses and false alarms}{
    We can devise error metrics which aim to take into account the false positives and negatives more:

    \begin{equation}
        \begin{aligned}
            &False Alarm Rate = False Positive Rate = \frac{FP}{FP + TN}\\
            &\text{\% of negatives we misclassified as positive}\\
            &Miss rate = False Negative Rate = \frac{FN}{TP+FN}\\
            &\text{\% of positives we misclassified as negative}\\
            &Recall = True Positive Rate = \frac{TP}{TP+FN}\\
            &\text{\% of positives we classified correctly}\\
            &Precision = \frac{TP}{TP+FP}\\
            &\text{\% of the things we predicted positive which were actually positive (dummy classifiers could be realy bad here)}\\
        \end{aligned}
    \end{equation}

    We can use a combination of these to quantify the performance of a model, depending on what's more important and on the domain.
    i.e. it's trivial to get 100\% recall or 0\% false alarm () 

}

\nDefinition{Utility and cost}{
    We can combine these metrics into a single utility measure. If we know the cost of false positives, or negatives (i.e. how costly is it when our system misclassifies something, preventive measures, evacuation, or cost of recovery, reconstruction)
    We can also evaluate how costly our model is in our domain.

    \begin{itemize}
        \item Detection cost: weighted average of FP, FN rates, good for event detection (earthquakes)
            \begin{equation}
                Cost = C_{FP} * FP + C_{FN} * FN
            \end{equation}
        \item F-measure: harmonic mean of recall, precision, good for information retrieval, internet searches (takes out true negatives of the equation)
            \begin{equation}
                F1 = 2/ (1/Recall + 1/ Precision)
            \end{equation}
        \item Domain-specific measures:
            \subitem- e.g. observed profit/loss from +/- market prediction
    \end{itemize}
}

\nDefinition{Classification thresholds}{
    Say two systems have the following performance:
    \begin{itemize}
        \item A: True positive = 50\%, False Positive = 20\%
        \item A: True positive = 100\%, False Positive = 60\%
    \end{itemize}

    which is better ? (assume no-apriori utility)
    \begin{itemize}
        \item very misleading question, A and B could be \textbf{THE EXACT SAME SYSTEM} but operating at different \textbf{thresholds}
    \end{itemize}

    \nHeader{ROC curves}
    many algorithms computeu "confidence" c(x)
    i.e. the threshold above which they classify something as a positive. For naive bayes this is naturally 0.5

    this threshold determines the error rates of the model.
    FP rate = $P(c(x) > t | ham)$, TP rate = $P(c(x) > t| spam)$

    We can evaluate the performance of the model across the entire spectrum of t's using \textbf{Receiver Operating Characteristic} curves.
    Simply plot the TPR vs FPR as t varies from $\infty $to $-\infty$

    \begin{center}
        \nImg[0.9]{roc_curves}
    \end{center}

    The AUC is a good metric for comparing different classifiers which can operate over a variety of thresholds.
}

\nDefinition{Evaluating regression}{
    All the above methods deal with discrete, classifiers. How do we evaluate regression ?
    There are many error measures for regression:

    \nHeader{Mean Squared Error}
    \begin{equation}
        \frac{1}{n}\sum_{i=1}^{n}f(x_{i}-y_{i})^{2}
    \end{equation}

    \textit{popular, well understood, differentiable but {\color{red}sensitive to outliers, asingle prediction which is off by 10, is equivalent to 100 predictions off by 1.}}
    \textit{\color{red}also sensitive to mean and scale}

    \begin{center}
        \nImg[0.4]{error_offset}
    \end{center}
    \begin{center}
        here orange predictor is not as bad, but will yield high error
    \end{center}

    \nHeader{Mean Absolute Error}
    \begin{equation}
        \frac{1}{n}\sum_{i=1}^{n}|f(x_{i}-y_{i})|
    \end{equation}
    \textit{less sensitive to outliers, many small errors = one large error. {\color{red}Sensitive to mean and scale}}

    \nHeader{Median Absolute Deviation}
    \begin{equation}
        median(|f(x_{i}-y_{i})|)
    \end{equation}

    \textit{robust, completely ignores outliers. {\color{red} difficult to wor with - cannot take derivatives, sensitive to mean and scale}}
    \nHeader{Correlation coefficient}
    \begin{equation}
        \frac{n \sum_{i=1}^{n}f(x_{i}-\mu_{f})(y_{i}-\mu_{y})}
        {\sqrt{n \sum_{i=1}^{n}f(x_{i}-\mu_{f})^{2}(y_{i}-\mu_{y})^{2}}}
    \end{equation}
    Basically ignores the "position" of your prediction function and likes predictors of similar shape to the data (without considering mean and scale)
    \textit{insensitive to mean \& scale, good for ranking tasks. Intuition: is your output small as target is small and vice-versa?}
}

\nSection{Linear regression}
\nSection{Logistic regression}
\nSection{Optimisation \& Regularisation}
\nSection{Support Vector Machines}
\nSection{Ethics}
\nSection{Nearest Neighbours}
\nSection{K-Means}
\nSection{Gaussian mixture models}
\nSection{Principal components analysis}
\nSection{Hierarchical Clustering}
\nSection{Perceptrons}
\nSection{Neural networks}

















\end{document}
