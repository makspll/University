\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{IAML Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{IAML}

\nSection{Introduction}

\nDefinition{Machine Learning}{
    A machine learning model \textbf{takes in} data, \textbf{outputs} predictions. It's a function of data really together with a set of training data.

    Learning = Representation + Evaluation + Optimisation
}
\nSection{Thinking about data}

\nDefinition{Classification}{
    Sort data points into discrete buckets based on training data 
}

\nDefinition{Regression}{
    Output a continuous/real value for each data point based on training data. 
}

\nDefinition{Clustering}{
    Detect which data points are related to which other data points, find outliers.
}

\nDefinition{Data representation}{
    What format do we feed the data in ? Most likely as a \textbf{bag of features}. I.e. collection of attribute-value pairs, every data point must have an attribute-value pair for each property (in most cases)
    
    Data representation has more impact on the performance of your ML algorithm than anything.

    \nHeader{Types of attributes}

    \begin{itemize}[noitemsep]
        \item \textbf{Categorical}
            \subitem- e.g. red/blue/brown
            \subitem- a set of possible \textbf{mutually exclusive} values
            \subitem- meaningful operators: equality comparison
            \subitem- usually represented as numbers 
            \subitem- problems: {\color{red} synnonymy is a major challenge e.g. some values might mean the same thing to a human but not to the machine (country == folk?)}   
        \item \textbf{Ordinal}
            \subitem- e.g. poor $<$ satisfactory $<$ good $<$ excellent
            \subitem- a set of possible \textbf{mutually exclusive} values, but with a \textbf{natural ordering}
            \subitem- meaningful operators: equality comparison, sorting
            \subitem- problems: {\color{red} sometimes hard to differentiate from categorical (single $<$ divorced)?}    
        \item \textbf{Numeric}
            \subitem- e.g. 3.1/5
            \subitem- meaningful operators: arithmetic, distance metrics, equality, sorting 
            \subitem- problems:
                \subsubitem- {\color{red} sensitive to extreme outliers (handle these \textbf{before normalization})}
                \subsubitem- {\color{red} skewed distributions (assymetric) - outliers might actually be real data (e.g. personal wealth data)}
                \subsubitem- {\color{red} Non-monotonic effect of attributes - e.g. predicting someone is going to win a marathon, here the relationship is not monotonic i.e. norect correlatio din, might be a curve with a "sweet spot"}
            \subitem- solutions:
                \subsubitem- \textit{Deal with outliers, maybe trim them for training phase only?}
                \subsubitem- \textit{use a log/atan scale to make data more linear}
                \subsubitem- \textit{discretize data into buckets}
    \end{itemize}
}
\nDefinition{Normalisation}{
    Normalization is the process of converting all the data such that each different attribute is roughly in the same range, and comparable.

    Normalization is mostly necessary for linear methods.
}


\nDefinition{Picking attributes}{
    We want:
    \begin{itemize}[noitemsep]
        \item all our attributes to have similar values if the data points that posses them are similar themselves!    
        \item small change in input $\rightarrow$ small change in values
    \end{itemize} 
    
    \nHeader{Images}

    For images, can we use pixel data as attributes directly ? it depends, if the pixel 20,20 always corresponds to the middle of a letter then yes, this is a meaningful attribute which might help us discern whether digits given have strokes going through the middle of the image!
    What if the pixel is always something random ? This could happen whenever we are looking for an object which can be anywhere in the image, in which case the attribute would be gibberish!
    In the case of image classification, we want attributes which are:
    \begin{itemize}
        \item invariant to size 
        \item invariant to rotation 
        \item invariant to illumination
    \end{itemize}
    
    How can we classify whether an image contains a desired object ? In general we do the following:
    \begin{itemize}
        \item \textbf{Segment} the image, into regions of pixels which we believe are related (by colour, texture, position etc..)
        \item Pick a number of attributes which you will use to describe each of the regions
        \item Then use those features in your machine learning algorithm!
    \end{itemize}
    Keep in mind though, the segmentation \textbf{will} make errors, we can hope that these will be consistent across all images (all images will have their legs chopped off). Sometimes we can segment using a grid
    
    \nHeader{Text}
        For textual data, often we can use the \textbf{bag-of-words} approach. I.e. we can form an attribute vector which counts the amount of occurrences of each word, regardless of position. This is invariant to word shuffling for example.
    \nHeader{Sound}
    For sound, the data is sound waves. How can we select attributes here? We can count the number of different frequencies occurring in the piece, (using Fourier's transform) and treat this as a feature vector!
    \begin{center}
        \nImg{attributes_sound}
    \end{center}
}



\nDefinition{Supervised Learning}{
    Supervised learning algorithms have some sort of "performance" metric they can use, i.e. test labels they can validate their guesses on.
    When the algorithm can measure accuracy directly it's a supervised algorithm.
}

\nDefinition{Unsupervised Learning}{
    Learning without a specific accuracy measure available. Algorithms in this area usually look for structure/patterns/information in the data which can be helpful in other ways.
    There is nothing specific the algorithm is looking for.
    Can be \textbf{direct} when the algorithm helps to make sense of the data directly, or \textbf{indirect} when it is "plugged" into another machine learning aglorithm as an attribute itself. 
}

\nDefinition{Semi-supervised Learning}{
    Using unsupervised methods to improve supervised algorithms. Usually have a few labelled examples but lots more unlabelled. 
}

\nDefinition{Multi-class classification}{
    Classification with multiple mutually exclusive labels/classes.
    
    Might be hard to tell when something belongs to none of the available classes.
}
\nDefinition{Binary classification}{
    Classification with 2 mutually exclusive labels/classes in each "run". 
    This way of classification can be applied to multiple-classes classification but with a "One-vs-Rest" meta-strategy (a vs not a, b vs not b).
    In this way a sample may belong to multiple classes but never to two sides of the one-vs-rest structure simultaneously in each run.
    
    In this classification method we can actually tell when something doesn't belong to any class!
}

\nDefinition{Analysing data}{
    We have to check for a number of things in our data sets:
    \begin{itemize}[noitemsep]
        \item Are there any dominating classes ? what would the best "dummy" model do ? always predicting no ?
        \item What should we use as the appropriate error metric ? How important are the false positives vs the false negatives ? 
    \end{itemize}
}

\nDefinition{Generative model}{
    A generative model, develops a probabilistic model of each class, i.e. tries to "model" the underlying probability distribution directly.
    The decision boundary becomes implicitly defined by the probabilities of each input being in each class.
}

\nDefinition{Discriminative model}{
    A discriminative model ignores the underlying model and tries to "separate" the data, i.e. it tries to model the boundaries that divide the classes.
    \textbf{Not designed to use unlabeled data} so cannot be used for unsupervised tasks.
}

\nDefinition{Dealing with data structure}{
    What do we do if the data input has some sort of hierarchical structure ?  Where the position of occurrence of a node affects its meaning?
    We can encode as attributes the existence of root-to-leaf paths in the entire tree, and use this bag-of-words approach to perform machine learning 
    \begin{center}
        \nImg{structure_input}
    \end{center}

    What if we need to predict the output structure from the input structure ? This is very difficult, but we can "trick" our classifier and turn this more into a search problem by embedding the possible outputs with each input and classifying on that instead:

    \begin{center}
        \nImg[0.9]{structure_output}
    \end{center}
    This of course means we have to search for all possible output structures!

    }

\nDefinition{Dealing with outliers}{
    \textbf{Outliers} are isolated instances of a class that are unlike any other instance of that class. These affect all learning models to some degree.

    There are some ways we can deal with outliers. One method is to remove the outliers just before we perform any sort of normalisation on the data, (ONLY FOR THE PURPOSES OF TRAINING!!)
    We can also put a confidence interval around our data, and removing values outside of those intervals (with x,y values outside of a normal range).
    Some data points might still be outliers even though they are within expected x,y ranges! (second figure)
    
    \begin{center}
        \nImg{outliers}    
    \end{center}

    Best way to deal with outliers ? \textbf{VISUALISE YOUR DATA}
    }



\nSection{Naive Bayes}

\nTheorem{Bayes rule}{
    \begin{equation}
        P(y|x) = \frac{P(x|y) P(y)}{\sum_{y'} P(x|y')P(y')}
    \end{equation}

    where:
    \begin{itemize}[noitemsep]
        \item y : one of the classes
        \item x : the input data, feature vector
        \item P(y) : \textbf{prior}, the probability of seeing elements of class y in general prior to making any observations
        \item $P(x|y)$ : \textbf{class model/likelihood}, given the data is in class y, how likely are the features that i am seeing
        \item P(x) = $\sum_{y'}P(x|y')P(y')$ : \textbf{normalizer}, does not affect the probabilities, but without this term we cannot compare data in terms of probability of being in each class (i.e. for confidence values)
    \end{itemize}

    \textit{Note: this works with probability densities too (which we're using almost exclusively)}
}

\nDefinition{Naive bayes}{
    Bayesian classifier, which assumes \textbf{conditional independence} between different attributes. It attempts to model the underlying probability distribution so it's a \textbf{generative} model.

    I.e.:
    \begin{equation}
        P(\vec{x}|y) = \prod_{i=1}^{d} P(x_{i}|x_{1} \hdots x_{i-1},y)=  P(x_{1}|y) \cdot P(x_{2}|y) \cdot P(x_{3}|y) \hdots
    \end{equation}
    It assumes that there are no correlations between the variables themselves, and any correlations are explained by the fact they belong to the same class.

    \nHeader{Example}
    The probability of going to the beach and getting a heat stroke are not independent: $P(B,S) > P(B)P(S)$.
    The may be independent if we know the weather is hot (i.e. external factor is what actually causes the dependence, which we can factor into the equation):
    $P(B,S|H) = P(B|H)P(S|H)$ This hidden factor in naive bayes is \textbf{the class}
}

\nDefinition{Gaussian Naive bayes}{
    A Naive Bayes classifier which uses the gaussian distribution as a class model for each class.

    \begin{equation}
        \LARGE
        p(x|y) = \frac{1}{\sqrt{2\pi \sigma_{x,y} ^{2}}}\exp^{-\frac{1}{2} \frac{(x - \mu_{x,y})^{2}}{\sigma_{x,y}^{2}}}
    \end{equation}

    With:
    \begin{itemize}[noitemsep]
        \item $\mu_{x,y}$ : the mean of the gaussian modelling class y's attribute x
        \item $\sigma_{x,y}^{2}$ : variance of the gaussian modelling class y's attribute x
        \item $x$ : the value of the attribute
        \item $y$ : the class 
    \end{itemize}
    \textit{Note: this is not actually a probability per-se, this is a p.d.f function which can be higher than 1}
    }

\nDefinition{Problems with Naive Bayes}{
    \nHeader{Covariance}
    Naive bayes cannot model covariance of the attributes, i.e.:
    \begin{center}
        \nImg{covariance_nb}
    \end{center}
    in the above case the only thing differentiating the classes is their relationship between x and y, i.e. the two attributes. Naive bayes can only model 
    \textbf{spherical} distributions
    
    \nHeader{Zero frequency problem}
        Should any data never appear under any of the classes the probability of it belonging to that class is always going to be zero.
        So say we are classifying emails as spam or non-spam, and the word "now" never occurred in non-spam emails, but had occurred in some spam emails. Any sentence containing the word "now" would always be classified as spam no matter the actual probability of it being spam!

        Solution, add a small epsilon to all "feature counts", or perform laplace smoothing: 
        \begin{equation}
            P(w,c) = \frac{num(w,c) + \epsilon}{num(c) + 2\epsilon}
        \end{equation}
        
        Zipf's law: 50\% of words occur once 
        \nHeader{Independence assumption}

        Continuing with the spam example, every word contributes to the total probability independently, easy to fool. Simply stuff lots of non-spammy words into email
    
        }

\nDefinition{Deaing with missing data}{

    Suppose we have missing data for some attribute i of a sample x. How can we compute the likelihood of this sample belonging to any class ? 

    We simply ignore it:
    \begin{equation}
        P(x_{1}\hdots x_{i} \hdots x_{d}|y) = \prod_{k \neq i}^{d} P(x_{i}|y)
    \end{equation}

    There is no need to fill in the gaps artificially because missing attribute data essentially has a probability of 1: 
    
    \begin{center}
        \nImg[0.8]{nb_missing_data}
    \end{center}
}


\nSection{Decision Trees}
\nSection{Generalisation \& Evaluation}
\nSection{Linear regression}
\nSection{Logistic regression}
\nSection{Optimisation \& Regularisation}
\nSection{Support Vector Machines}
\nSection{Ethics}
\nSection{Nearest Neighbours}
\nSection{K-Means}
\nSection{Gaussian mixture models}
\nSection{Principal components analysis}
\nSection{Hierarchical Clustering}
\nSection{Perceptrons}
\nSection{Neural networks}

















\end{document}
