\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}

\usepackage[a4paper,margin=0.5in]{geometry}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{IVR Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak
\nSection{Introduction to Vision}

\nDefinition{Computer vision}{Processing data from any modality which uses the electromagnetic spectrum and produces an image}
\nDefinition{Image}{Way of representing data in a picture-like format, with a direct correspondence to the scene being imaged}
\nDefinition{CCD Camera}{
    Charged couple device, light falls on an array of MOS capacitors (which are rectangular and not square). The capacitors form a shift register and output either a line at a time or the whole array at one time (line vs frame transfer)
    
    \centering
    \includegraphics[width=0.5\columnwidth]{ccd_camera.png}

    \raggedright
    these "buckets" can overflow, resulting in over-saturation of the image
    }
\nDefinition{Frame grabber}{Device which converts analog image signals to digital image signals. Essentially puts a discrete value on each pixel signal. 24bit color is usually required for robotics}
\nDefinition{Visual Erosion}{RGB is a function of the sensitivity of the sensor to reflected light of each color. The sum of those intensities may vary wildly from frame to frame depending on the distance of the object due to intensity of the reflected light. The object appears to "errode" with changes in lighting. CCD Cameras are also notoriously insensitive to red, meaning that one of the three color planes is not as helpful in distinguishing colors. HSI and SCT colour spaces aim to reduce visual erosion since the Hue - the main wavelength measured (\textbf{perceptually meaningful dimensions}) will not change with the object's relative position, only its saturation and intensity will! Equipment to capture HSI images is expensive, and conversions between colour spaces sometimes fail.}
\nDefinition{Region Segmentation}{Finding groups of pixels related to each other via color, within a certain threshold and identifying the centroids of those groups. Requires high contrast between the \textbf{foreground} (object of interest) and the \textbf{background} to work well.}

\nDefinition{Color histogramming}{
    a type of histogram (bar chart basically), the user specifies range of values for each bar, 
    (bucket) the size of the bar is the number of data points falling within the bar's "range". 
    These ranges could be set to capture different values of either the R,G,B color intensities.

    \centering
    \includegraphics[width=0.7\columnwidth]{"histogram.png"}

    \raggedright
    Such histograms can be \textbf{subtracted bucket-wise} from each other as a form of distance measure to compare image stimuli.
}

\nDefinition{Stereopsis}{The method of triangulating depth data from 2 POV's}
\nDefinition{Stereo camera pairs}{Usage of two cameras to extract range data by finding the same point on the images received from two (most likely parallel) cameras, and then finding the depth information using the geometry of the cameras. It can be hard to find the same point on two pictures ()\textbf{correspondence problem}), the method of picking a spot of interest is called an \textbf{interest operator}. Cameras can be mounted in parallel to produce \textbf{rectified images} (the distance between the two cameras is then known as the \textbf{disparity}). This can save computation time since the point of interest will appear in the same line of the image on both cameras (\textbf{epipolar lines})}

\nDefinition{Optic flow}{Information to do with: Shadow cues, texture, expected size of objects}
\nDefinition{Light stripping}{
    Method of projecting a pattern of light onto a surface of interest and observing the distortion to the pattern to visualise the surface and/or distance information. Does not work that well in natural conditions due to noise.
    
    \centering 
    \includegraphics[width=0.4\columnwidth]{"light_stripper.png"}
    
}

\nDefinition{Laser ranging}{like radar but using light (\textbf{lidar}), scanning components are expensive, a planar laser range finder is a cheaper alternative. Produces an intensity and range map.}
\nDefinition{Range segmentation}{Segmenting the image based on range data, can be used to determine the geometry of surfaces}

\nSection{Image Basics}


\nTheorem{Homogenous coordinates}{
    Homogenous (aka similar) coordinates are coordinates in space with one more dimension than in the corresponding cartesian space, in this space we can express linear translations as linear matrix transformations!
    Every point in the cartesian space becomes a line in the homogenous space!

    \centering
    \includegraphics[width=0.5\columnwidth]{homogenous.png}

    \raggedright
    Conversion to homogenous coordinates:
    \begin{equation}
        \begin{bmatrix}
            x \\ y \\ \vdots \\  
        \end{bmatrix}
        = 
        \begin{bmatrix}
            x \\ y \\ \vdots \\ 1
        \end{bmatrix}
    \end{equation}

    Conversion from homogenous coordinates
    \begin{equation}
        \begin{bmatrix}
            x \\ y \\ \vdots \\ w
        \end{bmatrix}
        = 
        \begin{bmatrix}
            x/w \\ y/w \\ \vdots
        \end{bmatrix} 
        \quad w \neq 0
    \end{equation}

    Notice how a point in homogenous space can be multiplied by any constant, and yet when it is converted back to normal space, it becomes the same point. \textbf{The ratio} between the components defines the line in homogenous space.
}

\nDefinition{Pinhole camera}{
    
    Capturing on a simple plane does not work because multiple rays from the same point in the scene travel to multiple parts of the film. We want the film to capture a single "ray" per point of interest
    \begin{center}
    \begin{tabular*}{\columnwidth}{m{0.5\columnwidth}m{0.5\columnwidth}}
        \includegraphics[width=0.45\columnwidth]{pinhole_failure.png}&
        \includegraphics[width=0.45\columnwidth]{pinhole_failure_2.png}
    \end{tabular*}
    \end{center}

    A camera setup using a tiny hole to filter and hence focus the light onto a single clear image.

    \centering 
    \includegraphics[width=0.6\columnwidth]{pinhole.png}
    
    \raggedright
    Using similar triangles, the point P:$(X,Y,Z)$ maps to point P' on the 2d surface of the image plane, at a distance f (\textbf{focal length}) from the pinhole as follows: 
    \begin{equation}
        x = \frac{-fX}{Z},
        y = \frac{-fY}{Z},
        z = f
    \end{equation}

    This projection of scene point to camera point can be expressed as a linear matrix transformation in homogenous space:

    \begin{equation}
        P_{h} =
        \begin{bmatrix}
            X \\ Y \\ -Z/f
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & -1/f & 0 
        \end{bmatrix}
        \begin{bmatrix}
            X \\ Y \\ Z \\ 1
        \end{bmatrix}
    \end{equation}

    To retrieve the projected point in cartesian space we simply divide by the third coordinate and discard it.

    \begin{equation}
        P_{c} = 
        \begin{bmatrix}
            X / (-Z/f) \\ Y / (-Z/f)
        \end{bmatrix}
        = 
        \begin{bmatrix}
            -fX/Z \\ -fY/Z
        \end{bmatrix}
    \end{equation}
    Which is identical to the projection above.

    This projection, preserves straight lines (\textbf{colinearity}) and their intersections, but looses information about angles and lengths (due to multiple points in 3D possibly mapping to the same point in 2D)
    
    Lines directly passing through the focal point are projected as points.
    
    Planes are preserved but those passing through the focal point are projected as lines.
}


\nTheorem{Vanishing point}{
    Any two parallel lines will converge to a certain point on the image as long as their directions are the same
    
    \begin{center}
    \begin{tabular*}{\columnwidth}{m{0.5\columnwidth}m{0.5\columnwidth}}
        \includegraphics[width=0.45\columnwidth]{vanishing_point.png}&
        \includegraphics[width=0.45\columnwidth]{vanishing_road.jpeg}
    \end{tabular*}
    \end{center}
}

\nDefinition{Detector response curve}{
    The curve showing which frequencies of light a detector perceives the most and which will dominate the actual "perceived" or "central" wavelength of light, 
    i.e. the curve showing which wavelength of light a detector is most sensitive to.
    Each sensor type acts as a filter to the incomming light, and can produce an output signal proportional to the amount of its central wavelength absorbed.
    \\
    \par
    The wavelength signal perceived is a function of many things:
    \begin{itemize}
        \item type of source light
        \item the reflective properties of the objects in the scene 
        \item the sensor detector curve
    \end{itemize}

    As such knowing the "real" wavelength of the light is very difficult.
    
    \begin{center}
    \begin{tabular*}{\columnwidth}{m{0.5\columnwidth}m{0.5\columnwidth}}
        \includegraphics[width=0.45\columnwidth]{sensor_curves.png}&
        \includegraphics[width=0.45\columnwidth]{sensor_function.png}
    \end{tabular*}
    \end{center}
}

\nSection{Problems with image capture}

\nDefinition{Focus problems}{
    Focus set to one distance, and other nearby distances in focus (depth of focus). Further or closer not so well focused.
    
    \centering
    \includegraphics[width=0.3\columnwidth]{focus_problem.png}
    
    \raggedright
    Solutions: Use smaller aperture and brighter light
}

\nDefinition{Shadow problems}{
    False colours due to different intensity of light (shadows) make it difficult to separate shapes of interest from shadows.
    (is the white part under this part a shadow or the edge ?)

    Main cause of problem: point of light sources, the perceived brightness at a surface is proportional to the \textbf{square} of the distance between the surface and the light source.

    \centering
    \includegraphics[width=0.3\columnwidth]{shadow_problems.png}
    \includegraphics[width=0.6\columnwidth]{diffuser_panels.png}

    \raggedright
    Solutions: increase ambient lighting by using diffusing panels or lots of point lights
    }

\nDefinition{Specularities/highlights}{
    (Saturated pixels set to red)

    \centering
    \includegraphics[width=0.3\columnwidth]{specularities.png}
    
    \raggedright
    Solutions: increase ambient lighting by using diffusing panels or lots of point lights, or use smaller aperture, reduce gain and adjust gamma
    
}

\nDefinition{Non-uniform ilumination}{
    Contrast on background enhanced: may cause analysis problems

    \centering
    \includegraphics[width=0.3\columnwidth]{non-uniform-ilum.png}

    \raggedright
    Solutions: increase ambient lighting by using diffusing panels or lots of point lights
    
}
\nDefinition{Radial lens distortion}{

    Lenses sometimes slightly distort the image "radially" making accurate measurements hard
    \nImgs[0.35]{radial-lens-distor.png}{lens-distortion-graphic.jpg}

    Solutions: more expensive lenses, view from further away
}
\nDefinition{Homography}{
    An invertible linear transformation $\mathbf{P}$ that maps points from one plane to another (think of it as a change of POV)

    \centering
    \includegraphics[width=0.8\columnwidth]{homography.png}

    \raggedright

    Given at least 4 corresponding points on each plane defining a POV, we can perform a least-square estimation of $\mathbf{P}$:
    \begin{equation}
        \mathbf{P} = 
        \begin{bmatrix}
            p_{11}&p_{12} &p_{13} \\
            p_{21}&p_{22} &p_{23} \\
            p_{31}&p_{32} &p_{33} 
        \end{bmatrix} 
    \end{equation}

    let $\vec{p} = (p_{11},p_{12} ,p_{13}
        p_{21},p_{22} ,p_{23},
        p_{31},p_{32} ,p_{33})$
        
    let $\mathbf{A}_{i} = 
        \begin{bmatrix}
            0 & 0 & 0 & -u_{i} & -v_{i} & -1 & y_{i}u_{i} & y_{i}v_{i} & y_{i} \\
            u_{i} & v_{i} & 1 & 0 & 0 & 0 & -x_{i}u_{i} & -x_{i}v_{i} & -x_{i} 
        \end{bmatrix}$
    
    construct $\mathbf{A} = 
        \begin{bmatrix}
            A_{1}\\A_{2}\\ \hdots \\ A_{N}
        \end{bmatrix}
    $

    Compute SVD($\mathbf{A}) = \mathbf{UDV'} $
    
    $\vec{p}$ is last column of $\mathbf{V}$ (eigenvector of smallest eigenvalue of $\mathbf{A}$)
    
    Then once we know the homography $\mathbf{P}$, then we can map (u,v) onto (x,y) using:
    \begin{equation}
        \begin{pmatrix}
            \lambda x \\ \lambda y \\ \lambda  
        \end{pmatrix}
        = \mathbf{P}
        \begin{pmatrix}
            u \\ v \\ 1
        \end{pmatrix}
    \end{equation}
    ($\lambda$ representing the fact that this coordinate is in homogenous space)

}

\nSection{Image Segmentation}
\nDefinition{Approaches}{
    Image segmentation is the process of grouping pixels which belong together semantically, i.e. perhaps because they belong to the same object.

    We can segment based on many facts:
    \begin{itemize}
        \item Contrast - objects have different lightness : use thresholding
        \item Change - objects different from background : background models 
        \item Similarity - objects have consistent colours : colour clustering
    \end{itemize}
}

\nDefinition{Thresholding}{
    This method assumes that pixels are separable based on their color values. We can pick threshold boundaries for each color value and select regions based on 
    regions of pixels which fall in those boundaries.

    \centering
    \includegraphics[width=0.8\columnwidth]{thresholding.png}
    \raggedright

    problems:
    \begin{itemize}
        \item Distributions may be broad and have some overlap leading to misclassified pixels
        \item variations in lighting might cause parts of the object to be missing, or shadows to be classified as objects
        \item color distributions might have more than 2 peaks
    \end{itemize}
}

\nDefinition{Convolutions}{
    General-purpose image (and signal) processing function.

    can be used to remove noise, smooth data, or detect features!

    In the case of thresholding, we can use convolutions to smooth the histogram.
    Imagine convolutions as a sliding window, where each point in the original image is replaced with the weighted average of the window at that position with the pixels.

    \centering
    \includegraphics[width=0.8\columnwidth]{convolution.png}

    \raggedright

    Convolution in 1D, with kernel of size (odd) N (even kernels require padding with zeros):
    \begin{equation}
        \textit{Output}(x) = \sum_{i = -\lfloor N/2 \rfloor}^{\lfloor N/2 \rfloor} \textit{weight}(i) * \textit{input}(x - i) 
    \end{equation}

    \centering
    \includegraphics[width=0.5\columnwidth]{convolution_example.png}
    
    \raggedright
    Convolution in 2D, with kernel of size (odd) N:
    \begin{equation}
        \textit{Output}(x) = \sum_{i = -\lfloor N/2 \rfloor}^{\lfloor N/2 \rfloor}\sum_{j = -\lfloor N/2 \rfloor}^{\lfloor N/2 \rfloor} \textit{weight}(i,j) * \textit{input}(x - i,y-j) 
    \end{equation}
}

\nDefinition{Smoothing kernel (2d gaussian)}{
    \centering
    \includegraphics[width=0.9\columnwidth]{smoothing_kernel.png}
}

\nDefinition{Edge Detection kernel}{
    \centering
    \includegraphics[width=0.9\columnwidth]{edge_detection_kernel.png}

}

\nDefinition{Background removal}{
    If we have 2 images, one with just the background (\textbf{B}) and one with background and foreground (the image \textbf{I}), we can

    \begin{equation}
        N = I - B       
    \end{equation}
    
    This difference will zero-out pixels with identical values to the background, and only leave those values which are different (either positive or negative depending on if the foreground is brighter or darker than the background at each point)

    We can do this for each channel of the image, and perform thresholding on the logical or between all the resulting differential pictures.

    \centering 
    \includegraphics[width=0.7\columnwidth]{bg_differencing.png}

    \raggedright

    we can also use division instead of substraction to achieve a similar effect:
    \begin{equation}
        N = I / B
    \end{equation}

    This in effect removes the effects of illumination since:

    \begin{equation}
        \textit{background}(i,j) = \textit{illumination}(i,j) \cdot \textit{bg\_reflectance}(i,j)
    \end{equation}
    \begin{equation}
        \textit{object}(i,j) = \textit{illumination}(i,j) \cdot \textit{obj\_reflectance}(i,j)
    \end{equation}

    The pixels with a value of 1 are going to be the background, pixels with value $>$ 1 are lighter objects and pixels with values $<$ 1 are darker objects (than the background)

    In both of these techniques, we might need to use an operator such as the \textbf{open} operator to remove noise artifacts (with values which are just around the values which signify background pixels but not quite)

    Neither will work well when the background in I and B varies wildly.

}

\nDefinition{RGB Normalisation}{
    differences in lighting can be dealt with by normalising the RGB values of the image:

    \begin{equation}
        (r',g',b') = (\frac{r}{r+g+b},\frac{g}{r+g+b},\frac{b}{r+g+b})
    \end{equation}

    since multiplying all values r,g,b in the original space by a constant, changes the brightness of the color, we remove this effect thanks to the equation above,
    mapping all different brightness values of the same colour to one value.

    \centering
    \includegraphics[width=0.5\columnwidth]{rgb_normalisation}
}

\nDefinition{Mean Shift Segmentation}{
    We can segment the image by performing clustering on the pixels by their color values (or any attributes for that reason)!

    The algorithm works as follows:

    \begin{enumerate}
        \item create a feature space over the attributes chosen to represent each pixel (for example for a grayscale this could be a 1d intensity axis)
            \newline \includegraphics[width=0.7\columnwidth]{msc_0.png}
        \item distribute a number of "search windows" or kernels over the space 
            \newline \includegraphics[width=0.7\columnwidth]{msc_1.png}
        \item calculate each window's mean 
        \item shift the center of each window to its mean 
            \newline \includegraphics[width=0.7\columnwidth]{msc_3.png} 
        \item repeat steps 3-4 until convergence 
            \newline \includegraphics[width=0.7\columnwidth]{msc_4.png}
        \item merge windows ending up in close-enough locations, and call these the clusters
        \item cluster each pixel according to which cluster its original window ended up at 
            \newline \includegraphics[width=0.7\columnwidth]{msc_5.png}
    \end{enumerate}

    the feature space can contain any number of dimensions, and so we could include spatial, color, texture-data, and so on. This is a very versatile algorithm.
    It is application-independent, model-free (does not assume any shape of clusters),
    only requires a single parameter (window size h) which affects the scale of the clustering
    It is robust to outliers and finds a variable number of modes given the same h.

    The output is heavily dependent on the window size h, however. And the selection of h is not trivial. The whole algorithm is rather expensive and does not scale well with the dimension of the feature space.
    }

\nSection{Description of Segments}

\nDefinition{Shape}{

    a set of points in the plane, or a continuous outline (silhouette)

    \centering
    \includegraphics[width=0.5\columnwidth]{shape.png}

    \raggedright

    \nHeader{Cues}

    shapes can give us cues (\textbf{interior} and \textbf{boundary} cues)about the objects they outline.

    Some classes are defined purely by the boundary of the shape, some are defined purely by the \textbf{contents/interior} of the shape (i.e. texture,color), and some are defined by a mixture of both

    \nHeader{Correspondence and recognition}

    We can draw conclusions about similarities between shapes using \textbf{point-to-point} correspondences or \textbf{shape characteristics} to help us recognize objects belonging to certain classes.

    \centering
    \includegraphics[width=0.7\columnwidth]{correspondence_recognition.png}

    \raggedright

    Good methods of finding similarities will be :
    \begin{itemize}
        \item Invariant to rigid transformations like: translation, rotation and scale
         \newline \nImg[0.4]{invariance_rigid.png}
        \item Tolerant to non-rigid deformations
         \newline \nImg[0.4]{tolerance_non-rigid.png}
    \end{itemize} 
    }


\nDefinition{Global shape descriptors}{
    Shape descriptors which put a number of a certain characteristic of a shape based on its \textbf{entirety} - hence "global".

    \nHeader{Convexity}

    Convexity describes the ratio of a shape's convex hull to its perimeter, values of 1 mean that the shape is entirely convex, and values $<$ 1 mean the shape is less convex.
        \begin{center}
        \nImg[0.2]{convexity.png}
    \end{center}
    \begin{equation}
        \textit{conv} = \frac{P_{\textit{hull}}}{P_{\textit{shape}}}
    \end{equation}

    \nHeader{Compactness}
    Compactness describes how close the perimeter of the shape is to the perimeter of the circle with the same area.
    \begin{itemize}
        \item If the circle of equal area has a smaller perimeter, this value will be smaller than 1, meaning that the shape's "mass" is distributed in a less compact manner.
        \item If the circle of equal area has equal parimeter, this value will be equal to 1, meaning the shape's "mass" is distributed as compactly as possible.
        \item This value cannot be greater than 1, as the circle is the most compact distribution of mass
    \end{itemize}
    \begin{center}   
        \nImg[0.2]{compactness.png}
    \end{center}
    \begin{equation}
        \textit{comp} = \frac{2\sqrt{A\pi}}{P_{\textit{shape}}}
    \end{equation}


    \nHeader{Elongation}

    The elongation is simply the ratio of the principal axes, i.e. the aspect ratio of a shape, this value can be anywhere between 0 (flat line) and $\infty$ (also flat line)
    This can be computed by taking the cross product of the principal axes with their length being set to the eigen values of the covariance matrix (if you treat each pixel as a data point)
    \begin{center}
        \nImg[0.2]{elongation.png}        
    \end{center}
    \begin{equation}
        \textit{elong} = \frac{c_{yy} + c_{xx} - \sqrt{(c_{yy}+c_{xx})^{2} -4(c_{xx}c_{yy} - c_{xy}^{2})}}
                                {c_{yy} + c_{xx} + \sqrt{(c_{yy}+c_{xx})^{2} -4(c_{xx}c_{yy} - c_{xy}^{2})}}
    \end{equation}

    \nHeader{Properties of these global descriptors}

    \begin{itemize}[noitemsep]
        \item[+] Invariant to translation/rotation/scale (rigid)
        \item[+] Robust to shape deformations (non-rigid)
        \item[+] Simple 
        \item[+] Fast to compute  
        \item[-] These do not find any point correspondences, 
        \item[-] Little power to discriminate between shapes (Can you discriminate between the shape of a horse and a plane with these ?) 
    \end{itemize}
}


\nDefinition{Moments}{
    Moments in mathematics are measures which put a number on the function of interest's graph. A shape can be thought of like the graph of some function defined on the 2D space (\textbf{f(x,y)}) 

    Family of stable \textbf{binary} (and grey level) shape descriptions which can be made invariant to translation, rotation and scaling

    Let $p_{yx}$ be the pixel value $ \in {0,1}$ at row y and column x

    Area $A = \sum_{y}\sum_{x}p_{yx}$
    
    Center of mass $(\hat{y},\hat{x}) = (\frac{1}{A}\sum_{y}\sum_{x}y \cdot p_{yx},\frac{1}{A}\sum_{y}\sum_{x}x \cdot p_{yx})$ i.e. average of x and y values weighted by "mass"

    \nHeader{Translation invariant}

    let $u,v \in \mathbb{Z}$

    then a family of 'central' (translation invariant) moments can be defined as:
    \begin{equation}
        m_{uv} = \sum_{y}\sum_{x}(y - \hat{y})^{u}(x - \hat{x})^{v}p_{yx}
    \end{equation}

    notice how with $u,v = 2$ this is somewhat similar to variance and a little close to the moment of inertia ($\sum_{p} mr^{2}$). This moment encapsulates the distribution of points around the center of mass, thanks to this 
    it does not matter where the shape is positioned.

    \nHeader{Scale invariant}

    We can make this family of moments invariant by noticing the fact that if we double the dimensions uniformly, then the moment $m_{uv}$
    increases by a factor of $2^{u}2^{v}$ w.r.t weightings ($y - \hat{y},x - \hat{x}$) and its area increases by 4.
    Hence $A^{\frac{u+v}{2}+1}$ grows by a factor of $4 \cdot 2^{u}2^{v}$,
    Therefore the ratio:
    \begin{equation}
        \mu_{uv} = \frac{m_{uv}}{A^{\frac{u+v}{2}+1}} = \frac{m_{uv}}{m_{00}^{\frac{u+v}{2}+1}}
    \end{equation}
    is invariant to scale (it cancels out the effects of increasing area, i.e. area = 1)

    \nHeader{Rotation invariant}

    We can generate a similar moment using complex numbers and multiple scale-invariant moments which is invariant to rotation:

    let $c_{uv} = \sum_{y}\sum_{x}((y - \hat{y}) + i(x - \hat{x}))^{u} ((y - \hat{y}) - i(x - \hat{x}))^{v}p_{yx}$
    
    then let:
    \begin{equation}
        \begin{aligned}
            &s_{11} = c_{11}/A^{2} \\
            &s_{20} = c_{20}/A^{2} \\
            &s_{21} = c_{21}/A^{2.5} \\
            &s_{12} = c_{12}/A^{2.5} \\
            &s_{30} = c_{30}/A^{2.5} \\
        \end{aligned}
    \end{equation}

    we can combine these to get rotation invariant descriptors in similar magnitudes like so:
    \begin{equation}
        \begin{aligned}
            &ci_{1} = \textit{real}(s_{11}) \\
            &ci_{2} = \textit{real}(10^{3}\cdot s_{21} \cdot s_{12}) \\
            &ci_{3} = 10^{4} \cdot \textit{real}(s_{20} \cdot s_{12}^{2})\\
            &ci_{4} = 10^{4} \cdot \textit{imag}(s_{20} \cdot s_{12}^{2})\\
            &ci_{5} = 10^{6} \cdot \textit{real}(s_{30} \cdot s_{12}^{3})\\
            &ci_{6} = 10^{6} \cdot \textit{imag}(s_{30} \cdot s_{12}^{3})\\
        \end{aligned}
    \end{equation}

    }

\nDefinition{Shape signatures}{
    We can represent the shape using a 1D function (\textbf{f(t)}) defined via the points on the boundary of the shape.
    Once we have such descriptors, we can establish similarity between two shapes using: $\int f(t) - f(t')$ i.e. the difference between the shape's descriptors integrated over t
    \nHeader{Centroid distance}

    for angle $t$, and point on boundary at that angle $p(t)$
    \begin{equation}
        \textit{r}(t) = d(p(t),\textit{centroid}) 
    \end{equation}
    \begin{center}
        \nImg{centroid_distance}    
    \end{center}

    \nHeader{Curvature}
    for angle $t$, and angle $\theta$ representing the angle between points $p(t)$ and $p({t+\Delta t})$ on the boundary at the angles $t$ and $t+ \Delta t$
    \begin{equation}
        k(t) = d\theta/dt
    \end{equation}

    \begin{center}
        \nImg{curvature_descriptor}
    \end{center}

    \nHeader{Properties of shape signatures}

    \begin{itemize}[noitemsep]
        \item[+] invariant to translation,scale(if shape is normalized), rotation (if orientation is normalized)
        \item[+] point correspondences (if both descriptors are aligned)
        \item[+] informative 
        \item[+] deformations affect signature locally  and not globally (i.e. at a single point of the signature)  
        \item[\texttildelow] manages to handle shape deformation to some degree
        \item[-] where to start t ? high computational cost of alignment of two signature functions
        \item[-] sensitive to noise (especially with derivatives)  
    \end{itemize}
}


\nDefinition{Shape Context}{
    Shape context is a shape descriptor utilizing the local properties of points on the boundary of each shape to establish \textbf{point-to-point} correspondences

    We do this by counting the number of other points around the points on the boundary of each shape in each bin of a polar-coordinate "kernel" (this forms a histogram)
    \begin{center}
        \nImgs{shape_context_descriptor_polar.png}{shape_context_descriptor}
    \end{center}

    We can compare the K-bin histograms $h_{i}(k),h_{j}(k)$ of two points $i,j$ on different shapes respectively, using the chi-squared test: 
    \begin{equation}
        C(i,j) = \frac{1}{2} \sum_{k=1}^{K}\frac{(h_{i}(k) - h_{j}(k))^{2}}{h_{i}(k) + h_{j}(k)}
    \end{equation} 
    This establishes a cost function over which we can pair-up the corresponding points on each shape, by finding the least-cost matching $\pi(p)$ of points on one shape to the other (perhaps using the hungarian or blossom algorithms)
    which minimizes the total cost:
    \begin{equation}
        H(\pi) = \sum_{p \in \textit{all\_points}} C(p,\pi(p))
    \end{equation}

    thus establishing a point-to-point correspondence between two shapes:
    \begin{center}
        \nImg[0.2]{point_correspondance}
    \end{center}

    \nHeader{Propertis of shape signatures}

    \begin{itemize}[noitemsep]
        \item[+] invariant to translation
        \item[+] invariant to scaling (if we normalize the radial distances between points in each shape by their mean)
        \item[+] informative - describes points in the context of the overall shape
        \item[+] handles non-rigid deformations quite well - more sensitive for deformations closest to the point of interest due to shape of kernel 
        \item[-] not invariant to scale (but could be added by measuring angles in terms of tangents at each point instead of global coordinates)  
        \item[-] many parameters (\# and size of bins, \# of iterations, \# number of points, etc..)
        \item[-] very expensive computationally  
    \end{itemize}
}

\nSection{Object recognition}

\nDefinition{Assumptions \& Approaches}{

    \nHeader{Approaches}
    Several approaches to classification/recognition. Choose the same class as objects with:
    \begin{itemize}
        \item \textbf{Shape} - similar shape descriptors 
        \item \textbf{Appearence} - similar pixel values 
        \item \textbf{Geometric} - similar structures in similar places with similar parameters 
        \item \textbf{Graph} - similar part relationships
        \item \textbf{Bag of words} - similar local feature descriptors (frankenstein objects made up of smaller objects)
    \end{itemize}

    \nHeader{Assumptions}
    Assumptions made in this course: 
    \begin{itemize}
        \item Flat objects, viewed orthographically
        \item Always looked at from same distance
        \item Good contrast everywhere 
        \item No specularities 
        \item shape-based recognition only 
    \end{itemize}

    \nHeader{Shape-based recognition}

    \begin{enumerate}
        \item Extract object from image via segmentation 
        \item Compute its properties 
        \item Use those properties to compute the class it belongs to 
        \item Learn/improve the model properties for the classes
    \end{enumerate}
}

\nDefinition{Probabilistic object recognition}{
    The process of classifying the shape into a class by calculating the probability of it belonging to each class.

    \nHeader{Bayes rule}

    we can calculate the probability of feature vector $\vec{x}$ (which may be a collection of shape descriptor values, or any other properties) being drawn from the probability distribution which best describes the class c as:
    \begin{equation}
        p(c|\vec{x}) = 
        \frac{p(\vec{x}|c)p(c)}{p(\vec{x})} =
        \frac{p(\vec{x}|c)p(c)}{\sum_{k}p(\vec{x}|k)p(k)}
    \end{equation}

    where:
    \begin{itemize}
        \item $p(\vec{x}|c)$ is the probability of observing the feature vector $\vec{x}$ if it belongs to class c (using the distribution of feature vectors from class c)
        \item $p(c)$ is the $\textit{a priori}$ probability of observing a feature vector from class c (before making any observations)
        \item $p(\vec{x})$ is the total probability of seeing the feature vector $\vec{x}$ amongst all the classes
    \end{itemize}
}

\nTheorem{Multivariate Gaussian distribution}{
    how do we model the probability $p(\vec{x}|c)$ of observing each feature class, knowing some feature vectors belonging to each class ?

    We can perform Maximum likelihood estimation (MLE) on the observed $k > n$ (n being the dimensionality of $\vec{x}$)"training" instances of data  and build a multivariate gaussian distribution for each class.
    MLE yields the following values for each class:

    \begin{itemize}
        \item mean vector of each feature $\vec{m}_{c}$ of dimension n - average value of each feature in class c:
            \begin{equation}
                \vec{m}_{c} = \frac{1}{k}\sum_{i=1}^{k}\vec{x}_{i}
            \end{equation}
        \item covariance matrix $\mathcal{A}_{c}$ - the $n x n$ matrix of co-variances betwen each pair of features/properties:
            \begin{equation}
                \mathcal{A}_{c} = \frac{1}{k-1}\sum_{i=1}^{k}(\vec{x}_{i}-\vec{m}_{c})(\vec{x}_{i} - \vec{m}_{c})^{T}
            \end{equation}
    \end{itemize}

    With those properties the multivariate gaussian is formed as follows:
    \begin{equation}
        p(\vec{x}|c) = \frac{1}{(2\pi)^{\frac{n}{2}}}\frac{1}{|\mathcal{A}_{c}|^{\frac{1}{2}}}\exp^{-\frac{1}{2}[(\vec{x} - \vec{m}_{c})^{T} \mathcal{A}_{c}^{-1}(\vec{x} - \vec{m}_{c})]}
    \end{equation}
}

\nDefinition{Recognition Algorithmics}{

    We split the data into:
    \begin{itemize}
        \item a \textbf{training} set to estimate the model's parameters (e.g. the gaussian distributions)
        \item a \textbf{validation} set to pick the ideal "hyper" parameters which affect the performance of the algorithm without necessarily affecting the underlying model 
        \item a \textbf{test} set to evaluate the performance of the algorithm 
    \end{itemize}

    note: \textit{we must have more training samples than the dimensions of the feature vectors used!} 
}

\nDefinition{Chamfer-based shape matching}{
    We can employ an entirely different method of object recognition. In a way similar to 2D convolutions:
    \begin{itemize}
        \item Extract edges/contours of image we want to identify object in (perhaps using convolutions)
        \item Create a chamfer or "template" which forms the shape of the object we want to identify in the image 
        \item Slide it over the image, at each point find the "chamfer distance" by calculating the average distance of points on the chamfer to closest edges in the image: 
            \begin{equation}
                D_{\textit{chamfer}}(T,I) = \frac{1}{|T|}\sum_{t \in T}d_{I}(t)
            \end{equation} 
            where: 
            \subitem $T,I$ are the sets of template and image points respectively 
            \subitem $d_{I}(t)$ is the minimum distance for any template point t to any point in the image I
            \subitem $|T|$ is the number of points in the template
    \end{itemize}

    \nHeader{Optimisations}
    the naive implementation is very expensive as we re-compute the distances between each time.
    Instead we can do this only once by producing a look-up image of distances encoding the distance between each pixel in the image to the nearest edge inside it:
    \begin{center}
        \nImg{chamfer_matching_map.png}         
    \end{center}
}
\
\end{document}
